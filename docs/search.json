[
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/annalisecramer/MADA/annalisecramer-MADA-portfolio-main/annalisecramer-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Allergies             0             1   4   9     0        5          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean    sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0  133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2   45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  24.2  2.54  20  23  25  26   28 ▃▂▂▇▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "presentation-exercise/visualization-exercise.html",
    "href": "presentation-exercise/visualization-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "I wanted to challenge myself to become better at making maps, since this is an area that I need improvement on. I chose a fun article about Americans’ favorite thanksgiving side dishes. Here’s the original plot I will attempt to recreate.\n To start, I’ll load packages and data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/annalisecramer/MADA/annalisecramer-MADA-portfolio-main/annalisecramer-MADA-portfolio\n\nhere()\n\n[1] \"/Users/annalisecramer/MADA/annalisecramer-MADA-portfolio-main/annalisecramer-MADA-portfolio\"\n\nthanksgiving &lt;- read_csv(here(\"presentation-exercise/thanksgiving-2015/thanksgiving-2015-poll-data.csv\"))\n\nNew names:\nRows: 1058 Columns: 65\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(64): Do you celebrate Thanksgiving?, What is typically the main dish at... dbl\n(1): RespondentID\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `Which of these side dishes aretypically served at your Thanksgiving dinner?\n  Please select all that apply. - Other (please specify)` -&gt; `Which of these\n  side dishes aretypically served at your Thanksgiving dinner? Please select\n  all that apply. - Other (please specify)...25`\n• `Which of these side dishes aretypically served at your Thanksgiving dinner?\n  Please select all that apply. - Other (please specify)` -&gt; `Which of these\n  side dishes aretypically served at your Thanksgiving dinner? Please select\n  all that apply. - Other (please specify)...26`\n• `Which type of pie is typically served at your Thanksgiving dinner? Please\n  select all that apply. - Other (please specify)` -&gt; `Which type of pie is\n  typically served at your Thanksgiving dinner? Please select all that apply. -\n  Other (please specify)...38`\n• `Which type of pie is typically served at your Thanksgiving dinner? Please\n  select all that apply. - Other (please specify)` -&gt; `Which type of pie is\n  typically served at your Thanksgiving dinner? Please select all that apply. -\n  Other (please specify)...39`\n• `Which of these desserts do you typically have at Thanksgiving dinner? Please\n  select all that apply.  - Other (please specify)` -&gt; `Which of these desserts\n  do you typically have at Thanksgiving dinner? Please select all that apply.\n  - Other (please specify)...50`\n• `Which of these desserts do you typically have at Thanksgiving dinner? Please\n  select all that apply.  - Other (please specify)` -&gt; `Which of these desserts\n  do you typically have at Thanksgiving dinner? Please select all that apply.\n  - Other (please specify)...51`\n\n\nHere’s my final plot. I used a color picker app to get the right colors, but the yellow and tealstill looks a little bit too pale.\nAfter several hours, I cannot figure out how to get Alaska and Hawaii. Some of my attempts are pictured further down in this document.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(mapdata)\n\n# Define region colors and labels\nregion_data &lt;- data.frame(\n  state = tolower(c(\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Rhode Island\", \"Connecticut\", \n                    \"New York\", \"New Jersey\", \"Pennsylvania\", \n                    \"Wisconsin\", \"Michigan\", \"Illinois\", \"Indiana\", \"Ohio\", \n                    \"North Dakota\", \"South Dakota\", \"Nebraska\", \"Kansas\", \"Minnesota\", \"Iowa\", \"Missouri\", \n                    \"Delaware\", \"Maryland\", \"Virginia\", \"West Virginia\", \"North Carolina\", \"South Carolina\", \"Georgia\", \"Florida\", \n                    \"Kentucky\", \"Tennessee\", \"Mississippi\", \"Alabama\", \n                    \"Oklahoma\", \"Texas\", \"Arkansas\", \"Louisiana\", \n                    \"Montana\", \"Idaho\", \"Wyoming\", \"Nevada\", \"Utah\", \"Colorado\", \"Arizona\", \"New Mexico\", \n                    \"Washington\", \"Oregon\", \"California\", \"Alaska\", \"Hawaii\")),\n\n  region = c(rep(\"SQUASH\", 6),  # New England\n             rep(\"SQUASH\", 3),  # Middle Atlantic\n             rep(\"ROLLS/BISCUITS\", 5),  # East North Central\n             rep(\"GREEN BEANS/CASSEROLE\", 7),  # West North Central\n             rep(\"MAC & CHEESE\", 8),  # South Atlantic\n             rep(\"MAC & CHEESE\", 4),  # East South Central\n             rep(\"CORNBREAD\", 4),  # West South Central\n             rep(\"SALAD\", 13)),  # Mountain & Pacific\n\n  color = c(rep(\"#D9A2C4\", 6),  # Pink (SQUASH)\n            rep(\"#D9A2C4\", 3),  # Pink (SQUASH)\n            rep(\"#B6D382\", 5),  # Lime Green (ROLLS/BISCUITS)\n            rep(\"#F8D867\", 7),  # Yellow (GREEN BEANS/CASSEROLE)\n            rep(\"#90A9D5\", 8),  # Blue (MAC & CHEESE)\n            rep(\"#90A9D5\", 4),  # Blue (MAC & CHEESE)\n            rep(\"#EBA482\", 4),  # Red-Orange (CORNBREAD)\n            rep(\"#77C2BE\", 13))  # Teal (SALAD)\n)\n\n# Get US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with region data\nmap_data &lt;- left_join(us_states, region_data, by = c(\"region\" = \"state\"))\n\n# Update legend_data to wrap text\nlegend_data &lt;- data.frame(\n  dish = c(\"SQUASH\", \"MAC & CHEESE\", \"ROLLS/ BISCUITS\", \"GREENBEANS/ CASSEROLE\", \"CORNBREAD\", \"SALAD\"),\n  color = c(\"#D9A2C4\", \"#90A9D5\", \"#B6D382\", \"#F8D867\", \"#EBA482\", \"#77C2BE\"),\n  x = c(-73, -80, -83, -104, -96, -132),  # Customize x positions for each label\n  y = c(49, 31, 48, 51, 27.5, 44)  # Customize y positions for each label\n)\n\n# Wrap the dish names to two lines\nlegend_data$dish &lt;- str_wrap(legend_data$dish, width = 10)  # Adjust 'width' to control wrapping\n\n# Plot the map with wrapped labels\nggplot(map_data, aes(long, lat, group = group, fill = color)) +\n  geom_polygon(color = \"grey65\", size = 0.3) +\n  scale_fill_identity() +\n  coord_map(\"albers\", lat0 = 25, lat1 = 50) +  # Curved map projection\n  geom_text(data = legend_data, \n            aes(x = x, y = y, label = dish, color = color),\n            inherit.aes = FALSE,\n            hjust = 0,\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_color_identity() +\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = expression(atop(\n    \"Most disproportionately common Thanksgiving side dish by region\",\n    atop(\"\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\",  \n    atop(scriptstyle(\"SURVEY DATES 11/17/2015                                               NO. OF RESPONDENTS 931\"),\n         \"\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\")  \n    ))\n  )) +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5),\n        panel.background = element_rect(fill = \"grey95\", color = NA),  # Light grey background\n        plot.background = element_rect(fill = \"grey95\", color = NA))   # Light grey background \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nI’m going to find some basic info to help feed into AI.\n\nunique(thanksgiving$`US Region`)\n\n [1] \"Middle Atlantic\"    \"East South Central\" \"Mountain\"          \n [4] \"Pacific\"            \"East North Central\" \"West North Central\"\n [7] \"West South Central\" \"South Atlantic\"     \"New England\"       \n[10] NA                  \n\n\nHere’s my initial prompt to chatGPT:\nI need code to make a figure in R of a map of the united states by state. Title will be “Side Dishes Of America’s Regions”, and the subtitle will be “Most disproportionately common Thanksgiving side dish by region”. Middle Atlantic region should be pale pink and be “SQUASH”. East South Central region should be soft blue and should be “MAC & CHEESE”. Mountain region should be light teal and be “SALAD”. Pacfic region should be light teal and be “SALAD”. East North Central region should be pale lime green and be “ROLLS/BISCUITS”. West North Central should be yellow and be “GREEN BEANS/CASSEROLE”. West South Central should be soft orange-red and be “CORNBREAD”. South Atlantic should be soft blue and be “MAC & CHEESE”. New England should be pale pink and be “SQUASH”. Do you have any questions? Do you know which states are in these regions?\nChatGPT confirmed that it had the states in each of the nine regions correct and provided this code.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(maps)\nlibrary(mapdata)\n\n# Define region colors and labels\nregion_data &lt;- data.frame(\n  state = tolower(c(\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Rhode Island\", \"Connecticut\", \n                    \"New York\", \"New Jersey\", \"Pennsylvania\", \n                    \"Wisconsin\", \"Michigan\", \"Illinois\", \"Indiana\", \"Ohio\", \n                    \"North Dakota\", \"South Dakota\", \"Nebraska\", \"Kansas\", \"Minnesota\", \"Iowa\", \"Missouri\", \n                    \"Delaware\", \"Maryland\", \"Virginia\", \"West Virginia\", \"North Carolina\", \"South Carolina\", \"Georgia\", \"Florida\", \n                    \"Kentucky\", \"Tennessee\", \"Mississippi\", \"Alabama\", \n                    \"Oklahoma\", \"Texas\", \"Arkansas\", \"Louisiana\", \n                    \"Montana\", \"Idaho\", \"Wyoming\", \"Nevada\", \"Utah\", \"Colorado\", \"Arizona\", \"New Mexico\", \n                    \"Washington\", \"Oregon\", \"California\", \"Alaska\", \"Hawaii\")),\n  region = c(rep(\"SQUASH\", 6), rep(\"SQUASH\", 3), rep(\"ROLLS/BISCUITS\", 5), rep(\"GREEN BEANS/CASSEROLE\", 7),\n             rep(\"MAC & CHEESE\", 8), rep(\"MAC & CHEESE\", 4), rep(\"CORNBREAD\", 4), rep(\"SALAD\", 8), rep(\"SALAD\", 5)),\n  color = c(rep(\"#f4c2c2\", 6), rep(\"#f4c2c2\", 3), rep(\"#bfff00\", 5), rep(\"#ffff66\", 7),\n            rep(\"#aec6cf\", 8), rep(\"#aec6cf\", 4), rep(\"#ff6347\", 4), rep(\"#20b2aa\", 8), rep(\"#20b2aa\", 5))\n)\n\n# Get US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with region data\nmap_data &lt;- left_join(us_states, region_data, by = c(\"region\" = \"state\"))\n\n# Plot map\nggplot(map_data, aes(long, lat, group = group, fill = color)) +\n  geom_polygon(color = \"black\", size = 0.3) +\n  scale_fill_identity() +\n  coord_fixed(1.3) +\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = \"Most disproportionately common Thanksgiving side dish by region\") +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5))\n\n\n\n\n\n\n\n\nI provided the following prompt to ChatGPT to update this figure:\nThis map needs a few adjustments. Alaska and Hawaii need to be included in the Pacific region. The pink color should be D9A2C4. The blue color should be 90A9D5. The lime green color should be B6D382. The yellow color should be F8D867. The red-orange color should be EBA482. The teal color should be 77C2BE. Next to each color area, there should be words for each type of food, the text should be the same color as each region and not overlapping the map. The map need to have the latitude curves, not be flat and straight.\nHere’s the reply with the updated figure.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(maps)\nlibrary(mapdata)\n\n# Define region colors and labels\nregion_data &lt;- data.frame(\n  state = tolower(c(\"maine\", \"new hampshire\", \"vermont\", \"massachusetts\", \"rhode island\", \"connecticut\", \n                    \"new york\", \"new jersey\", \"pennsylvania\", \n                    \"wisconsin\", \"michigan\", \"illinois\", \"indiana\", \"ohio\", \n                    \"north dakota\", \"south dakota\", \"nebraska\", \"kansas\", \"minnesota\", \"iowa\", \"missouri\", \n                    \"delaware\", \"maryland\", \"virginia\", \"west virginia\", \"north carolina\", \"south carolina\", \"georgia\", \"florida\", \n                    \"kentucky\", \"tennessee\", \"mississippi\", \"alabama\", \n                    \"oklahoma\", \"texas\", \"arkansas\", \"louisiana\", \n                    \"montana\", \"idaho\", \"wyoming\", \"nevada\", \"utah\", \"colorado\", \"arizona\", \"new mexico\", \n                    \"washington\", \"oregon\", \"california\", \"alaska\", \"hawaii\")),\n\n  region = c(rep(\"SQUASH\", 6),  # New England\n             rep(\"SQUASH\", 3),  # Middle Atlantic\n             rep(\"ROLLS/BISCUITS\", 5),  # East North Central\n             rep(\"GREEN BEANS/CASSEROLE\", 7),  # West North Central\n             rep(\"MAC & CHEESE\", 8),  # South Atlantic\n             rep(\"MAC & CHEESE\", 4),  # East South Central\n             rep(\"CORNBREAD\", 4),  # West South Central\n             rep(\"SALAD\", 13)),  # Mountain & Pacific\n\n  color = c(rep(\"#D9A2C4\", 6),  # Pink (SQUASH)\n            rep(\"#D9A2C4\", 3),  # Pink (SQUASH)\n            rep(\"#B6D382\", 5),  # Lime Green (ROLLS/BISCUITS)\n            rep(\"#F8D867\", 7),  # Yellow (GREEN BEANS/CASSEROLE)\n            rep(\"#90A9D5\", 8),  # Blue (MAC & CHEESE)\n            rep(\"#90A9D5\", 4),  # Blue (MAC & CHEESE)\n            rep(\"#EBA482\", 4),  # Red-Orange (CORNBREAD)\n            rep(\"#77C2BE\", 13))  # Teal (SALAD)\n)\n\n# Get US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with region data\nmap_data &lt;- left_join(us_states, region_data, by = c(\"region\" = \"state\"))\n\n# Compute label positions\nlabel_positions &lt;- map_data %&gt;%\n  group_by(region) %&gt;%\n  summarise(long = mean(range(long)), lat = mean(range(lat)), color = first(color), .groups = \"drop\")\n\n# Plot map\nggplot(map_data, aes(long, lat, fill = color)) +  # Remove group = group\n  geom_polygon(color = \"black\", size = 0.3) +\n  scale_fill_identity() +\n  coord_map(\"albers\", lat0 = 25, lat1 = 50) +  # Adjust for latitude curves\n  geom_text(data = label_positions, aes(x = long, y = lat, label = region, color = color), size = 4, fontface = \"bold\") +\n  scale_color_identity() +\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = \"Most disproportionately common Thanksgiving side dish by region\") +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5))\n\n\n\n\n\n\n\n\nThis plot is a mess! I’m going to try using Claude instead of chatGPT, giving it the same two prompts as above.\n\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(mapproj)\nlibrary(dplyr)\n\n# Define regions and their states\nregions &lt;- list(\n  \"New England\" = c(\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Rhode Island\", \"Connecticut\"),\n  \"Middle Atlantic\" = c(\"New York\", \"Pennsylvania\", \"New Jersey\"),\n  \"East North Central\" = c(\"Wisconsin\", \"Michigan\", \"Illinois\", \"Indiana\", \"Ohio\"),\n  \"West North Central\" = c(\"Missouri\", \"North Dakota\", \"South Dakota\", \"Nebraska\", \"Kansas\", \"Minnesota\", \"Iowa\"),\n  \"South Atlantic\" = c(\"Delaware\", \"Maryland\", \"District of Columbia\", \"Virginia\", \"West Virginia\", \"North Carolina\", \"South Carolina\", \"Georgia\", \"Florida\"),\n  \"East South Central\" = c(\"Kentucky\", \"Tennessee\", \"Mississippi\", \"Alabama\"),\n  \"West South Central\" = c(\"Oklahoma\", \"Texas\", \"Arkansas\", \"Louisiana\"),\n  \"Mountain\" = c(\"Idaho\", \"Montana\", \"Wyoming\", \"Nevada\", \"Utah\", \"Colorado\", \"Arizona\", \"New Mexico\"),\n  \"Pacific\" = c(\"Alaska\", \"Washington\", \"Oregon\", \"California\", \"Hawaii\")\n)\n\n# Create a data frame with state-region mappings\nstate_regions &lt;- data.frame(\n  state = unlist(regions),\n  region = rep(names(regions), sapply(regions, length))\n)\n\n# Define colors and foods by region\nregion_colors &lt;- c(\n  \"New England\" = \"#D9A2C4\",\n  \"Middle Atlantic\" = \"#D9A2C4\",\n  \"East North Central\" = \"#B6D382\",\n  \"West North Central\" = \"#F8D867\",\n  \"South Atlantic\" = \"#90A9D5\",\n  \"East South Central\" = \"#90A9D5\",\n  \"West South Central\" = \"#EBA482\",\n  \"Mountain\" = \"#77C2BE\",\n  \"Pacific\" = \"#77C2BE\"\n)\n\nregion_foods &lt;- c(\n  \"New England\" = \"SQUASH\",\n  \"Middle Atlantic\" = \"SQUASH\",\n  \"East North Central\" = \"ROLLS/BISCUITS\",\n  \"West North Central\" = \"GREEN BEANS/CASSEROLE\",\n  \"South Atlantic\" = \"MAC & CHEESE\",\n  \"East South Central\" = \"MAC & CHEESE\",\n  \"West South Central\" = \"CORNBREAD\",\n  \"Mountain\" = \"SALAD\",\n  \"Pacific\" = \"SALAD\"\n)\n\n# Get state map data\nstates_map &lt;- map_data(\"state\")\n\n# Convert state names to title case to match our data\nstates_map$region &lt;- tools::toTitleCase(states_map$region)\n\n# Add region information to the map data\nstates_map &lt;- left_join(states_map, state_regions, by = c(\"region\" = \"state\"))\n\n# Create label data frame\nlabel_data &lt;- data.frame(\n  long = c(-80, -82, -98, -110, -120, -85, -93, -103, -70),\n  lat = c(45, 38, 45, 45, 45, 32, 35, 38, 47),\n  region = names(region_foods),\n  label = region_foods\n)\n\n# Create the map\nggplot(states_map, aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = region), color = \"white\", size = 0.2) +\n  coord_map(\"albers\", lat0 = 39, lat1 = 45) +\n  scale_fill_manual(values = region_colors) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    axis.title = element_blank()\n  ) +\n  # Add labels for each region\n  geom_text(data = label_data, \n    aes(label = label, color = region, group = NULL),\n    size = 3, fontface = \"bold\") +\n  scale_color_manual(values = region_colors) +\n  labs(\n    title = \"Side Dishes Of America's Regions\",\n    subtitle = \"Most disproportionately common Thanksgiving side dish by region\"\n  )\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\n\n\n\n\n\n\n\n\n\nThat’s even worse. Back to chatGPT. I’m giving it the same prompt but taking out the part with the food labels next to the regions, because that seems to be where it’s getting confused.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(maps)\nlibrary(mapdata)\n\n# Define region colors and labels\nregion_data &lt;- data.frame(\n  state = tolower(c(\"maine\", \"new hampshire\", \"vermont\", \"massachusetts\", \"rhode island\", \"connecticut\", \n                    \"new york\", \"new jersey\", \"pennsylvania\", \n                    \"wisconsin\", \"michigan\", \"illinois\", \"indiana\", \"ohio\", \n                    \"north dakota\", \"south dakota\", \"nebraska\", \"kansas\", \"minnesota\", \"iowa\", \"missouri\", \n                    \"delaware\", \"maryland\", \"virginia\", \"west virginia\", \"north carolina\", \"south carolina\", \"georgia\", \"florida\", \n                    \"kentucky\", \"tennessee\", \"mississippi\", \"alabama\", \n                    \"oklahoma\", \"texas\", \"arkansas\", \"louisiana\", \n                    \"montana\", \"idaho\", \"wyoming\", \"nevada\", \"utah\", \"colorado\", \"arizona\", \"new mexico\", \n                    \"washington\", \"oregon\", \"california\", \"alaska\", \"hawaii\")),\n\n  region = c(rep(\"SQUASH\", 6),  # New England\n             rep(\"SQUASH\", 3),  # Middle Atlantic\n             rep(\"ROLLS/BISCUITS\", 5),  # East North Central\n             rep(\"GREEN BEANS/CASSEROLE\", 7),  # West North Central\n             rep(\"MAC & CHEESE\", 8),  # South Atlantic\n             rep(\"MAC & CHEESE\", 4),  # East South Central\n             rep(\"CORNBREAD\", 4),  # West South Central\n             rep(\"SALAD\", 13)),  # Mountain & Pacific\n\n  color = c(rep(\"#D9A2C4\", 6),  # Pink (SQUASH)\n            rep(\"#D9A2C4\", 3),  # Pink (SQUASH)\n            rep(\"#B6D382\", 5),  # Lime Green (ROLLS/BISCUITS)\n            rep(\"#F8D867\", 7),  # Yellow (GREEN BEANS/CASSEROLE)\n            rep(\"#90A9D5\", 8),  # Blue (MAC & CHEESE)\n            rep(\"#90A9D5\", 4),  # Blue (MAC & CHEESE)\n            rep(\"#EBA482\", 4),  # Red-Orange (CORNBREAD)\n            rep(\"#77C2BE\", 13))  # Teal (SALAD)\n)\n\n# Get US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with region data\nmap_data &lt;- left_join(us_states, region_data, by = c(\"region\" = \"state\"))\n\n# Plot map with curved projection\nggplot(map_data, aes(long, lat, group = group, fill = color)) +\n  geom_polygon(color = \"black\", size = 0.3) +\n  scale_fill_identity() +\n  coord_map(\"albers\", lat0 = 25, lat1 = 50) +  # Curved map projection to adjust for latitude lines\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = \"Most disproportionately common Thanksgiving side dish by region\") +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5))\n\n\n\n\n\n\n\n\nNot bad. ChatGPT, we need to add labels of which type of food these represent.\n\n# Get US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with region data\nmap_data &lt;- left_join(us_states, region_data, by = c(\"region\" = \"state\"))\n\n# Create labels data frame for the legend\nlegend_data &lt;- data.frame(\n  dish = c(\"SQUASH\", \"MAC & CHEESE\", \"ROLLS/BISCUITS\", \"GREEN BEANS/CASSEROLE\", \"CORNBREAD\", \"SALAD\"),\n  color = c(\"#D9A2C4\", \"#90A9D5\", \"#B6D382\", \"#F8D867\", \"#EBA482\", \"#77C2BE\"),\n  x = rep(-65, 6),  # Position on right side of map\n  y = seq(45, 25, length.out = 6)  # Spread vertically\n)\n\n# Plot map with curved projection and labels\nggplot(map_data, aes(long, lat, group = group, fill = color)) +\n  geom_polygon(color = \"grey65\", size = 0.3) +\n  scale_fill_identity() +\n  coord_map(\"albers\", lat0 = 25, lat1 = 50) +  # Curved map projection\n  geom_text(data = legend_data, \n            aes(x = x, y = y, label = dish, color = color),\n            inherit.aes = FALSE,\n            hjust = 0,\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_color_identity() +\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = \"Most disproportionately common Thanksgiving side dish by region\") +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5))\n\n\n\n\n\n\n\n\n\n# Get US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with region data\nmap_data &lt;- left_join(us_states, region_data, by = c(\"region\" = \"state\"))\n\n# Create labels data frame for the legend\nlegend_data &lt;- data.frame(\n  dish = c(\"SQUASH\", \"MAC & CHEESE\", \"ROLLS/BISCUITS\", \"GREEN BEANS/CASSEROLE\", \"CORNBREAD\", \"SALAD\"),\n  color = c(\"#D9A2C4\", \"#90A9D5\", \"#B6D382\", \"#F8D867\", \"#EBA482\", \"#77C2BE\"),\n  x = rep(-65, 6),  # Position on right side of map\n  y = seq(45, 25, length.out = 6)  # Spread vertically\n)\n\n# Add Alaska and Hawaii labels manually\nalaska_hawaii_labels &lt;- data.frame(\n  dish = c(\"ALASKA\", \"HAWAII\"),\n  color = c(\"#77C2BE\", \"#77C2BE\"),  # Same color as Pacific region (SALAD)\n  x = c(-140, -155),  # Position outside the map for Alaska and Hawaii\n  y = c(65, 55)  # Adjust vertical placement for clarity\n)\n\n# Plot map with curved projection, labels, and separate Alaska/Hawaii positions\nggplot(map_data, aes(long, lat, group = group, fill = color)) +\n  geom_polygon(color = \"grey65\", size = 0.3) +\n  scale_fill_identity() +\n  coord_map(\"albers\", lat0 = 25, lat1 = 50) +  # Curved map projection\n  geom_text(data = legend_data, \n            aes(x = x, y = y, label = dish, color = color),\n            inherit.aes = FALSE,\n            hjust = 0,\n            size = 3.5,\n            fontface = \"bold\") +\n  geom_text(data = alaska_hawaii_labels, \n            aes(x = x, y = y, label = dish, color = color),\n            inherit.aes = FALSE,\n            hjust = 0,\n            size = 4,\n            fontface = \"bold\") +\n  scale_color_identity() +\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = \"Most disproportionately common Thanksgiving side dish by region\") +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5))\n\n\n\n\n\n\n\n\nI asked to move Alaska nad Hawaii closer.\n\nregion_data &lt;- data.frame(\n  state = tolower(c(\"maine\", \"new hampshire\", \"vermont\", \"massachusetts\", \"rhode island\", \"connecticut\", \n                    \"new york\", \"new jersey\", \"pennsylvania\", \n                    \"wisconsin\", \"michigan\", \"illinois\", \"indiana\", \"ohio\", \n                    \"north dakota\", \"south dakota\", \"nebraska\", \"kansas\", \"minnesota\", \"iowa\", \"missouri\", \n                    \"delaware\", \"maryland\", \"virginia\", \"west virginia\", \"north carolina\", \"south carolina\", \"georgia\", \"florida\", \n                    \"kentucky\", \"tennessee\", \"mississippi\", \"alabama\", \n                    \"oklahoma\", \"texas\", \"arkansas\", \"louisiana\", \n                    \"montana\", \"idaho\", \"wyoming\", \"nevada\", \"utah\", \"colorado\", \"arizona\", \"new mexico\", \n                    \"washington\", \"oregon\", \"california\", \"alaska\", \"hawaii\")),\n  region = c(rep(\"SQUASH\", 6),  # New England\n             rep(\"SQUASH\", 3),  # Middle Atlantic\n             rep(\"ROLLS/BISCUITS\", 5),  # East North Central\n             rep(\"GREEN BEANS/CASSEROLE\", 7),  # West North Central\n             rep(\"MAC & CHEESE\", 8),  # South Atlantic\n             rep(\"MAC & CHEESE\", 4),  # East South Central\n             rep(\"CORNBREAD\", 4),  # West South Central\n             rep(\"SALAD\", 13)),  # Mountain & Pacific\n  color = c(rep(\"#D9A2C4\", 6),  # Pink (SQUASH)\n            rep(\"#D9A2C4\", 3),  # Pink (SQUASH)\n            rep(\"#B6D382\", 5),  # Lime Green (ROLLS/BISCUITS)\n            rep(\"#F8D867\", 7),  # Yellow (GREEN BEANS/CASSEROLE)\n            rep(\"#90A9D5\", 8),  # Blue (MAC & CHEESE)\n            rep(\"#90A9D5\", 4),  # Blue (MAC & CHEESE)\n            rep(\"#EBA482\", 4),  # Red-Orange (CORNBREAD)\n            rep(\"#77C2BE\", 13))  # Teal (SALAD)\n)\n\n# Get US map data including Alaska and Hawaii\nus_states &lt;- map_data(\"state\")\nalaska &lt;- map_data(\"world\", \"USA:alaska\")\nhawaii &lt;- map_data(\"world\", \"USA:hawaii\")\n\n# Transform Alaska coordinates\nalaska$long &lt;- alaska$long + 50\nalaska$lat &lt;- alaska$lat + 35\nalaska &lt;- alaska[alaska$long &gt; -130 & alaska$lat &gt; 51,]\n\n# Transform Hawaii coordinates\nhawaii$long &lt;- hawaii$long + 50\nhawaii$lat &lt;- hawaii$lat + 6\n\n# Combine all map data\nmap_data &lt;- bind_rows(\n  left_join(us_states, region_data, by = c(\"region\" = \"state\")),\n  left_join(alaska %&gt;% mutate(region = \"alaska\"), region_data, by = c(\"region\" = \"state\")),\n  left_join(hawaii %&gt;% mutate(region = \"hawaii\"), region_data, by = c(\"region\" = \"state\"))\n)\n\n# Create labels data frame with specific coordinates for each label\nlegend_data &lt;- data.frame(\n  dish = c(\"SQUASH\", \"MAC & CHEESE\", \"ROLLS/BISCUITS\", \"GREEN BEANS/CASSEROLE\", \"CORNBREAD\", \"SALAD\"),\n  color = c(\"#D9A2C4\", \"#90A9D5\", \"#B6D382\", \"#F8D867\", \"#EBA482\", \"#77C2BE\"),\n  x = c(-63, -63, -63, -63, -63, -63),  # Consistent x alignment\n  y = c(45, 41, 37, 33, 29, 25)  # Individual y coordinates\n)\n\n# Plot map with curved projection and labels\nggplot(map_data, aes(long, lat, group = group, fill = color)) +\n  geom_polygon(color = \"#666666\", size = 0.3) +\n  scale_fill_identity() +\n  coord_map(\"albers\", lat0 = 25, lat1 = 50) +\n  geom_text(data = legend_data, \n            aes(x = x, y = y, label = dish, color = color),\n            inherit.aes = FALSE,\n            hjust = 0,\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_color_identity() +\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = \"Most disproportionately common Thanksgiving side dish by region\") +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5))\n\n\n\n\n\n\n\n\nI’ve spent quite long working on this with ChatGPT and Claude. I suppose these tools aren’t the best for working the map plots. One of them wasn’t too terrible!\nNext, onto the table. First I need to process my data. I used AI to help with this.\n\nnames(thanksgiving)\n\n [1] \"RespondentID\"                                                                                                                                \n [2] \"Do you celebrate Thanksgiving?\"                                                                                                              \n [3] \"What is typically the main dish at your Thanksgiving dinner?\"                                                                                \n [4] \"What is typically the main dish at your Thanksgiving dinner? - Other (please specify)\"                                                       \n [5] \"How is the main dish typically cooked?\"                                                                                                      \n [6] \"How is the main dish typically cooked? - Other (please specify)\"                                                                             \n [7] \"What kind of stuffing/dressing do you typically have?\"                                                                                       \n [8] \"What kind of stuffing/dressing do you typically have? - Other (please specify)\"                                                              \n [9] \"What type of cranberry saucedo you typically have?\"                                                                                          \n[10] \"What type of cranberry saucedo you typically have? - Other (please specify)\"                                                                 \n[11] \"Do you typically have gravy?\"                                                                                                                \n[12] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Brussel sprouts\"                 \n[13] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Carrots\"                         \n[14] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Cauliflower\"                     \n[15] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Corn\"                            \n[16] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Cornbread\"                       \n[17] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Fruit salad\"                     \n[18] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Green beans/green bean casserole\"\n[19] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Macaroni and cheese\"             \n[20] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Mashed potatoes\"                 \n[21] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Rolls/biscuits\"                  \n[22] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Squash\"                          \n[23] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Vegetable salad\"                 \n[24] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Yams/sweet potato casserole\"     \n[25] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Other (please specify)...25\"     \n[26] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Other (please specify)...26\"     \n[27] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Apple\"                                    \n[28] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Buttermilk\"                               \n[29] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Cherry\"                                   \n[30] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Chocolate\"                                \n[31] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Coconut cream\"                            \n[32] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Key lime\"                                 \n[33] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Peach\"                                    \n[34] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Pecan\"                                    \n[35] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Pumpkin\"                                  \n[36] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Sweet Potato\"                             \n[37] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - None\"                                     \n[38] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Other (please specify)...38\"              \n[39] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Other (please specify)...39\"              \n[40] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Apple cobbler\"                       \n[41] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Blondies\"                            \n[42] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Brownies\"                            \n[43] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Carrot cake\"                         \n[44] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Cheesecake\"                          \n[45] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Cookies\"                             \n[46] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Fudge\"                               \n[47] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Ice cream\"                           \n[48] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Peach cobbler\"                       \n[49] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - None\"                                \n[50] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Other (please specify)...50\"         \n[51] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Other (please specify)...51\"         \n[52] \"Do you typically pray before or after the Thanksgiving meal?\"                                                                                \n[53] \"How far will you travel for Thanksgiving?\"                                                                                                   \n[54] \"Will you watch any of the following programs on Thanksgiving? Please select all that apply. - Macy's Parade\"                                 \n[55] \"What's the age cutoff at your \\\"kids' table\\\" at Thanksgiving?\"                                                                              \n[56] \"Have you ever tried to meet up with hometown friends on Thanksgiving night?\"                                                                 \n[57] \"Have you ever attended a \\\"Friendsgiving?\\\"\"                                                                                                 \n[58] \"Will you shop any Black Friday sales on Thanksgiving Day?\"                                                                                   \n[59] \"Do you work in retail?\"                                                                                                                      \n[60] \"Will you employer make you work on Black Friday?\"                                                                                            \n[61] \"How would you describe where you live?\"                                                                                                      \n[62] \"Age\"                                                                                                                                         \n[63] \"What is your gender?\"                                                                                                                        \n[64] \"How much total combined money did all members of your HOUSEHOLD earn last year?\"                                                             \n[65] \"US Region\"                                                                                                                                   \n\nthanksgiving2 &lt;- thanksgiving %&gt;%\n  select(`US Region`, `Do you celebrate Thanksgiving?`, `What is your gender?`, `Have you ever attended a \\\"Friendsgiving?\\\"`, `What's the age cutoff at your \\\"kids' table\\\" at Thanksgiving?`, `How much total combined money did all members of your HOUSEHOLD earn last year?`)\n\n\nunique(thanksgiving2$`What is your gender?`)\n\n[1] \"Male\"   \"Female\" NA      \n\nunique(thanksgiving2$`Do you celebrate Thanksgiving?`)\n\n[1] \"Yes\" \"No\" \n\nunique(thanksgiving2$`Have you ever attended a \\\"Friendsgiving?\\\"`)\n\n[1] \"No\"  \"Yes\" NA   \n\nunique(thanksgiving2$`What's the age cutoff at your \\\"kids' table\\\" at Thanksgiving?`)\n\n [1] \"12\"            \"19\"            \"13\"            \"10 or younger\"\n [5] \"20\"            \"21 or older\"   NA              \"15\"           \n [9] \"14\"            \"18\"            \"16\"            \"17\"           \n[13] \"11\"           \n\nunique(thanksgiving2$`How much total combined money did all members of your HOUSEHOLD earn last year?`)\n\n [1] \"$75,000 to $99,999\"   \"$50,000 to $74,999\"   \"$0 to $9,999\"        \n [4] \"$200,000 and up\"      \"$100,000 to $124,999\" \"$25,000 to $49,999\"  \n [7] \"Prefer not to answer\" \"$10,000 to $24,999\"   \"$150,000 to $174,999\"\n[10] \"$175,000 to $199,999\" \"$125,000 to $149,999\" NA                    \n\n# Function to handle age cutoff values and convert to numeric, including NA handling\nget_age_cutoff_value &lt;- function(age_cutoff) {\n  if (is.na(age_cutoff)) {\n    return(NA)  # Return NA if the value is missing\n  } else if (age_cutoff == \"10 or younger\") {\n    return(10)  # Assign 10 for \"10 or younger\"\n  } else if (age_cutoff == \"21 or older\") {\n    return(21)  # Assign 21 for \"21 or older\"\n  } else {\n    return(as.numeric(age_cutoff))  # Convert other values to numeric\n  }\n}\n\nget_income_value &lt;- function(income_range) {\n  if (is.na(income_range) | income_range == \"Prefer not to answer\") {\n    return(NA)  # Return NA for missing values or 'Prefer not to answer'\n  } else if (income_range == \"$0 to $9,999\") {\n    return(5000)\n  } else if (income_range == \"$10,000 to $24,999\") {\n    return(17500)\n  } else if (income_range == \"$25,000 to $49,999\") {\n    return(37500)\n  } else if (income_range == \"$50,000 to $74,999\") {\n    return(62500)\n  } else if (income_range == \"$75,000 to $99,999\") {\n    return(87500)\n  } else if (income_range == \"$100,000 to $124,999\") {\n    return(112500)\n  } else if (income_range == \"$125,000 to $149,999\") {\n    return(137500)\n  } else if (income_range == \"$150,000 to $174,999\") {\n    return(162500)\n  } else if (income_range == \"$175,000 to $199,999\") {\n    return(187500)\n  } else if (income_range == \"$200,000 and up\") {\n    return(200000)  # You can adjust this value if needed\n  } else {\n    return(NA)\n  }\n}\n\nthanksgiving2 %&gt;%\n  filter(!is.na(`US Region`)) %&gt;%  # Exclude rows where `US Region` is NA\n   mutate(Household_Income_Numeric = sapply(`How much total combined money did all members of your HOUSEHOLD earn last year?`, get_income_value)) %&gt;%\n  mutate(Age_Cutoff_Numeric = sapply(`What's the age cutoff at your \"kids' table\" at Thanksgiving?`, get_age_cutoff_value)) %&gt;%\n  group_by(`US Region`) %&gt;%\n  summarise(\n    Percent_Female = mean(`What is your gender?` == \"Female\", na.rm = TRUE) * 100,\n    Percent_Celebrate_Thanksgiving = mean(`Do you celebrate Thanksgiving?` == \"Yes\", na.rm = TRUE) * 100,\n    Percent_Attended_Friendsgiving = mean(`Have you ever attended a \"Friendsgiving?\"` == \"Yes\", na.rm = TRUE) * 100,\n    Age_Cutoff_Avg = mean(Age_Cutoff_Numeric, na.rm = TRUE),\n    Household_Income_Avg = mean(Household_Income_Numeric, na.rm = TRUE)\n  )\n\n# A tibble: 9 × 6\n  `US Region`       Percent_Female Percent_Celebrate_Th…¹ Percent_Attended_Fri…²\n  &lt;chr&gt;                      &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1 East North Centr…           52.7                   96.7                   20.7\n2 East South Centr…           55                     93.3                   25  \n3 Middle Atlantic             52.2                   91.2                   23.4\n4 Mountain                    68.1                   87.2                   46.3\n5 New England                 55.2                   94.8                   23.6\n6 Pacific                     50.7                   89.0                   40.8\n7 South Atlantic              55.1                   94.9                   27.1\n8 West North Centr…           55.4                   95.9                   12.7\n9 West South Centr…           46.2                   93.4                   35.3\n# ℹ abbreviated names: ¹​Percent_Celebrate_Thanksgiving,\n#   ²​Percent_Attended_Friendsgiving\n# ℹ 2 more variables: Age_Cutoff_Avg &lt;dbl&gt;, Household_Income_Avg &lt;dbl&gt;\n\n\nNow, making the table. Statistics are automatically calcualted before feeding into the table.\n\nlibrary(gt)\n\n#data manipulation code\nsummary_data &lt;- thanksgiving2 %&gt;%\n  filter(!is.na(`US Region`)) %&gt;%  # Exclude rows where `US Region` is NA\n  mutate(Household_Income_Numeric = sapply(`How much total combined money did all members of your HOUSEHOLD earn last year?`, get_income_value)) %&gt;%\n  mutate(Age_Cutoff_Numeric = sapply(`What's the age cutoff at your \"kids' table\" at Thanksgiving?`, get_age_cutoff_value)) %&gt;%\n  group_by(`US Region`) %&gt;%\n  summarise(\n    Percent_Female = mean(`What is your gender?` == \"Female\", na.rm = TRUE) * 100,\n    Percent_Celebrate_Thanksgiving = mean(`Do you celebrate Thanksgiving?` == \"Yes\", na.rm = TRUE) * 100,\n    Percent_Attended_Friendsgiving = mean(`Have you ever attended a \"Friendsgiving?\"` == \"Yes\", na.rm = TRUE) * 100,\n    Age_Cutoff_Avg = mean(Age_Cutoff_Numeric, na.rm = TRUE),\n    Household_Income_Avg = mean(Household_Income_Numeric, na.rm = TRUE)\n  )\n\nsummary_data %&gt;%\n  mutate(\n    Percent_Female = Percent_Female / 100,\n    Percent_Celebrate_Thanksgiving = Percent_Celebrate_Thanksgiving / 100,\n    Percent_Attended_Friendsgiving = Percent_Attended_Friendsgiving / 100\n  ) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Thanksgiving Statistics by US Region\",\n  ) %&gt;%\n  cols_label(\n    `US Region` = \"Region\",\n    Percent_Female = \"Percent Female\",\n    Percent_Celebrate_Thanksgiving = \"Percent Who Celebrate Thanksgiving\",\n    Percent_Attended_Friendsgiving = \"Percent Who Have Attended Friendsgiving\",\n    Age_Cutoff_Avg = \"Average Kids' Table Age Cutoff\",\n    Household_Income_Avg = \"Average Household Income\"\n  ) %&gt;%\n  fmt_percent(\n    columns = c(Percent_Female, Percent_Celebrate_Thanksgiving, Percent_Attended_Friendsgiving),\n    decimals = 2\n  ) %&gt;%\n  fmt_number(\n    columns = c(Age_Cutoff_Avg),\n    decimals = 1\n  ) %&gt;%\n  fmt_number(\n    columns = c(Household_Income_Avg),\n    decimals = 1\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body(columns = \"US Region\")\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels(columns = c(Percent_Female, Percent_Celebrate_Thanksgiving, Percent_Attended_Friendsgiving, Age_Cutoff_Avg, Household_Income_Avg))\n  ) %&gt;%\n  tab_spanner(\n    label = \"Behavioral Data\",\n    columns = c(Percent_Female, Percent_Celebrate_Thanksgiving, Percent_Attended_Friendsgiving)\n  ) %&gt;%\n  tab_spanner(\n    label = \"Demographic Data\",\n    columns = c(Age_Cutoff_Avg, Household_Income_Avg)\n  ) %&gt;%\n  tab_style(\n    style = cell_borders(sides = c(\"left\", \"right\"), color = \"grey\", weight = px(0.5)),\n    locations = cells_column_labels(columns = c(Percent_Female, Percent_Celebrate_Thanksgiving, Percent_Attended_Friendsgiving, Age_Cutoff_Avg, Household_Income_Avg))\n  ) %&gt;%\n  tab_style(\n    style = cell_borders(sides = c(\"left\", \"right\"), color = \"grey\", weight = px(0.5)),\n    locations = cells_body(columns = c(Percent_Female, Percent_Celebrate_Thanksgiving, Percent_Attended_Friendsgiving, Age_Cutoff_Avg, Household_Income_Avg))\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#B6D382\"),  # Color for the first row\n    locations = cells_body(rows = 1)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#90A9D5\"),  # Color for the second row\n    locations = cells_body(rows = 2)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#D9A2C4\"),  # Color for the third row\n    locations = cells_body(rows = 3)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#77C2BE\"),  # Color for the fourth row\n    locations = cells_body(rows = 4)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#D9A2C4\"),  # Color for the fifth row\n    locations = cells_body(rows = 5)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#77C2BE\"),  # Color for the sixth row\n    locations = cells_body(rows = 6)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#90A9D5\"),  # Color for the seventh row\n    locations = cells_body(rows = 7)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color =  \"#F8D867\"),  # Color for the eighth row\n    locations = cells_body(rows = 8)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#EBA482\"),  # Color for the ninth row\n    locations = cells_body(rows = 9)\n  ) %&gt;%\n  # Add the dollar sign using a custom format\n  tab_style(\n    style = cell_text(align = \"right\"),\n    locations = cells_body(columns = \"Household_Income_Avg\")\n  ) %&gt;%\n  text_transform(\n    locations = cells_body(columns = \"Household_Income_Avg\"),\n    fn = function(x) paste0(\"$\", formatC(x, format = \"f\", big.mark = \",\"))\n  ) %&gt;%\n  tab_caption(\n    caption = \"Table 1. Summary statistics of 931 Americans answering a poll regarding their Thanksgiving traditions and plans, aquired 11/17/2015. Respondants who did not answer their US geographical region are not included. \"\n  )\n\n\n\n\n\nTable 1. Summary statistics of 931 Americans answering a poll regarding their Thanksgiving traditions and plans, aquired 11/17/2015. Respondants who did not answer their US geographical region are not included.\n\n\n\n\n\n\n\n\n\n\nThanksgiving Statistics by US Region\n\n\nRegion\n\nBehavioral Data\n\n\nDemographic Data\n\n\n\nPercent Female\nPercent Who Celebrate Thanksgiving\nPercent Who Have Attended Friendsgiving\nAverage Kids' Table Age Cutoff\nAverage Household Income\n\n\n\n\nEast North Central\n52.67%\n96.67%\n20.69%\n12.8\n$85,808.8\n\n\nEast South Central\n55.00%\n93.33%\n25.00%\n12.9\n$82,053.6\n\n\nMiddle Atlantic\n52.20%\n91.19%\n23.45%\n13.4\n$96,532.8\n\n\nMountain\n68.09%\n87.23%\n46.34%\n14.0\n$87,000.0\n\n\nNew England\n55.17%\n94.83%\n23.64%\n13.5\n$97,606.4\n\n\nPacific\n50.68%\n89.04%\n40.77%\n13.2\n$80,600.8\n\n\nSouth Atlantic\n55.14%\n94.86%\n27.09%\n12.9\n$81,426.6\n\n\nWest North Central\n55.41%\n95.95%\n12.68%\n12.2\n$76,475.4\n\n\nWest South Central\n46.15%\n93.41%\n35.29%\n12.7\n$87,120.3\n\n\n\n\n\n\n\ntesting"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "Fitting Exercise",
    "section": "",
    "text": "To start, I’ll load packages.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(nlmixr2data)\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(here)\n\nhere() starts at /Users/annalisecramer/MADA/annalisecramer-MADA-portfolio-main/annalisecramer-MADA-portfolio\n\n\nLoad the data and explore variable names, then plot. Then write code to make a plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. Stratify by dose (e.g., use a different color for each dose, or facets).\n\ndata &lt;- read_csv(here(\"fitting-exercise\", \"data\", \"Mavoglurant_A2121_nmpk.csv\"))\n\nRows: 2678 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): ID, CMT, EVID, EVI2, MDV, DV, LNDV, AMT, TIME, DOSE, OCC, RATE, AG...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnames(data) #WT=weight, CMP=compartment number, EVID=eventID, MDV=missingDV, AMT=DoseAmountKeyword, DV=dependentvariableMavoglurant, OCC=occasion, HT=height\n\n [1] \"ID\"   \"CMT\"  \"EVID\" \"EVI2\" \"MDV\"  \"DV\"   \"LNDV\" \"AMT\"  \"TIME\" \"DOSE\"\n[11] \"OCC\"  \"RATE\" \"AGE\"  \"SEX\"  \"RACE\" \"WT\"   \"HT\"  \n\nggplot(data = data, aes(x = TIME, y = DV, color = factor(DOSE), group = ID)) +\n  geom_line() +\n  labs(\n    title = \"Mavoglurant concentration over time by dose group\",\n    x = \"Time in hours\",\n    y = \"Mavoglurant Concentration (DV)\",\n    color = \"Dose\")\n\n\n\n\n\n\n\n\nLet us keep patients who only recived one dose.\n\ndata2 &lt;- data %&gt;%\n  filter(OCC == 1)\n\nNext, we will exclude observations with TIME=0, and compute the sum of each DV variable. We will select observations where time=0, too. Data3 combines sums of time \\(\\ne\\) 0 observations and time = 0 observations.\n\ndata_no0 &lt;- data2 %&gt;%\n  filter(TIME != 0) #get observations where time is not 0\n\ndata_sum &lt;- data_no0 %&gt;% #from those non time=0, sum by dose\n  group_by(ID) %&gt;%\n  summarize(Y = sum(DV, na.rm = TRUE)) %&gt;%\n  ungroup()\n\ndata_with0 &lt;- data2 %&gt;%\n  filter(TIME == 0)\n\ndata3 &lt;- left_join(data_sum, data_with0, by=\"ID\")\n\nBelow is code that converts RACE and SEX to factor variables and keeps only certain variables that we want to work with: Y,DOSE,AGE,SEX,RACE,WT,HT\n\ndata4 &lt;- data3 %&gt;%\n  mutate(RACE = as.factor(RACE)) %&gt;% #convert to factors\n  mutate(SEX = as.factor(SEX)) %&gt;%\n  select(Y, DOSE, AGE, SEX, RACE, WT, HT)\n\nhere()\n\n[1] \"/Users/annalisecramer/MADA/annalisecramer-MADA-portfolio-main/annalisecramer-MADA-portfolio\"\n\nsaveRDS(data4, file = here(\"fitting-exercise\", \"data\", \"data4.rds\"))\n\nLet’s explore the data with tables and plots.\nFirst, we’ll make a correlation plot, I used chatGPT to help with this. We can see most of these variables have weak correlations with each other, with the exception of Y/Dose and Weight/Height, both of which have moderately steong negative correlations.\n\ncorr_matrix &lt;- cor(data4[sapply(data4, is.numeric)], use = \"complete.obs\")\n\n# Plot the correlation matrix\ncorrplot(corr_matrix, method = \"circle\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45, \n         main = \"Correlation Plot for data4\")\n\n\n\n\n\n\n\n\nWe will next plot the distributions for our variables to check if they’re reasonable. I used chatGPT to help with this becuase I was having trouble getting the data into the right format (long).\nI think most of these distributions look good. Dose shows a smallers group for the moderate dose, we will keep that in mind. Age shows a that people in their 30s are underrepresented, and perhaps those in their late 20s are overrepresented.\n\n# Select numeric columns only\nnumeric_data4 &lt;- data4 %&gt;% select(where(is.numeric))\n\n# Pivot data to long format for ggplot\nlong_data4 &lt;- pivot_longer(numeric_data4, cols = everything(), names_to = \"Variable\", values_to = \"Value\")\n\n# Plot the distributions\nggplot(long_data4, aes(x = Value)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  facet_wrap(~ Variable, scales = \"free\") +\n  labs(title = \"Distributions of Numeric Variables in data4\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNow, let’s make some scatterplots and boxplots.\nNotes:\n\nNot much trend with Age or Weight\nHeight shows fewer individuals with low height/high dose\nDose doesn’t tell us much that we didn’t already knkow from previous figures\nInterested to know what races 7 and 88 are, I don’t want to mess with them without access to a codebook– could be unknown, don’t know, NA, or multiple, some of those are valid answers\nSex has a few outliers but I am not going to remove them\n\n\n#scatterplots\nggplot(data4, aes(x = AGE, y = Y)) +\n  geom_point(color = \"dodgerblue2\") +\n  labs(title = \"Y vs Age\", x = \"Age\", y = \"Total Drug (Y)\")\n\n\n\n\n\n\n\nggplot(data4, aes(x = WT, y = Y)) +\n  geom_point(color = \"indianred2\") +\n  labs(title = \"Y vs Weight\", x = \"Weight\", y = \"Total Drug (Y)\")\n\n\n\n\n\n\n\nggplot(data4, aes(x = HT, y = Y)) +\n  geom_point(color = \"olivedrab3\") +\n  labs(title = \"Y vs Height\", x = \"Height\", y = \"Total Drug (Y)\")\n\n\n\n\n\n\n\nggplot(data4, aes(x = DOSE, y = Y)) +\n  geom_point(color = \"darkorchid2\") +\n  labs(title = \"Y vs Dose\", x = \"Dose\", y = \"Total Drug (Y)\")\n\n\n\n\n\n\n\n#boxplots\nggplot(data4, aes(x = factor(RACE), y = Y)) +\n  geom_boxplot(fill = \"hotpink2\") +\n  labs(title = \"Y vs Race\", x = \"Race\", y = \"Total Drug (Y)\")\n\n\n\n\n\n\n\nggplot(data4, aes(x = factor(SEX), y = Y)) +\n  geom_boxplot(fill = \"orange2\") +\n  labs(title = \"Y vs Sex\", x = \"Sex\", y = \"Total Drug (Y)\")\n\n\n\n\n\n\n\n\nNext, a summary table with usefule summary statistics. ChatGPT helped with this.\nMeans and medians are pretty close for all categories– this tells me there’s no crazy outliers.\n\nsummary_table &lt;- data4 %&gt;%\n  summarise(across(where(is.numeric), \n                   list(mean = ~ mean(.x, na.rm = TRUE),\n                        median = ~ median(.x, na.rm = TRUE),\n                        min = ~ min(.x, na.rm = TRUE),\n                        max = ~ max(.x, na.rm = TRUE)))) %&gt;%\n  pivot_longer(everything(), names_to = c(\"variable\", \".value\"), names_sep = \"_\") %&gt;%\n  mutate(range = paste0(min, \" - \", max)) %&gt;%\n  select(variable, mean, median, range)\n\nprint(summary_table)\n\n# A tibble: 5 × 4\n  variable    mean  median range                    \n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                    \n1 Y        2445.   2349.   826.43 - 5606.58         \n2 DOSE       36.5    37.5  25 - 50                  \n3 AGE        33      31    18 - 50                  \n4 WT         82.6    82.1  56.6 - 115.3             \n5 HT          1.76    1.77 1.520030662 - 1.930120071\n\n\nWith our exploratory analysis complete, we will begin building models.\nFit a linear model to the continuous outcome (Y) using the main predictor of interest, which we’ll assume here to be DOSE.\n\nmodel1 &lt;- linear_reg()\n\nmodel1_fit &lt;- \n  model1 %&gt;% \n  fit(Y ~ DOSE, data = data4)\n\nmodel1_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ DOSE, data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n     323.06        58.21  \n\n\nFit a linear model to the continuous outcome (Y) using all predictors.\n\nmodel2 &lt;- linear_reg()\n\nmodel2_fit &lt;- \n  model2 %&gt;% \n  fit(Y ~ DOSE + AGE + SEX + RACE + WT + HT, data = data4)\n\nmodel2_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ DOSE + AGE + SEX + RACE + WT + HT, data = data)\n\nCoefficients:\n(Intercept)         DOSE          AGE         SEX2        RACE2        RACE7  \n   3386.863       59.935        3.155     -357.734      155.034     -405.320  \n     RACE88           WT           HT  \n    -53.505      -23.047     -748.487  \n\n\nFor both models, compute RMSE and R-squared and print them.\n\n#predictions for model1\nmodel1_pred &lt;- predict(\n  model1_fit,\n  new_data = data4\n)\n\n#put predictions with data\nmodel1_result &lt;- data4 %&gt;%\n  select(Y, DOSE) %&gt;%\n  bind_cols(model1_pred)\n\n#get RMSE\nmodel1_result %&gt;%\n  yardstick::rmse(\n    truth = Y, \n    estimate = .pred\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        666.\n\n#get R^2\nmodel1_result %&gt;%\n  yardstick::rsq(\n    truth = Y, \n    estimate = .pred\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.516\n\n#do the same for model2 (full model)\n#predictions for model2\nmodel2_pred &lt;- predict(\n  model2_fit,\n  new_data = data4\n)\n\n#put predictions with data, add other vairables\nmodel2_result &lt;- data4 %&gt;%\n  select(Y, DOSE, AGE, SEX, RACE, WT, HT) %&gt;%\n  bind_cols(model2_pred)\n\n#get RMSE\nmodel2_result %&gt;%\n  yardstick::rmse(\n    truth = Y, \n    estimate = .pred\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        591.\n\n#get R^2\nmodel2_result %&gt;%\n  yardstick::rsq(\n    truth = Y, \n    estimate = .pred\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.619\n\n\nWe will make the same reduced and full model, but this time SEX will be the outcome instead of Y.\nFit a logistic model to the categorical/binary outcome (SEX) using the main predictor of interest, which we’ll again assume here to be DOSE.\n\nmodel3 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\nmodel3_fit &lt;- model3 %&gt;%\n  fit(SEX ~ DOSE, data = data4)\n\nFit a logistic model to SEX using all predictors.\n\nmodel4 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\nmodel4_fit &lt;- model4 %&gt;%\n  fit(SEX ~ DOSE + AGE + RACE + HT + WT + Y, data = data4)\n\nFor both models, compute accuracy and ROC-AUC and print them.\n\n#make predicitons for class\nmodel3_pred &lt;- predict(model3_fit, new_data = data4)\nmodel4_pred &lt;- predict(model4_fit, new_data = data4)\n\n#put predictions with data\nmodel3_result &lt;- data4 %&gt;%\n  bind_cols(model3_pred)\nmodel4_result &lt;- data4 %&gt;%\n  bind_cols(model4_pred)\n\n#calculate accuracy\nmodel3_accuracy &lt;- metrics(data = tibble(truth = data4$SEX, predicted = model3_pred$.pred_class), \n                    truth = truth, estimate = predicted)\nmodel4_accuracy &lt;- metrics(data = tibble(truth = data4$SEX, predicted = model4_pred$.pred_class), \n                    truth = truth, estimate = predicted)\n\n#print accuracy\nprint(model3_accuracy)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.867\n2 kap      binary         0    \n\nprint(model4_accuracy)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.942\n2 kap      binary         0.741\n\n#make predictions for non-class\nmodel3_pred &lt;- predict(model3_fit, new_data = data4, type = \"prob\")\nmodel4_pred &lt;- predict(model4_fit, new_data = data4, type = \"prob\")\n\n#calculate ROC_AUC\nmodel3_roc_auc &lt;- roc_auc(data = tibble(truth = data4$SEX,\n                                 .pred_1 = model3_pred$.pred_1), \n                   truth = truth, .pred_1)\n\nmodel4_roc_auc &lt;- roc_auc(data = tibble(truth = data4$SEX, \n                                 .pred_1 = model4_pred$.pred_1), \n                   truth = truth, .pred_1)\n\nprint(model3_roc_auc)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.592\n\nprint(model4_roc_auc)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.980\n\n\nThis section marks the beginning of the modeule 10 exercise.\n\nrngseed = 1234 #use this later to set seed\n\ndata5 &lt;- data4 %&gt;% #remove race vairbale because we won't be using it\n  select(Y, DOSE, AGE, SEX, WT, HT)\n\nset.seed(rngseed) #set seed\n\ndata_split &lt;- initial_split(data5, prop = 3/4) #3/4 of the data is for training\n\ntrain_data &lt;- training(data_split) #create dataframes for the sets\ntest_data  &lt;- testing(data_split) #based on tidymodels instructions\n\n#now we will fit our two models\nmodel_dose &lt;- linear_reg() #linear model with dose as predictor using training data\nmodel_dose_fit &lt;- \n  model_dose %&gt;% \n  fit(Y ~ DOSE, data = train_data)\nmodel_dose_fit \n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ DOSE, data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n     535.45        53.42  \n\nmodel_all &lt;- linear_reg() #linear model with all productors using training data\nmodel_all_fit &lt;- \n  model_all %&gt;% \n  fit(Y ~ DOSE + AGE + SEX + WT + HT, data = train_data)\nmodel_all_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ DOSE + AGE + SEX + WT + HT, data = data)\n\nCoefficients:\n(Intercept)         DOSE          AGE         SEX2           WT           HT  \n  4396.7716      55.3445      -0.4174    -568.9967     -22.6399   -1129.6696  \n\n\nNext, we’ll calculate the RMSE for these two new models and a null model.\nThe null model has a RMSE of 948.35.\nThe model with only dose as a predictor has a RMSE of 702.81.\nThe full model has a RMSE of 627.44. This is the best model given this is the lowest RMSE out of the 3 models.\n\nset.seed(rngseed) #set seed\n\n#predictions for model1\nmodel_dose_pred &lt;- predict(\n  model_dose_fit,\n  new_data = train_data)\n\n#put predictions with data\nmodel_dose_result &lt;- train_data %&gt;%\n  select(Y, DOSE) %&gt;%\n  bind_cols(model_dose_pred)\n\n#get RMSE\nmodel_dose_result %&gt;%\n  yardstick::rmse(\n    truth = Y, \n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        703.\n\n#do the same for the model with all predictors\n#predictions for model2\nmodel_all_pred &lt;- predict(\n  model_all_fit,\n  new_data = train_data)\n\n#put predictions with data, add other vairables\nmodel_all_result &lt;- train_data %&gt;%\n  select(Y, DOSE, AGE, SEX, WT, HT) %&gt;%\n  bind_cols(model_all_pred)\n\n#get RMSE\nmodel_all_result %&gt;%\n  yardstick::rmse(\n    truth = Y, \n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        627.\n\n#I used chatGPT to help with this\n# Define null model\nnull_mod &lt;- null_model() %&gt;%\n  set_engine(\"parsnip\") %&gt;%\n  set_mode(\"regression\")\n\n# Create a workflow without specifying intercept\nnull_wf &lt;- workflow() %&gt;%\n  add_model(null_mod) %&gt;%\n  add_formula(Y ~ .)  # Only Y, no predictors will be used\n\n# Fit the null model\nnull_fit &lt;- fit(null_wf, data = train_data)\n\n# Get predictions from the null model\nnull_preds &lt;- predict(null_fit, train_data)\n\n# Combine predictions with actual data\nnull_preds &lt;- bind_cols(train_data, null_preds)\n\n# Calculate RMSE\nrmse_result &lt;- rmse(null_preds, truth = Y, estimate = .pred)\n\n# Print the result\nprint(rmse_result)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        948.\n\n\nNow, we’re going to do a 10-fold cross-validation with the testing data.\nThe simple CV model has an RMSE of 690.54.\nThe full CV model has an RMSE of 645.69.\nThe cross validation process has the same conclusion as the earlier process– that the full model is best, with lowest RMSE. However, it’s not quite as low, and the single model has a much closer RMSE this time. I’d be interested to see if this holds with even more iterations.\nNext, I’m going to change the seed and rerun, then we’ll see how the results look.\nWith a different seed value, the single model has a RMSE of 691.72.\nThe full model has a RMSE of 633.46.\nThis agrees with earlier results, that the full model has the lowest RMSE. However, the single and full models aren’t quite so close. This makes me feel more confident in these results.\n\nset.seed(1938) #set seed\n\nfolds &lt;- vfold_cv(train_data, v = 10) #set number of folds\nfolds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits         id    \n   &lt;list&gt;         &lt;chr&gt; \n 1 &lt;split [81/9]&gt; Fold01\n 2 &lt;split [81/9]&gt; Fold02\n 3 &lt;split [81/9]&gt; Fold03\n 4 &lt;split [81/9]&gt; Fold04\n 5 &lt;split [81/9]&gt; Fold05\n 6 &lt;split [81/9]&gt; Fold06\n 7 &lt;split [81/9]&gt; Fold07\n 8 &lt;split [81/9]&gt; Fold08\n 9 &lt;split [81/9]&gt; Fold09\n10 &lt;split [81/9]&gt; Fold10\n\n#for dose model\ndose_wf &lt;- \n  workflow() %&gt;%\n  add_model(model_dose) %&gt;%\n  add_formula(Y ~ DOSE)\n\n# 10-fold cv for model_dose\ndose_fit_rs &lt;- \n  dose_wf %&gt;% \n  fit_resamples(folds)\ndose_fit_rs\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits         id     .metrics         .notes          \n   &lt;list&gt;         &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [81/9]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [81/9]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [81/9]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [81/9]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [81/9]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [81/9]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [81/9]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [81/9]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [81/9]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [81/9]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n# Collect metrics for model_dose\ndose_metrics &lt;- collect_metrics(dose_fit_rs)\n\n\n\n\n#for model with all predictors\nall_wf &lt;- \n  workflow() %&gt;%\n  add_model(model_all) %&gt;%\n  add_formula(Y ~ DOSE + WT + AGE + SEX + HT)\n\n# 10-fold cv for model_all\nall_fit_rs &lt;- \n  all_wf %&gt;% \n  fit_resamples(folds)\nall_fit_rs\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits         id     .metrics         .notes          \n   &lt;list&gt;         &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [81/9]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [81/9]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [81/9]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [81/9]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [81/9]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [81/9]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [81/9]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [81/9]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [81/9]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [81/9]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n# Collect metrics for model_all\nall_metrics &lt;- collect_metrics(all_fit_rs)\n\n\n\n\n# Filter RMSE for both models\ndose_rmse &lt;- dose_metrics %&gt;% filter(.metric == \"rmse\")\nall_rmse &lt;- all_metrics %&gt;% filter(.metric == \"rmse\")\n\n# Print RMSE for both models\nprint(dose_rmse)\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    692.    10    63.2 Preprocessor1_Model1\n\nprint(all_rmse)\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    633.    10    61.5 Preprocessor1_Model1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Annie’s website and data analysis portfolio for MADA",
    "section": "",
    "text": "Hello!\nWelcome to my website. Read about me in the About Me tab."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "To start, I’ll load packages.\n#install.packages(\"dslabs\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dslabs)\nThen, check out the package info and data.\nhelp(gapminder)\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\nclass(gapminder) #data frame, nice, I can use tidyverse on it\n\n[1] \"data.frame\"\nafricadata &lt;- gapminder |&gt; #pull out the African countries' data\n  filter(continent == \"Africa\")\nNext, I start making smaller dataframes with only specific variables within them.\ninfants1 &lt;- africadata |&gt; #create object with just infant mortality and life expectancy\n  select(infant_mortality, life_expectancy)\nstr(infants1) #its a data frame\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(infants1)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\npopulation1 &lt;- africadata |&gt; #create object with just population and life expectancy\n  select(population, life_expectancy)\nstr(population1) #its a data frame\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(population1)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51\nI’ll make plots using ggplot. We can see that lower infant mortality is negatively correlated with higher life expectancy. Higher population is positively correlated with life expectancy. The data appears in “streaks” in both plots as this shows year by year data within each African country. I’ll add a second set of plots colored by country to show this more clearly.\nggplot(data=infants1, aes(x=infant_mortality, y=life_expectancy)) +\n  geom_point()\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nggplot(data=population1, aes(x=population, y=life_expectancy)) +\n  geom_point() +\n  scale_x_log10()\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nggplot(data=africadata, aes(x=infant_mortality, y=life_expectancy, col=country)) +\n  geom_point()\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nggplot(data=africadata, aes(x=population, y=life_expectancy, col=country)) +\n  geom_point() +\n  scale_x_log10()\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\nmissing_data &lt;- africadata |&gt; #by year, count number of NAs AKA missings\n  group_by(year) |&gt;\n  summarize(missing = sum(is.na(infant_mortality)))\n\nggplot(data=missing_data, aes(x=year, y=missing)) + #make a bar plot to easily see when/where the NAs are\n  geom_col()\nTo avoid missingness, lets’ pull only rhe data from 2000.\nafricadata2000 &lt;- africadata |&gt; #pull out year 200 only\n  filter(year==2000)\n\nggplot(data=africadata2000, aes(x=infant_mortality, y=life_expectancy, col=country)) + #make same plots as above, but only usiing year 2000\n  geom_point()\n\n\n\n\n\n\n\nggplot(data=africadata2000, aes(x=population, y=life_expectancy, col=country)) +\n  geom_point()\nFinally, I fit two simple linear models to the 2000 data, life expectency explained by infant mortality and then by population. Using alpha=0.05 as a cutoff, I will assume the null hypotheses to be each predictor is not associated with life expectancy. With a p-value of 2.83e-08, we reject the null hypothesis to conclude that infant moratlity is a significant predictor of life expectancy in African countries in 2000. With a p-value of 0.616, we fail to reject the null hypothesis to conlcude that population is not a significant predictor of life expectency in African countries in 2000.\nfit1 &lt;- lm(data=africadata2000, life_expectancy~infant_mortality)\nfit2 &lt;- lm(data=africadata2000, life_expectancy~population)\n\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = africadata2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#this-section-contributed-by-mohammed-zuber",
    "href": "coding-exercise/coding-exercise.html#this-section-contributed-by-mohammed-zuber",
    "title": "R Coding Exercise",
    "section": "This section contributed by Mohammed Zuber",
    "text": "This section contributed by Mohammed Zuber"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#loading-and-checking-data",
    "href": "coding-exercise/coding-exercise.html#loading-and-checking-data",
    "title": "R Coding Exercise",
    "section": "Loading and Checking Data",
    "text": "Loading and Checking Data\n\n# help() function pulls up the help page for the data to see what it contains.\nhelp(research_funding_rates) \n# str () give us the overview of the datasets\nstr(research_funding_rates) \n\n'data.frame':   9 obs. of  10 variables:\n $ discipline         : chr  \"Chemical sciences\" \"Physical sciences\" \"Physics\" \"Humanities\" ...\n $ applications_total : num  122 174 76 396 251 183 282 834 505\n $ applications_men   : num  83 135 67 230 189 105 156 425 245\n $ applications_women : num  39 39 9 166 62 78 126 409 260\n $ awards_total       : num  32 35 20 65 43 29 56 112 75\n $ awards_men         : num  22 26 18 33 30 12 38 65 46\n $ awards_women       : num  10 9 2 32 13 17 18 47 29\n $ success_rates_total: num  26.2 20.1 26.3 16.4 17.1 15.8 19.9 13.4 14.9\n $ success_rates_men  : num  26.5 19.3 26.9 14.3 15.9 11.4 24.4 15.3 18.8\n $ success_rates_women: num  25.6 23.1 22.2 19.3 21 21.8 14.3 11.5 11.2\n\n# summary () gives summary of data\nsummary(research_funding_rates) \n\n  discipline        applications_total applications_men applications_women\n Length:9           Min.   : 76.0      Min.   : 67.0    Min.   :  9       \n Class :character   1st Qu.:174.0      1st Qu.:105.0    1st Qu.: 39       \n Mode  :character   Median :251.0      Median :156.0    Median : 78       \n                    Mean   :313.7      Mean   :181.7    Mean   :132       \n                    3rd Qu.:396.0      3rd Qu.:230.0    3rd Qu.:166       \n                    Max.   :834.0      Max.   :425.0    Max.   :409       \n  awards_total      awards_men     awards_women   success_rates_total\n Min.   : 20.00   Min.   :12.00   Min.   : 2.00   Min.   :13.4       \n 1st Qu.: 32.00   1st Qu.:22.00   1st Qu.:10.00   1st Qu.:15.8       \n Median : 43.00   Median :30.00   Median :17.00   Median :17.1       \n Mean   : 51.89   Mean   :32.22   Mean   :19.67   Mean   :18.9       \n 3rd Qu.: 65.00   3rd Qu.:38.00   3rd Qu.:29.00   3rd Qu.:20.1       \n Max.   :112.00   Max.   :65.00   Max.   :47.00   Max.   :26.3       \n success_rates_men success_rates_women\n Min.   :11.4      Min.   :11.20      \n 1st Qu.:15.3      1st Qu.:14.30      \n Median :18.8      Median :21.00      \n Mean   :19.2      Mean   :18.89      \n 3rd Qu.:24.4      3rd Qu.:22.20      \n Max.   :26.9      Max.   :25.60      \n\n#class() function to check what type of object research_funding_rates is\nclass(research_funding_rates) # obtaining the type of object research_funding_rates is\n\n[1] \"data.frame\""
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-data",
    "href": "coding-exercise/coding-exercise.html#processing-data",
    "title": "R Coding Exercise",
    "section": "Processing Data",
    "text": "Processing Data\n\n# Filtering the research funding data (I am not subsetting any data since the dataset is already very small)\nfundingdata &lt;- research_funding_rates \n\n# Getting an overview of the data structure\nstr(fundingdata) \n\n'data.frame':   9 obs. of  10 variables:\n $ discipline         : chr  \"Chemical sciences\" \"Physical sciences\" \"Physics\" \"Humanities\" ...\n $ applications_total : num  122 174 76 396 251 183 282 834 505\n $ applications_men   : num  83 135 67 230 189 105 156 425 245\n $ applications_women : num  39 39 9 166 62 78 126 409 260\n $ awards_total       : num  32 35 20 65 43 29 56 112 75\n $ awards_men         : num  22 26 18 33 30 12 38 65 46\n $ awards_women       : num  10 9 2 32 13 17 18 47 29\n $ success_rates_total: num  26.2 20.1 26.3 16.4 17.1 15.8 19.9 13.4 14.9\n $ success_rates_men  : num  26.5 19.3 26.9 14.3 15.9 11.4 24.4 15.3 18.8\n $ success_rates_women: num  25.6 23.1 22.2 19.3 21 21.8 14.3 11.5 11.2\n\n# Summary of fundingdata\nsummary(fundingdata) \n\n  discipline        applications_total applications_men applications_women\n Length:9           Min.   : 76.0      Min.   : 67.0    Min.   :  9       \n Class :character   1st Qu.:174.0      1st Qu.:105.0    1st Qu.: 39       \n Mode  :character   Median :251.0      Median :156.0    Median : 78       \n                    Mean   :313.7      Mean   :181.7    Mean   :132       \n                    3rd Qu.:396.0      3rd Qu.:230.0    3rd Qu.:166       \n                    Max.   :834.0      Max.   :425.0    Max.   :409       \n  awards_total      awards_men     awards_women   success_rates_total\n Min.   : 20.00   Min.   :12.00   Min.   : 2.00   Min.   :13.4       \n 1st Qu.: 32.00   1st Qu.:22.00   1st Qu.:10.00   1st Qu.:15.8       \n Median : 43.00   Median :30.00   Median :17.00   Median :17.1       \n Mean   : 51.89   Mean   :32.22   Mean   :19.67   Mean   :18.9       \n 3rd Qu.: 65.00   3rd Qu.:38.00   3rd Qu.:29.00   3rd Qu.:20.1       \n Max.   :112.00   Max.   :65.00   Max.   :47.00   Max.   :26.3       \n success_rates_men success_rates_women\n Min.   :11.4      Min.   :11.20      \n 1st Qu.:15.3      1st Qu.:14.30      \n Median :18.8      Median :21.00      \n Mean   :19.2      Mean   :18.89      \n 3rd Qu.:24.4      3rd Qu.:22.20      \n Max.   :26.9      Max.   :25.60      \n\n\n\n# Load necessary package\nlibrary(dplyr)\n\n# Create proportion_women_applicant and success rate difference\nfundingdata &lt;- research_funding_rates %&gt;%\n  mutate(\n    success_rate_difference = success_rates_men - success_rates_women, # Difference in success rates\n    proportion_women_applicants = applications_women / applications_total # Proportion of female applicants\n  )\n\n\n# Check if the new variables exist\nstr(fundingdata)\n\n'data.frame':   9 obs. of  12 variables:\n $ discipline                 : chr  \"Chemical sciences\" \"Physical sciences\" \"Physics\" \"Humanities\" ...\n $ applications_total         : num  122 174 76 396 251 183 282 834 505\n $ applications_men           : num  83 135 67 230 189 105 156 425 245\n $ applications_women         : num  39 39 9 166 62 78 126 409 260\n $ awards_total               : num  32 35 20 65 43 29 56 112 75\n $ awards_men                 : num  22 26 18 33 30 12 38 65 46\n $ awards_women               : num  10 9 2 32 13 17 18 47 29\n $ success_rates_total        : num  26.2 20.1 26.3 16.4 17.1 15.8 19.9 13.4 14.9\n $ success_rates_men          : num  26.5 19.3 26.9 14.3 15.9 11.4 24.4 15.3 18.8\n $ success_rates_women        : num  25.6 23.1 22.2 19.3 21 21.8 14.3 11.5 11.2\n $ success_rate_difference    : num  0.9 -3.8 4.7 -5 -5.1 ...\n $ proportion_women_applicants: num  0.32 0.224 0.118 0.419 0.247 ...\n\n# Creating two new objects for further analysis\nfunding_variable_one &lt;- fundingdata %&gt;% select(applications_total, success_rates_total)\n\n# Obtaining the structure of funding_variable_one\nstr(funding_variable_one) \n\n'data.frame':   9 obs. of  2 variables:\n $ applications_total : num  122 174 76 396 251 183 282 834 505\n $ success_rates_total: num  26.2 20.1 26.3 16.4 17.1 15.8 19.9 13.4 14.9\n\n# Summarizing the details about funding_variable_one\nsummary(funding_variable_one) \n\n applications_total success_rates_total\n Min.   : 76.0      Min.   :13.4       \n 1st Qu.:174.0      1st Qu.:15.8       \n Median :251.0      Median :17.1       \n Mean   :313.7      Mean   :18.9       \n 3rd Qu.:396.0      3rd Qu.:20.1       \n Max.   :834.0      Max.   :26.3       \n\n# Creating another dataset with only two variables as listed\nfunding_variable_two &lt;- fundingdata %&gt;% select(proportion_women_applicants, success_rate_difference)\n\n# Obtaining the structure of funding_variable_two\nstr(funding_variable_two) \n\n'data.frame':   9 obs. of  2 variables:\n $ proportion_women_applicants: num  0.32 0.224 0.118 0.419 0.247 ...\n $ success_rate_difference    : num  0.9 -3.8 4.7 -5 -5.1 ...\n\n# Obtaining the summary about the dataset funding_variable_two\nsummary(funding_variable_two)  \n\n proportion_women_applicants success_rate_difference\n Min.   :0.1184              Min.   :-10.4000       \n 1st Qu.:0.2470              1st Qu.: -5.0000       \n Median :0.4192              Median :  0.9000       \n Mean   :0.3563              Mean   :  0.3111       \n 3rd Qu.:0.4468              3rd Qu.:  4.7000       \n Max.   :0.5149              Max.   : 10.1000"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#plotting",
    "href": "coding-exercise/coding-exercise.html#plotting",
    "title": "R Coding Exercise",
    "section": "Plotting",
    "text": "Plotting\n\nFigure 1 Scatterplot of Total Applications vs. Success Rate\nThe scatterplot shows a negative trend, where disciplines with more applications tend to have lower success rates. Fields with fewer applications have higher success rates, possibly due to lower competition. In contrast, disciplines with over 500 applications see success rates drop below 15%, likely due to increased competition for limited funding. One outlier with 800+ applications and the lowest success rate suggests that some fields are highly competitive with stricter selection criteria.\n\n# Scatterplot of Total Applications vs. Success Rate\nggplot(funding_variable_one, aes(x = applications_total, y = success_rates_total)) + \n  geom_point(color = \"blue\", alpha = 0.7) +\n  labs(\n    title = \"Total Applications vs. Success Rate\",\n    x = \"Total Applications\",\n    y = \"Overall Success Rate\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 2: Proportion of Female Applicants vs. Gender Success Rate Difference\nThe scatterplot shows no clear correlation between the proportion of female applicants and the gender success rate difference, as points are scattered across both positive and negative values. Some disciplines with higher female representation still exhibit gender disparities in success rates. In certain fields, men have a higher success rate, while in others, women do better, indicating variability across disciplines. This suggests that factors beyond applicant proportion, such as funding policies or selection criteria, may influence gender differences in research funding success.\n\n#Scatterplot Proportion of Female Applicants vs. Gender Success Rate Difference\nggplot(funding_variable_two, aes(x = proportion_women_applicants, y = success_rate_difference)) + \n  geom_point(color = \"red\", alpha = 0.7) +\n  labs(\n    title = \"Proportion of Female Applicants vs. Gender Success Rate Difference\",\n    x = \"Proportion of Female Applicants\",\n    y = \"Success Rate Difference (Men - Women)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nTable 1: Top Disciplines Based on Total Applications\n\n# Extracting the top disciplines based on total applications\ntop_disciplines &lt;- fundingdata %&gt;%\n  select(discipline, applications_total, awards_total, awards_men, awards_women) %&gt;%\n  arrange(desc(applications_total)) %&gt;%  # Sorting by total applications\n  head(5)  # Displaying top 5 disciplines\n\n# Print the summary table\nprint(top_disciplines)\n\n           discipline applications_total awards_total awards_men awards_women\n1     Social sciences                834          112         65           47\n2    Medical sciences                505           75         46           29\n3          Humanities                396           65         33           32\n4 Earth/life sciences                282           56         38           18\n5  Technical sciences                251           43         30           13\n\n\n\n\nFigure 3: Total awards By Discipline\n\n# Creating a bar plot for total awards by discipline \nggplot(fundingdata, aes(x = reorder(discipline, awards_total), y = awards_total)) + \n  geom_bar(stat = \"identity\", fill = \"steelblue\", color = \"black\") + \n  labs(\n    title = \"Total Awards by Discipline\",\n    x = \"Discipline\",\n    y = \"Total Awards\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate text for better readability"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#fitting-a-linear-model",
    "href": "coding-exercise/coding-exercise.html#fitting-a-linear-model",
    "title": "R Coding Exercise",
    "section": "Fitting a linear model",
    "text": "Fitting a linear model\nIn the first model, the p-value is less than 0.05 (0.0197). Therefore, this suggests that the number of applications is a significant predictor of success rates. The negative coefficient (-0.0149) indicates that as the number of applications increases, the success rate decreases, likely due to increased competition. The R-squared of 0.564 means that 56.4% of the variation in success rates is explained by the number of applications.\nIn the second model, the p-value is greater than 0.05 (0.6013). Therefore, there is no strong statistical evidence to suggest that the proportion of female applicants is a significant predictor of the gender success rate difference. The R-squared of 0.041 means that only 4.1% of the variation in gender success rate differences is explained by the proportion of female applicants, which is weak.\n\n# Linear model: Success Rate vs. Total Applications\nfit1 &lt;- lm(success_rates_total ~ applications_total, data = fundingdata)\n# Linear model: Success Rate Difference vs. Proportion of Female Applicants\nfit2 &lt;- lm(success_rate_difference ~ proportion_women_applicants, data = fundingdata)\n# Summary of the model\nsummary(fit1)\n\n\nCall:\nlm(formula = success_rates_total ~ applications_total, data = fundingdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0423 -1.2761 -0.8761  2.2346  4.4509 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        23.562590   1.901882  12.389 5.13e-06 ***\napplications_total -0.014865   0.004939  -3.009   0.0197 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.309 on 7 degrees of freedom\nMultiple R-squared:  0.564, Adjusted R-squared:  0.5018 \nF-statistic: 9.057 on 1 and 7 DF,  p-value: 0.01968\n\nsummary(fit2)\n\n\nCall:\nlm(formula = success_rate_difference ~ proportion_women_applicants, \n    data = fundingdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.4201  -4.3029   0.9603   5.6813   8.8712 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                   -3.302      7.017  -0.471    0.652\nproportion_women_applicants   10.140     18.534   0.547    0.601\n\nResidual standard error: 7.118 on 7 degrees of freedom\nMultiple R-squared:  0.041, Adjusted R-squared:  -0.096 \nF-statistic: 0.2993 on 1 and 7 DF,  p-value: 0.6013"
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "To start, I’ll load packages.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nHere’s the prompt I gave chatGPT:\nWrite me code for R that generates data. I want the data to have 500 people who are in a clinical trial for a cancer drug. There should be roughly the same amount of males and females. Ages are 60-90, with more people in the mid to late 70s. Average BMI should be 28 for males, 30 for females. Age and BMI are slightly correlated, as age goes up, BMI slightly goes down. The study takes place over 2 years, with checkpoints every 3 months. There are 3 treatment groups, randomly assigned: placebo, 10mg, 20mg dose. Include a tumor variable that is measured in mm. Over time, the 20mg treatment group should show significant reduction in tumor size, the 10mg tumor group showing tumor staying the same size or slightly shrinking, and the placebo group can show anything in tumor size. Add a patient ID for each individual patient.\n\nset.seed(123)  # For reproducibility\n\n# Number of participants\nn &lt;- 500  \n\n# Generate patient IDs\npatient_ids &lt;- sprintf(\"P%03d\", 1:n)  # P001, P002, ..., P500\n\n# Generate gender\ngender &lt;- sample(c(\"Male\", \"Female\"), n, replace = TRUE)\n\n# Generate ages (60-90) with more in mid to late 70s\nages &lt;- round(rnorm(n, mean = 76, sd = 6))\nages &lt;- pmax(pmin(ages, 90), 60)  # Keep within bounds\n\n# Generate BMI (correlated with age, decreasing slightly as age increases)\nbmi_male &lt;- round(rnorm(n, mean = 28 - (ages - 76) * 0.1, sd = 2), 1)\nbmi_female &lt;- round(rnorm(n, mean = 30 - (ages - 76) * 0.1, sd = 2), 1)\nbmi &lt;- ifelse(gender == \"Male\", bmi_male, bmi_female)\n\n# Treatment groups (random assignment)\ntreatment &lt;- sample(c(\"Placebo\", \"10mg\", \"20mg\"), n, replace = TRUE)\n\n# Number of time points (every 3 months for 2 years)\ntime_points &lt;- seq(0, 24, by = 3)  # Months\n\n# Baseline tumor size (randomly generated between 20mm-60mm)\nbaseline_tumor &lt;- round(runif(n, min = 20, max = 60), 1)\n\n# Function to model tumor size change based on treatment\ntumor_reduction &lt;- function(base_size, treatment, time) {\n  if (treatment == \"Placebo\") {\n    return(base_size + rnorm(1, mean = 0, sd = 5))  # Tumor size fluctuates\n  } else if (treatment == \"10mg\") {\n    return(base_size * (1 - 0.005 * time) + rnorm(1, mean = 0, sd = 2))  # Small reduction\n  } else {  # 20mg group\n    return(base_size * (1 - 0.02 * time) + rnorm(1, mean = 0, sd = 2))  # Significant reduction\n  }\n}\n\n# Create dataset with repeated measures\ntrial_data &lt;- data.frame()\nfor (i in 1:n) {\n  for (t in time_points) {\n    new_row &lt;- data.frame(\n      Patient_ID = patient_ids[i],\n      Gender = gender[i],\n      Age = ages[i],\n      BMI = bmi[i],\n      Treatment = treatment[i],\n      Time = t,\n      Tumor_Size = max(tumor_reduction(baseline_tumor[i], treatment[i], t), 0)  # Ensure no negative values\n    )\n    trial_data &lt;- rbind(trial_data, new_row)\n  }\n}\n\n# View first few rows\nhead(trial_data)\n\n  Patient_ID Gender Age  BMI Treatment Time Tumor_Size\n1       P001   Male  74 31.3      20mg    0   55.43543\n2       P001   Male  74 31.3      20mg    3   48.42171\n3       P001   Male  74 31.3      20mg    6   49.58208\n4       P001   Male  74 31.3      20mg    9   43.98880\n5       P001   Male  74 31.3      20mg   12   42.12862\n6       P001   Male  74 31.3      20mg   15   37.27120\n\n# Save to CSV\n#write.csv(trial_data, \"clinical_trial_data.csv\", row.names = FALSE)\n\nOkay. First to test this code, I’m going to make a plot to look at the gender distribution, including making a dataset of only the starting values. They look alright.\n\nunique(trial_data$Gender)\n\n[1] \"Male\"   \"Female\"\n\nstart &lt;- trial_data %&gt;%\n  filter(Time==0)\nggplot(data=start, aes(x=Gender)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNow, let’s look at BMI at the start of the trial between males and females, and then at the end of the trial. It doesn’t look like it changes over time. Looks like the code given by AI kept everyone’s weight exactly the same, which isn’t totally realistic. However, I didn’t specify it to do this. We can see that it does show at the start of the trial that older people have slightly lower BMIs in both genders than younger people, as I asked.\n\nunique(trial_data$Time)\n\n[1]  0  3  6  9 12 15 18 21 24\n\nend &lt;- trial_data %&gt;%\n  filter(Time==24)\nggplot(data=start, aes(x=BMI, col=Gender)) +\n  geom_density(size=2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nggplot(data=end, aes(x=BMI, col=Gender)) +\n  geom_density(size=2)\n\n\n\n\n\n\n\nggplot(data = trial_data, aes(x = Time, y = BMI, group = Patient_ID, color = Gender)) +\n  geom_line(alpha = 0.2) +  # Individual spaghetti lines with transparency\n  stat_summary(fun = mean, geom = \"line\", aes(group = Gender), size = 1.2, color = \"black\") +  # Mean lines\n  stat_summary(fun = mean, geom = \"point\", aes(group = Gender), size = 3, color = \"black\")  # Mean points\n\n\n\n\n\n\n\nggplot(data = start, aes(x = Age, y = BMI, color = Gender)) +\n  geom_point(alpha = 0.6) +  # Scatter plot with transparency\n  geom_smooth(method = \"lm\", aes(group = Gender), se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNext, let’s look at the distribution of treatment groups. They don’t look perfectly even, but I asked for them to be randomly assignened and I guess I should have specified I meant for them to be even.\n\nggplot(data=start, aes(x=Treatment, fill=Treatment)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNow, I’ll look at the distribution of tumor size by treatment group. Looks good.\n\nggplot(data=start, aes(x=Treatment, y=Tumor_Size, fill=Treatment)) +\n  geom_violin()\n\n\n\n\n\n\n\nggplot(data=end, aes(x=Treatment, y=Tumor_Size, fill=Treatment)) +\n  geom_violin()\n\n\n\n\n\n\n\n\nNext I will make a linear model to see if treatment group has an effect on tumor size. All parameters have p-values below an significance level of alpha=0.05, so we conclude that treatment group is a significant predictor of tumor size.\n\nmodel1 &lt;- lm(data=trial_data, Tumor_Size~Treatment)\nsummary(model1)\n\n\nCall:\nlm(formula = Tumor_Size ~ Treatment, data = trial_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.513  -8.991  -0.547   8.725  33.123 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       37.0048     0.2808 131.789  &lt; 2e-16 ***\nTreatment20mg     -7.0365     0.4023 -17.491  &lt; 2e-16 ***\nTreatmentPlacebo   2.0495     0.4172   4.912 9.33e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.3 on 4497 degrees of freedom\nMultiple R-squared:  0.1049,    Adjusted R-squared:  0.1045 \nF-statistic: 263.5 on 2 and 4497 DF,  p-value: &lt; 2.2e-16\n\n\nI’m going to build a model that explores whther gender is a predictor for tumor size. I didn’t ask the data to contain this association, so we shouldn’t see a significant p-value. With a p-value of 0.434, we see that Gender is found to be a significant predictor of Tumor Size. Interesting.\n\nmodel2 &lt;- lm(data=trial_data, Tumor_Size~Gender)\nsummary(model2)\n\n\nCall:\nlm(formula = Tumor_Size ~ Gender, data = trial_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.456  -9.398  -0.489   9.040  36.625 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  34.8333     0.2574  135.32   &lt;2e-16 ***\nGenderMale    0.7197     0.3563    2.02   0.0434 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.94 on 4498 degrees of freedom\nMultiple R-squared:  0.0009063, Adjusted R-squared:  0.0006842 \nF-statistic:  4.08 on 1 and 4498 DF,  p-value: 0.04345"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
<<<<<<< HEAD
    "text": "Hi! My name is Annalise, but everyone calls me Annie. I use she/her pronouns.\nI’m a second year PhD student in the Epidemiology and Biostatistics Department. I work with Dr. Spencer Fox, and I broadly do stuff related to flu evolution– currently, we’re working on developing county by county risk for H5N1 based on childhood immunological imprinting. It’s a work in progress :)"
=======
    "text": "To start, I’ll load packages.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.0     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(nlmixr2data)\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(here)\n\nhere() starts at C:/Users/mz13171/Downloads/MADA-course/annalisecramer-MADA-portfolio\n\n\nLoad the data and explore variable names, then plot. Then write code to make a plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. Stratify by dose (e.g., use a different color for each dose, or facets).\n\ndata &lt;- read_csv(here(\"fitting-exercise\", \"data\", \"Mavoglurant_A2121_nmpk.csv\"))\n\nRows: 2678 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): ID, CMT, EVID, EVI2, MDV, DV, LNDV, AMT, TIME, DOSE, OCC, RATE, AG...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnames(data) #WT=weight, CMP=compartment number, EVID=eventID, MDV=missingDV, AMT=DoseAmountKeyword, DV=dependentvariableMavoglurant, OCC=occasion, HT=height\n\n [1] \"ID\"   \"CMT\"  \"EVID\" \"EVI2\" \"MDV\"  \"DV\"   \"LNDV\" \"AMT\"  \"TIME\" \"DOSE\"\n[11] \"OCC\"  \"RATE\" \"AGE\"  \"SEX\"  \"RACE\" \"WT\"   \"HT\"  \n\nggplot(data = data, aes(x = TIME, y = DV, color = factor(DOSE), group = ID)) +\n  geom_line() +\n  labs(\n    title = \"Mavoglurant concentration over time by dose group\",\n    x = \"Time in hours\",\n    y = \"Mavoglurant Concentration (DV)\",\n    color = \"Dose\")\n\n\n\n\n\n\n\n\nLet us keep patients who only recived one dose.\n\ndata2 &lt;- data %&gt;%\n  filter(OCC == 1)\n\nNext, we will exclude observations with TIME=0, and compute the sum of each DV variable. We will select observations where time=0, too. Data3 combines sums of time \\(\\ne\\) 0 observations and time = 0 observations.\n\ndata_no0 &lt;- data2 %&gt;%\n  filter(TIME != 0) #get observations where time is not 0\n\ndata_sum &lt;- data_no0 %&gt;% #from those non time=0, sum by dose\n  group_by(ID) %&gt;%\n  summarize(Y = sum(DV, na.rm = TRUE)) %&gt;%\n  ungroup()\n\ndata_with0 &lt;- data2 %&gt;%\n  filter(TIME == 0)\n\ndata3 &lt;- left_join(data_sum, data_with0, by=\"ID\")\n\nBelow is code that converts RACE and SEX to factor variables and keeps only certain variables that we want to work with: Y,DOSE,AGE,SEX,RACE,WT,HT\n\ndata4 &lt;- data3 %&gt;%\n  mutate(RACE = as.factor(RACE)) %&gt;% #convert to factors\n  mutate(SEX = as.factor(SEX)) %&gt;%\n  select(Y, DOSE, AGE, SEX, RACE, WT, HT)\n\nLet’s explore the data with tables and plots.\nFirst, we’ll make a correlation plot, I used chatGPT to help with this. We can see most of these variables have weak correlations with each other, with the exception of Y/Dose and Weight/Height, both of which have moderately steong negative correlations.\n\ncorr_matrix &lt;- cor(data4[sapply(data4, is.numeric)], use = \"complete.obs\")\n\n# Plot the correlation matrix\ncorrplot(corr_matrix, method = \"circle\", type = \"upper\", \n         tl.col = \"black\", tl.srt = 45, \n         main = \"Correlation Plot for data4\")\n\n\n\n\n\n\n\n\nWe will next plot the distributions for our variables to check if they’re reasonable. I used chatGPT to help with this becuase I was having trouble getting the data into the right format (long).\nI think most of these distributions look good. Dose shows a smallers group for the moderate dose, we will keep that in mind. Age shows a that people in their 30s are underrepresented, and perhaps those in their late 20s are overrepresented.\n\n# Select numeric columns only\nnumeric_data4 &lt;- data4 %&gt;% select(where(is.numeric))\n\n# Pivot data to long format for ggplot\nlong_data4 &lt;- pivot_longer(numeric_data4, cols = everything(), names_to = \"Variable\", values_to = \"Value\")\n\n# Plot the distributions\nggplot(long_data4, aes(x = Value)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  facet_wrap(~ Variable, scales = \"free\") +\n  labs(title = \"Distributions of Numeric Variables in data4\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNow, let’s make some scatterplots and boxplots.\nNotes:\n\nNot much trend with Age or Weight\nHeight shows fewer individuals with low height/high dose\nDose doesn’t tell us much that we didn’t already knkow from previous figures\nInterested to know what races 7 and 88 are, I don’t want to mess with them without access to a codebook– could be unknown, don’t know, NA, or multiple, some of those are valid answers\nSex has a few outliers but I am not going to remove them\n\n\n#scatterplots\nggplot(data4, aes(x = AGE, y = Y)) +\n  geom_point(color = \"dodgerblue2\") +\n  labs(title = \"Y vs Age\", x = \"Age\", y = \"Total Drug (Y)\")\n\n\n\n\n\n\n\nggplot(data4, aes(x = WT, y = Y)) +\n  geom_point(color = \"indianred2\") +\n  labs(title = \"Y vs Weight\", x = \"Weight\", y = \"Total Drug (Y)\")\n\n\n\n\n\n\n\nggplot(data4, aes(x = HT, y = Y)) +\n  geom_point(color = \"olivedrab3\") +\n  labs(title = \"Y vs Height\", x = \"Height\", y = \"Total Drug (Y)\")\n\n\n\n\n\n\n\nggplot(data4, aes(x = DOSE, y = Y)) +\n  geom_point(color = \"darkorchid2\") +\n  labs(title = \"Y vs Dose\", x = \"Dose\", y = \"Total Drug (Y)\")\n\n\n\n\n\n\n\n#boxplots\nggplot(data4, aes(x = factor(RACE), y = Y)) +\n  geom_boxplot(fill = \"hotpink2\") +\n  labs(title = \"Y vs Race\", x = \"Race\", y = \"Total Drug (Y)\")\n\n\n\n\n\n\n\nggplot(data4, aes(x = factor(SEX), y = Y)) +\n  geom_boxplot(fill = \"orange2\") +\n  labs(title = \"Y vs Sex\", x = \"Sex\", y = \"Total Drug (Y)\")\n\n\n\n\n\n\n\n\nNext, a summary table with usefule summary statistics. ChatGPT helped with this.\nMeans and medians are pretty close for all categories– this tells me there’s no crazy outliers.\n\nsummary_table &lt;- data4 %&gt;%\n  summarise(across(where(is.numeric), \n                   list(mean = ~ mean(.x, na.rm = TRUE),\n                        median = ~ median(.x, na.rm = TRUE),\n                        min = ~ min(.x, na.rm = TRUE),\n                        max = ~ max(.x, na.rm = TRUE)))) %&gt;%\n  pivot_longer(everything(), names_to = c(\"variable\", \".value\"), names_sep = \"_\") %&gt;%\n  mutate(range = paste0(min, \" - \", max)) %&gt;%\n  select(variable, mean, median, range)\n\nprint(summary_table)\n\n# A tibble: 5 × 4\n  variable    mean  median range                    \n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                    \n1 Y        2445.   2349.   826.43 - 5606.58         \n2 DOSE       36.5    37.5  25 - 50                  \n3 AGE        33      31    18 - 50                  \n4 WT         82.6    82.1  56.6 - 115.3             \n5 HT          1.76    1.77 1.520030662 - 1.930120071\n\n\nWith our exploratory analysis complete, we will begin building models.\nFit a linear model to the continuous outcome (Y) using the main predictor of interest, which we’ll assume here to be DOSE.\n\nmodel1 &lt;- linear_reg()\n\nmodel1_fit &lt;- \n  model1 %&gt;% \n  fit(Y ~ DOSE, data = data4)\n\nmodel1_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ DOSE, data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n     323.06        58.21  \n\n\nFit a linear model to the continuous outcome (Y) using all predictors.\n\nmodel2 &lt;- linear_reg()\n\nmodel2_fit &lt;- \n  model2 %&gt;% \n  fit(Y ~ DOSE + AGE + SEX + RACE + WT + HT, data = data4)\n\nmodel2_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ DOSE + AGE + SEX + RACE + WT + HT, data = data)\n\nCoefficients:\n(Intercept)         DOSE          AGE         SEX2        RACE2        RACE7  \n   3386.863       59.935        3.155     -357.734      155.034     -405.320  \n     RACE88           WT           HT  \n    -53.505      -23.047     -748.487  \n\n\nFor both models, compute RMSE and R-squared and print them.\n\n#predictions for model1\nmodel1_pred &lt;- predict(\n  model1_fit,\n  new_data = data4\n)\n\n#put predictions with data\nmodel1_result &lt;- data4 %&gt;%\n  select(Y, DOSE) %&gt;%\n  bind_cols(model1_pred)\n\n#get RMSE\nmodel1_result %&gt;%\n  yardstick::rmse(\n    truth = Y, \n    estimate = .pred\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        666.\n\n#get R^2\nmodel1_result %&gt;%\n  yardstick::rsq(\n    truth = Y, \n    estimate = .pred\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.516\n\n#do the same for model2 (full model)\n#predictions for model2\nmodel2_pred &lt;- predict(\n  model2_fit,\n  new_data = data4\n)\n\n#put predictions with data, add other vairables\nmodel2_result &lt;- data4 %&gt;%\n  select(Y, DOSE, AGE, SEX, RACE, WT, HT) %&gt;%\n  bind_cols(model2_pred)\n\n#get RMSE\nmodel2_result %&gt;%\n  yardstick::rmse(\n    truth = Y, \n    estimate = .pred\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        591.\n\n#get R^2\nmodel2_result %&gt;%\n  yardstick::rsq(\n    truth = Y, \n    estimate = .pred\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.619\n\n\nWe will make the same reduced and full model, but this time SEX will be the outcome instead of Y.\nFit a logistic model to the categorical/binary outcome (SEX) using the main predictor of interest, which we’ll again assume here to be DOSE.\n\nmodel3 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\nmodel3_fit &lt;- model3 %&gt;%\n  fit(SEX ~ DOSE, data = data4)\n\nFit a logistic model to SEX using all predictors.\n\nmodel4 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\nmodel4_fit &lt;- model4 %&gt;%\n  fit(SEX ~ DOSE + AGE + RACE + HT + WT + Y, data = data4)\n\nFor both models, compute accuracy and ROC-AUC and print them.\n\n#make predicitons for class\nmodel3_pred &lt;- predict(model3_fit, new_data = data4)\nmodel4_pred &lt;- predict(model4_fit, new_data = data4)\n\n#put predictions with data\nmodel3_result &lt;- data4 %&gt;%\n  bind_cols(model3_pred)\nmodel4_result &lt;- data4 %&gt;%\n  bind_cols(model4_pred)\n\n#calculate accuracy\nmodel3_accuracy &lt;- metrics(data = tibble(truth = data4$SEX, predicted = model3_pred$.pred_class), \n                    truth = truth, estimate = predicted)\nmodel4_accuracy &lt;- metrics(data = tibble(truth = data4$SEX, predicted = model4_pred$.pred_class), \n                    truth = truth, estimate = predicted)\n\n#print accuracy\nprint(model3_accuracy)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.867\n2 kap      binary         0    \n\nprint(model4_accuracy)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.942\n2 kap      binary         0.741\n\n#make predictions for non-class\nmodel3_pred &lt;- predict(model3_fit, new_data = data4, type = \"prob\")\nmodel4_pred &lt;- predict(model4_fit, new_data = data4, type = \"prob\")\n\n#calculate ROC_AUC\nmodel3_roc_auc &lt;- roc_auc(data = tibble(truth = data4$SEX,\n                                 .pred_1 = model3_pred$.pred_1), \n                   truth = truth, .pred_1)\n\nmodel4_roc_auc &lt;- roc_auc(data = tibble(truth = data4$SEX, \n                                 .pred_1 = model4_pred$.pred_1), \n                   truth = truth, .pred_1)\n\nprint(model3_roc_auc)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.592\n\nprint(model4_roc_auc)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.980\n\n\nThis section marks the beginning of the modeule 10 exercise.\n\nrngseed = 1234 #use this later to set seed\n\ndata5 &lt;- data4 %&gt;% #remove race vairbale because we won't be using it\n  select(Y, DOSE, AGE, SEX, WT, HT)\n\nset.seed(rngseed) #set seed\n\ndata_split &lt;- initial_split(data5, prop = 3/4) #3/4 of the data is for training\n\ntrain_data &lt;- training(data_split) #create dataframes for the sets\ntest_data  &lt;- testing(data_split) #based on tidymodels instructions\n\n#now we will fit our two models\nmodel_dose &lt;- linear_reg() #linear model with dose as predictor using training data\nmodel_dose_fit &lt;- \n  model_dose %&gt;% \n  fit(Y ~ DOSE, data = train_data)\nmodel_dose_fit \n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ DOSE, data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n     535.45        53.42  \n\nmodel_all &lt;- linear_reg() #linear model with all productors using training data\nmodel_all_fit &lt;- \n  model_all %&gt;% \n  fit(Y ~ DOSE + AGE + SEX + WT + HT, data = train_data)\nmodel_all_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ DOSE + AGE + SEX + WT + HT, data = data)\n\nCoefficients:\n(Intercept)         DOSE          AGE         SEX2           WT           HT  \n  4396.7716      55.3445      -0.4174    -568.9967     -22.6399   -1129.6696  \n\n\nNext, we’ll calculate the RMSE for these two new models and a null model.\nThe null model has a RMSE of 948.35.\nThe model with only dose as a predictor has a RMSE of 702.81.\nThe full model has a RMSE of 627.44. This is the best model given this is the lowest RMSE out of the 3 models.\n\nset.seed(rngseed) #set seed\n\n#predictions for model1\nmodel_dose_pred &lt;- predict(\n  model_dose_fit,\n  new_data = train_data)\n\n#put predictions with data\nmodel_dose_result &lt;- train_data %&gt;%\n  select(Y, DOSE) %&gt;%\n  bind_cols(model_dose_pred)\n\n#get RMSE\nmodel_dose_result %&gt;%\n  yardstick::rmse(\n    truth = Y, \n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        703.\n\n#do the same for the model with all predictors\n#predictions for model2\nmodel_all_pred &lt;- predict(\n  model_all_fit,\n  new_data = train_data)\n\n#put predictions with data, add other vairables\nmodel_all_result &lt;- train_data %&gt;%\n  select(Y, DOSE, AGE, SEX, WT, HT) %&gt;%\n  bind_cols(model_all_pred)\n\n#get RMSE\nmodel_all_result %&gt;%\n  yardstick::rmse(\n    truth = Y, \n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        627.\n\n#I used chatGPT to help with this\n# Define null model\nnull_mod &lt;- null_model() %&gt;%\n  set_engine(\"parsnip\") %&gt;%\n  set_mode(\"regression\")\n\n# Create a workflow without specifying intercept\nnull_wf &lt;- workflow() %&gt;%\n  add_model(null_mod) %&gt;%\n  add_formula(Y ~ .)  # Only Y, no predictors will be used\n\n# Fit the null model\nnull_fit &lt;- fit(null_wf, data = train_data)\n\n# Get predictions from the null model\nnull_preds &lt;- predict(null_fit, train_data)\n\n# Combine predictions with actual data\nnull_preds &lt;- bind_cols(train_data, null_preds)\n\n# Calculate RMSE\nrmse_result &lt;- rmse(null_preds, truth = Y, estimate = .pred)\n\n# Print the result\nprint(rmse_result)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        948.\n\n\nNow, we’re going to do a 10-fold cross-validation with the testing data.\nThe simple CV model has an RMSE of 690.54.\nThe full CV model has an RMSE of 645.69.\nThe cross validation process has the same conclusion as the earlier process– that the full model is best, with lowest RMSE. However, it’s not quite as low, and the single model has a much closer RMSE this time. I’d be interested to see if this holds with even more iterations.\nNext, I’m going to change the seed and rerun, then we’ll see how the results look.\nWith a different seed value, the single model has a RMSE of 691.72.\nThe full model has a RMSE of 633.46.\nThis agrees with earlier results, that the full model has the lowest RMSE. However, the single and full models aren’t quite so close. This makes me feel more confident in these results.\n\nset.seed(1938) #set seed\n\nfolds &lt;- vfold_cv(train_data, v = 10) #set number of folds\nfolds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits         id    \n   &lt;list&gt;         &lt;chr&gt; \n 1 &lt;split [81/9]&gt; Fold01\n 2 &lt;split [81/9]&gt; Fold02\n 3 &lt;split [81/9]&gt; Fold03\n 4 &lt;split [81/9]&gt; Fold04\n 5 &lt;split [81/9]&gt; Fold05\n 6 &lt;split [81/9]&gt; Fold06\n 7 &lt;split [81/9]&gt; Fold07\n 8 &lt;split [81/9]&gt; Fold08\n 9 &lt;split [81/9]&gt; Fold09\n10 &lt;split [81/9]&gt; Fold10\n\n#for dose model\ndose_wf &lt;- \n  workflow() %&gt;%\n  add_model(model_dose) %&gt;%\n  add_formula(Y ~ DOSE)\n\n# 10-fold cv for model_dose\ndose_fit_rs &lt;- \n  dose_wf %&gt;% \n  fit_resamples(folds)\ndose_fit_rs\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits         id     .metrics         .notes          \n   &lt;list&gt;         &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [81/9]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [81/9]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [81/9]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [81/9]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [81/9]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [81/9]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [81/9]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [81/9]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [81/9]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [81/9]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n# Collect metrics for model_dose\ndose_metrics &lt;- collect_metrics(dose_fit_rs)\n\n\n\n\n#for model with all predictors\nall_wf &lt;- \n  workflow() %&gt;%\n  add_model(model_all) %&gt;%\n  add_formula(Y ~ DOSE + WT + AGE + SEX + HT)\n\n# 10-fold cv for model_all\nall_fit_rs &lt;- \n  all_wf %&gt;% \n  fit_resamples(folds)\nall_fit_rs\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits         id     .metrics         .notes          \n   &lt;list&gt;         &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [81/9]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [81/9]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [81/9]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [81/9]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [81/9]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [81/9]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [81/9]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [81/9]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [81/9]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [81/9]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n# Collect metrics for model_all\nall_metrics &lt;- collect_metrics(all_fit_rs)\n\n\n\n\n# Filter RMSE for both models\ndose_rmse &lt;- dose_metrics %&gt;% filter(.metric == \"rmse\")\nall_rmse &lt;- all_metrics %&gt;% filter(.metric == \"rmse\")\n\n# Print RMSE for both models\nprint(dose_rmse)\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    692.    10    63.2 Preprocessor1_Model1\n\nprint(all_rmse)\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    633.    10    61.5 Preprocessor1_Model1\n\n\n\nThis section added by Mohammed Zuber\nNow, here I am generating a scatter plot of observed vs. predicted values for all three models.\n\n# Load necessary packages\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Combine predictions into a single dataframe\nmodel_dose_result &lt;- model_dose_result %&gt;%\n  mutate(model = \"Model 1: DOSE only\")\n\nmodel_all_result &lt;- model_all_result %&gt;%\n  mutate(model = \"Model 2: All Predictors\")\n\nnull_preds &lt;- null_preds %&gt;%\n  mutate(model = \"Null Model\")\n\n# Combine all results\nall_predictions &lt;- bind_rows(model_dose_result, model_all_result, null_preds)\n\n# Plot observed vs predicted values\nggplot(all_predictions, aes(x = Y, y = .pred, color = model)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +\n  labs(title = \"Observed vs. Predicted Values\",\n       x = \"Observed Y\",\n       y = \"Predicted Y\",\n       color = \"Model\") +\n  scale_x_continuous(limits = c(0, 5000)) +\n  scale_y_continuous(limits = c(0, 5000)) +\n  theme_minimal()\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe scatter plot compares observed Y values to predicted Y values for three models: the null model, Model 1 (DOSE only), and Model 2 (all predictors). The null model predictions form a horizontal line, indicating it only predicts the mean Y value for all observations without considering any predictors. Model 1, which includes only DOSE as a predictor, shows three distinct horizontal lines, reflecting the limited number of dose levels and its inability to capture finer variations. Model 2, which includes all predictors, produces a more continuous spread of predictions that align more closely with the 45-degree reference line, indicating improved predictive accuracy. However, there is still some scatter around the diagonal, suggesting that additional factors not included in the model may influence Y. Overall, Model 2 performs the best, but further refinements or additional variables may be needed to improve prediction accuracy.\nNow Plotting predicted vs. residuals to check for patterns.\n\n# Compute residuals for Model 2\nmodel_all_result &lt;- model_all_result %&gt;%\n  mutate(residuals = .pred - Y)\n\n# Residuals plot\nggplot(model_all_result, aes(x = .pred, y = residuals)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residuals vs. Predicted Values (Model 2)\",\n       x = \"Predicted Values\",\n       y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe residuals vs. predicted values plot for Model 2 shows that residuals are generally scattered around zero, indicating a reasonably good fit.\nIn this step, I am Using bootstrapping to assess uncertainty in Model 2 predictions.\n\n# Set seed for reproducibility\nset.seed(rngseed)\n\n# Generate 100 bootstrap samples from training data\nboot_samples &lt;- bootstraps(train_data, times = 100)\n\n# Function to fit Model 2 on a bootstrap sample and predict on training data\nbootstrap_preds &lt;- map(boot_samples$splits, function(split) {\n  \n  # Extract bootstrap sample\n  data_sample &lt;- analysis(split)\n  \n  # Fit Model 2 (full model) on the bootstrap sample\n  fit &lt;- model_all %&gt;% fit(Y ~ DOSE + AGE + SEX + WT + HT, data = data_sample)\n  \n  # Generate predictions on the original training data\n  predict(fit, new_data = train_data)$.pred\n})\n\n# Convert list of predictions into a matrix (rows: data points, columns: bootstrap samples)\nbootstrap_preds_matrix &lt;- do.call(cbind, bootstrap_preds)\n\n# Compute the median and 89% confidence intervals (5.5% and 94.5% percentiles)\nbootstrap_summary &lt;- apply(bootstrap_preds_matrix, 1, quantile, probs = c(0.055, 0.5, 0.945)) %&gt;%\n  t() %&gt;%  # Transpose for better structure\n  as.data.frame()\n\n# Rename columns for clarity\ncolnames(bootstrap_summary) &lt;- c(\"lower_bound\", \"median\", \"upper_bound\")\n\n# Merge the bootstrap results with the observed values\nbootstrap_results &lt;- train_data %&gt;%\n  mutate(original_pred = model_all_result$.pred,  # Original model predictions\n         lower_bound = bootstrap_summary$lower_bound,  # Lower confidence bound\n         median = bootstrap_summary$median,  # Median bootstrap prediction\n         upper_bound = bootstrap_summary$upper_bound)  # Upper confidence bound\n\n# Visualization: Observed vs Predicted values with Bootstrap Confidence Intervals\nggplot(bootstrap_results, aes(x = Y, y = original_pred)) +\n  geom_point(color = \"blue\", alpha = 0.6) +  # Original model predictions as black dots\n  geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound), width = 0.2, color = \"purple\") +  # CI bars\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +  # Ideal 45-degree line\n  scale_x_continuous(limits = c(0, 5000)) +  # Set x-axis range from 0 to 5000\n  scale_y_continuous(limits = c(0, 5000)) +  # Set y-axis range from 0 to 5000\n  labs(title = \"Bootstrap Confidence Intervals for Model 2 Predictions\",\n       x = \"Observed\",\n       y = \"Predicted\") +\n  theme_minimal()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe bootstrap confidence interval plot for Model 2 shows predicted values against observed values, with error bars representing the uncertainty in predictions. Most predictions align closely with the 45-degree reference line, indicating that the model performs reasonably well in capturing the true values. However, the presence of wider confidence intervals for some points suggests higher uncertainty in predictions, particularly at higher observed values. This implies that the model may have greater variability in its estimates for certain data points, possibly due to missing predictors or high noise in the data. While the majority of points follow the expected trend, a few deviations indicate that some observations are not well predicted, reinforcing the need for potential improvements. Overall, the model shows a strong predictive trend, but refinement through additional features or more complex modeling techniques may further enhance accuracy."
>>>>>>> origin/main
  },
  {
    "objectID": "aboutme.html#introduction",
    "href": "aboutme.html#introduction",
    "title": "About me",
    "section": "",
<<<<<<< HEAD
    "text": "Hi! My name is Annalise, but everyone calls me Annie. I use she/her pronouns.\nI’m a second year PhD student in the Epidemiology and Biostatistics Department. I work with Dr. Spencer Fox, and I broadly do stuff related to flu evolution– currently, we’re working on developing county by county risk for H5N1 based on childhood immunological imprinting. It’s a work in progress :)"
  },
  {
    "objectID": "aboutme.html#academic-background",
    "href": "aboutme.html#academic-background",
    "title": "About me",
    "section": "Academic Background",
    "text": "Academic Background\nFor undergrad, I attended Westfield State University in Westfield, MA (about half an hour south of UMass, if you’re familiar). My bachelors are in biotechnology and general mathematics, I did half pure and half applied math. Working in a hospital lab during the height of covid developed my interest in respiratory viruses, and completing an REU in UGA’s Odum School of Ecology fine tuned my infectious disease interests towards mathematical modeling. After graduating, I moved to Athens to begin my PhD.\nI have a strong preference for R, and I’ve used SAS for a few classes. In undergrad we used VBA/excel, but that’s not very helpful for this field. I’m a stronger mathematician than programmer, but that’s been shifting as I move through this degree program, and that’s something that I hope this course will continue to push me towards. I’ve taken the department required courses (BIOS8010, BIOS8020, BIOS8030, BIOS8200) and a handful of STAT department courses to fill out my skillset (Bayesian statistics, Time series analysis). I’m making progress on building my data analysis skills on top of my mathematical background, but there’s still plenty more to learn!\nI’ve completed several data analysis projects throughout my academic career, using data from phylogenetic sequences to survey data to case counts. I hope to get more into case count data and eventually move into forecasting. I’m hoping this course will help me gain more confidence working with data and staying organized."
  },
  {
    "objectID": "aboutme.html#non-academic-background",
    "href": "aboutme.html#non-academic-background",
    "title": "About me",
    "section": "Non-Academic background",
    "text": "Non-Academic background\nI grew up in rural Western Massachusetts, leading to my hobby for hiking. I especially like mountainy hikes with great views. I do a little day trip once or twice a month to go hiking somewhere in Northern GA/NC/TN. My all time favorite hike was with my brother in Death Valley. It’s the only place I’ve been to where there’s no encouragement to stay on the trails– the climate is so extreme, humans can’t do much damage, so we just walked across the salt flats for miles. We both preferred less busy environments, so we planned our trip for December to avoid crowds. Along the way, we found this tool that helped us plan the trip. It’s interesting to me to see the differences during 2020-2021, as well as effects of climate variation on visit counts. Additionally, the site provides visitation data sets.\nSince I’m newish to the area still, let me know your trail recommendations!\n\n\n\nMe at Charlie’s Bunion in Great Smoky Mountains NP, this past June. It was too cloudy to see the view."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "This data is from the CDC website, link here. It shows information from information from recent serological tests on SARS-CoV-2 samples that examine positivity, reactivity, and anitbody counts. Specifically, it examines whether different tests determine if samples test positive or negative using different types of covid tests. The mNT titer level represents the most dilute possible solution of anitbody to neutralize the virus.\nTo start, I’ll load packages and the data. Looking at the structure of the data, we can see there’s 204 obersvatinos and 14 variables. We will not look at all the vairables, so let’s choose the ones we want.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/annalisecramer/MADA/annalisecramer-MADA-portfolio-main/annalisecramer-MADA-portfolio\n\nhere()\n\n[1] \"/Users/annalisecramer/MADA/annalisecramer-MADA-portfolio-main/annalisecramer-MADA-portfolio\"\n\ndata &lt;- read_csv(here(\"cdcdata-exercise\", \"Examination_of_SARS-CoV-2_serological_test_results_from_multiple_commercial_and_laboratory_platforms_with_an_in-house_serum_panel_20250205.csv\"))\n\nRows: 204 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): PCR confirmed, Abbott Reactivity, Ortho Reactivity, In-House CDC E...\ndbl (10): Sample #, Abbott S/C Values, Abbott S/C Values (Log10), Ortho Inde...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata2 &lt;- data %&gt;%\n  select(`Sample #`, `PCR confirmed`, `Abbott Reactivity`, `Ortho Reactivity`, `In-House CDC ELISA Reactivity`, `mNT TITER (Log10)`)\n\nFirst, let’s look at the Sample # variable. It’s just integers from 1 to 204, each occuring one time.\n\nunique(data2$`Sample #`)\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204\n\ndata2$`Sample #`\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204\n\nggplot(data=data2, aes(x=`Sample #`)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNext, let’s explore the variable for PCR confirmed. This variable is either negative or positive.\n\nunique(data2$`PCR confirmed`)\n\n[1] \"Pos\" \"Neg\"\n\ncount_table &lt;- table(data2$`PCR confirmed`) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n    count_table      \nNeg         117 57.35\nPos          87 42.65\n\n\nNow we will look at the Abbott Reactivity vairable. There are two values, Reactive and Non-Reactive.\n\nunique(data2$`Abbott Reactivity`)\n\n[1] \"Reactive\"     \"Non-reactive\"\n\ncount_table &lt;- table(data2$`Abbott Reactivity`) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n             count_table      \nNon-reactive         120 58.82\nReactive              84 41.18\n\n\nNext, let us examine Ortho Reactivity. There are two values, Reactive and Non-Reactive.\n\nunique(data2$`Ortho Reactivity`)\n\n[1] \"Reactive\"     \"Non-reactive\"\n\ncount_table &lt;- table(data2$`Ortho Reactivity`) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n             count_table      \nNon-reactive         120 58.82\nReactive              84 41.18\n\n\nOne more reactivity test to examine. There are two responses for In-House CDC ELISA Reactivity, Reactive and Non-Reactive.\n\nunique(data2$`In-House CDC ELISA Reactivity`)\n\n[1] \"Reactive\"     \"Non-reactive\"\n\ncount_table &lt;- table(data2$`In-House CDC ELISA Reactivity`) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n             count_table      \nNon-reactive         118 57.84\nReactive              86 42.16\n\n\nNow, let us examine mNT TITER (Log10). This is not a character variable, it is numerical with continuous responses that ranger between 1 and 4. Many samples have a value of 1.\n\nunique(data2$`mNT TITER (Log10)`)\n\n [1] 1.2553 2.3674 3.3420 1.4314 2.7193 1.7559 2.5855 2.4914 2.1987 2.2455\n[11] 2.6776 2.9547 1.5051 3.7748 2.4265 2.5315 1.0000 3.0128 2.7945 3.7083\n[21] 1.4771 2.7284 1.6232 3.5465 1.6532 2.8062 2.8865 1.3010 3.1035 3.1414\n[31] 1.6721 2.7938 2.8357 2.5933 2.6653 2.9513 2.1000 1.7064 2.2310 2.2175\n[41] 2.5038 2.7372 3.1380 2.8169 2.8014 3.3162 2.7177 1.2788 2.8370 3.2240\n[51] 3.1562 2.2095 2.1903 3.3185 2.5441 2.5416 2.5877 2.0414 3.7676 2.6454\n[61] 2.8993 2.0755 1.8921 3.2582 2.4150 2.7649 1.3617 1.9685 2.7267 3.2577\n[71] 2.5955 3.4260 3.0026 2.1790 3.4156 1.3222 2.5490 3.0030 3.4298 3.4492\n[81] 2.3032 1.2041\n\nggplot(data=data2, aes(x=`mNT TITER (Log10)`)) +\n  geom_histogram(binwidth=0.1)\n\n\n\n\n\n\n\n\nReturning to the different types of tests, we know most of these have the same results across samples. However, a few are different. I used AI to help write the code for reshpaing and plotting the data. We can see they arre very close.\n\ndata3 &lt;- data2 %&gt;% #change the test data to all the same answers\n  mutate(`PCR confirmed` = recode(`PCR confirmed`, \n                                     \"Pos\" = \"yes\", \n                                     \"Neg\" = \"no\")) %&gt;%\n  mutate(`Abbott Reactivity` = recode(`Abbott Reactivity`, \n                                     \"Reactive\" = \"yes\", \n                                     \"Non-reactive\" = \"no\")) %&gt;%\n  mutate(`Ortho Reactivity` = recode(`Ortho Reactivity`, \n                                     \"Reactive\" = \"yes\", \n                                     \"Non-reactive\" = \"no\")) %&gt;%\n  mutate(`In-House CDC ELISA Reactivity` = recode(`In-House CDC ELISA Reactivity`, \n                                    \"Reactive\" = \"yes\", \n                                     \"Non-reactive\" = \"no\"))\n\n# Reshape the dataset to long format for easy plotting\ndata3_long &lt;- data3 %&gt;%\n  gather(key = \"Test\", value = \"Outcome\", \n         `PCR confirmed`, `Abbott Reactivity`, \n         `Ortho Reactivity`, `In-House CDC ELISA Reactivity`)\n\n# Ensure that Outcome is a factor with the correct levels\ndata3_long$Outcome &lt;- factor(data3_long$Outcome, levels = c(\"no\", \"yes\"))\n\n# Create a stacked bar plot\nggplot(data3_long, aes(x = Test, fill = Outcome)) + \n  geom_bar(position = \"fill\") +  # Position 'fill' normalizes the height to proportions\n  labs(\n    title = \"Comparison of Test Reactivity\",\n    x = \"Test\",\n    y = \"Proportion of Outcomes\",\n    fill = \"Reactivity Outcome\"\n  ) +\n  scale_fill_manual(values = c(\"no\" = \"red3\", \"yes\" = \"forestgreen\")) + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\nThis section contributed by Pooja Gokhale. Based on the give data, I used an AI prompt to give me the code to generate synthetic data. I used the counts and percentages from the original data. For mNT Titer Log10, we will compute the mean and range.\n\nmean(data2$`mNT TITER (Log10)`)\n\n[1] 1.625027\n\nsd(data2$`mNT TITER (Log10)`)\n\n[1] 0.8635054\n\nrange(data2$`mNT TITER (Log10)`)\n\n[1] 1.0000 3.7748\n\n\nUsing these, I got the folllowing code. The code was set to produce 200 observations. In the original dataset, about 125 out of 204 observations were in the range of 1, with others in the range of 2-3.7. So in the synthetic data, 60% of the values were assumed to be near 1 for mNT Titer Log10.\n\nset.seed(123)  # For reproducibility\n\n# Number of samples\nn_samples &lt;- 200\n\n# Generate Sample Number\nsample_number &lt;- 1:n_samples\n\n# Assign PCR Confirmed based on proportions\npcr_confirmed &lt;- sample(c(\"Pos\", \"Neg\"), n_samples, replace = TRUE, prob = c(0.4265, 0.5735))\n\n# Assign Abbott Reactivity based on proportions\nabbott_reactivity &lt;- sample(c(\"Reactive\", \"Non-reactive\"), n_samples, replace = TRUE, prob = c(0.4118, 0.5882))\n\n# Assign Ortho Reactivity based on proportions\northo_reactivity &lt;- sample(c(\"Reactive\", \"Non-reactive\"), n_samples, replace = TRUE, prob = c(0.4118, 0.5882))\n\n# Assign In-house CDC ELISA Reactivity based on proportions\ninhouse_cdc_elisa &lt;- sample(c(\"Reactive\", \"Non-reactive\"), n_samples, replace = TRUE, prob = c(0.4216, 0.5784))\n\n# Generate mNT Titer (log10) values following the required distribution\nmnt_titer &lt;- c(\n  rnorm(120, mean = 1.2, sd = 0.2),  # ~125 values near 1\n  rnorm(80, mean = 2.8, sd = 0.4)    # ~79 values in the 2-3.77 range\n)\n\n# Ensure values stay within the expected range\nmnt_titer[mnt_titer &lt; 1] &lt;- 1        # Clamp minimum to 1\nmnt_titer[mnt_titer &gt; 3.7748] &lt;- 3.7748  # Clamp maximum to 3.7748\n\n# Adjust mean and SD to match the target values\nwhile (abs(mean(mnt_titer) - 1.625) &gt; 0.01 || abs(sd(mnt_titer) - 0.8635) &gt; 0.01) {\n  mnt_titer &lt;- scale(mnt_titer) * 0.8635 + 1.625  # Rescale to match mean & SD\n  mnt_titer[mnt_titer &lt; 1] &lt;- 1\n  mnt_titer[mnt_titer &gt; 3.7748] &lt;- 3.7748\n}\n\n# Shuffle the values to randomize order\nmnt_titer &lt;- sample(mnt_titer, n_samples, replace = FALSE)\n\n# Combine into a data frame\nsynthetic_data &lt;- data.frame(\n  Sample_Number = sample_number,\n  PCR_Confirmed = pcr_confirmed,\n  Abbott_Reactivity = abbott_reactivity,\n  Ortho_Reactivity = ortho_reactivity,\n  Inhouse_CDC_ELISA = inhouse_cdc_elisa,\n  mNT_Titer_Log10 = mnt_titer\n)\n\nwrite.csv(synthetic_data, \"synthetic_data.csv\", row.names = FALSE)\n\nNow let us look at the summary of this synthetic data.\nFirst. let’s look at PCR confirmed\n\ncount_table &lt;- table(synthetic_data$PCR_Confirmed) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2))\n\n    count_table     \nNeg         117 58.5\nPos          83 41.5\n\n\nAbout 58% are Negative, and 42% are Positive, compared to 57.35% and 42.65% in the original dataset.\nNow, let’s look at Abbott reactivity\n\ncount_table &lt;- table(synthetic_data$Abbott_Reactivity) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n             count_table     \nNon-reactive         121 60.5\nReactive              79 39.5\n\n\nHere, 60.5% are non-reactive and 39.5% are Reactive, compared to 58.82% and 41.18% respectively in the original dataset.\nNow, let’s look at Ortho reactivity\n\ncount_table &lt;- table(synthetic_data$Ortho_Reactivity) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n             count_table     \nNon-reactive         121 60.5\nReactive              79 39.5\n\n\nThis is similar to Abbott reactivity, like the original dataset.\nNow, let’s look at In-House CDC Elisa reactivity\n\ncount_table &lt;- table(synthetic_data$Inhouse_CDC_ELISA) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n             count_table     \nNon-reactive         115 57.5\nReactive              85 42.5\n\n\nHere, 57.5% are Non-reactive and 42.5% are Reactive, compared to 57.84% and 42.16% respectively in the original dataset.\nNow, let’s look at mNT Titer Log10.\n\nggplot(synthetic_data, aes(x=mNT_Titer_Log10)) +\n  geom_histogram(binwidth=0.1)\n\n\n\n\n\n\n\n\nWe can see that the distribution of the mNT Titer Log10 in the synthetic data is very similar to that of the original dataset.\nNow, let’s plot the data.\n\nsynthetic_data2 &lt;- synthetic_data %&gt;% #change the test data to all the same answers\n  mutate(PCR_Confirmed = recode(PCR_Confirmed, \n                                     \"Pos\" = \"yes\", \n                                     \"Neg\" = \"no\")) %&gt;%\n  mutate(Abbott_Reactivity = recode(Abbott_Reactivity, \n                                     \"Reactive\" = \"yes\", \n                                     \"Non-reactive\" = \"no\")) %&gt;%\n  mutate(Ortho_Reactivity = recode(Ortho_Reactivity, \n                                     \"Reactive\" = \"yes\", \n                                     \"Non-reactive\" = \"no\")) %&gt;%\n  mutate(Inhouse_CDC_ELISA = recode(Inhouse_CDC_ELISA, \n                                    \"Reactive\" = \"yes\", \n                                     \"Non-reactive\" = \"no\"))\n\n# Reshape the dataset to long format for easy plotting\nsynthetic_data_long &lt;- synthetic_data2 %&gt;%\n  gather(key = \"Test\", value = \"Outcome\", \n         PCR_Confirmed, Abbott_Reactivity, \n         Ortho_Reactivity, Inhouse_CDC_ELISA)\n\n# Ensure that Outcome is a factor with the correct levels\nsynthetic_data_long$Outcome &lt;- factor(synthetic_data_long$Outcome, levels = c(\"no\", \"yes\"))\n\n# Create a stacked bar plot\nggplot(synthetic_data_long, aes(x = Test, fill = Outcome)) + \n  geom_bar(position = \"fill\") +  # Position 'fill' normalizes the height to proportions\n  labs(\n    title = \"Comparison of Test Reactivity\",\n    x = \"Test\",\n    y = \"Proportion of Outcomes\",\n    fill = \"Reactivity Outcome\"\n  ) +\n  scale_fill_manual(values = c(\"no\" = \"red3\", \"yes\" = \"forestgreen\")) + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\nSimilar to the original dataset, approximately 38% of the outcome was Yes, while 62% was No.\nUsing the AI prompt, I was able to produce a synthetic dataset which was very similar to the original."
  },
  {
    "objectID": "presentation-exercise/study-drugs/pierson_study_drugs.html",
    "href": "presentation-exercise/study-drugs/pierson_study_drugs.html",
    "title": "Analysis for the story College Students Aren’t The Only Ones Abusing Adderall",
    "section": "",
    "text": "%matplotlib inline \n\n\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = \"&lt;style&gt;div.cell{width:100%;margin-left:1%;margin-right:auto;}.container { width:100% !important; } &lt;/style&gt; \"\n    return HTML(styles)\ncss_styling()\n\n \n\n\n\nFirst load data\n\nfrom pylab import *\nimport pandas as pd\nimport numpy as np\nimport os\nimport statsmodels.api as sm\nfrom statsmodels.iolib.summary2 import summary_col\n\n#load in NSDUH data\nnsduh_data = pd.read_csv('ICPSR_35509/DS0001/35509-0001-Data.tsv', sep = '\\t')\n#load in Niche data\nniche_data = pd.read_csv('niche/SchoolPoll.csv')\n#load in IPEDS data\ndef getIPEDSData():\n    files = ['ipeds_data/CSV_7312015-162.csv', 'ipeds_data/CSV_7312015-826.csv']\n    \n    for i, f in enumerate(files):\n        d = pd.read_csv(f)\n        d.index = d.unitid \n        del d['unitid']\n        if i == 0:\n            all_d = d\n        else:\n            all_d = pd.concat([all_d, d])\n    ipeds_d = {}\n    for c in all_d.columns:\n        new_name = c.replace(' - ', '_').replace('/', '_').replace(',', '_').replace('.', '_').replace(' ', '_')\n        new_name = new_name.replace(\"Percent_of_total_enrollment_that_are_\", '')\n        ipeds_d[new_name] = dict(zip(all_d.index, all_d[c]))\n    return ipeds_d\nipeds_data = getIPEDSData()\n#load in Google data\ngoogle_filenames = ['ADHD_Indexed.csv','Adderall_Indexed.csv']\ngoogle_data = {}\nfor filename in google_filenames:\n    d = pd.read_csv(os.path.join('google_data', filename))\n    d.index = d.Month\n    del d['Month']\n    d = d.transpose()\n    google_data[filename.replace('_Indexed.csv', '')] = d\n    \n\n\n“When I looked at more recent data (the 2013 National Survey on Drug Use and Health (NSDUH), an annual government survey that includes more than 55,000 Americans) the difference turned out to be closer to 1.3x, not 2x.”\n\n#People who respond 1 have used Adderall non-medically; no one else has.  \nnsduh_data['ADDERALL'] = (nsduh_data['ADDERALL'] == 1)*1.\nnsduh_data['RITALIN'] = (nsduh_data['RITMPHEN'] == 1)*1.\n\n\n\ndef stratifyByCat(nsduh_data, cat, drug_to_measure = 'ADDERALL', sortByVal = False):\n    \"\"\"\n    Checked. Computes the weighted and unweighted mean values of Adderall, stratifying by \n    the levels in cat. \n    \"\"\"\n    levels = sorted(list(set(nsduh_data[cat].dropna())))\n    print 'Category', cat\n    table = []\n    for l in levels:\n        idxs = nsduh_data[cat] == l\n        summed_weights = nsduh_data.loc[idxs]['ANALWT_C'].sum()\n        mu = (nsduh_data.loc[idxs][drug_to_measure] * nsduh_data.loc[idxs]['ANALWT_C']).sum() / summed_weights\n        unweighted_mu = nsduh_data.loc[idxs][drug_to_measure].mean()\n        if idxs.sum() &gt; 25:\n            table.append([drug_to_measure, l, mu, unweighted_mu, idxs.sum()])\n    if sortByVal:\n        table = sorted(table, key = lambda x:x[2])[::-1]\n    for row in table:\n        print 'Mean value of %s for level %s is %2.3f; unweighted, %2.3f (%i values)' % (tuple(row))\n    print 'Ratio between maximum value and minimum value', max([a[2] for a in table]) / min([a[2] for a in table])\nprint 'COLLENR levels:\\n1: FT college students age 18 - 22; 2: other age 18 - 22; 3: other'\nstratifyByCat(nsduh_data, 'COLLENR')     \nstratifyByCat(nsduh_data, 'COLLENR', drug_to_measure = 'RITALIN')     \n\n\nCOLLENR levels:\n1: FT college students age 18 - 22; 2: other age 18 - 22; 3: other\nCategory COLLENR\nMean value of ADDERALL for level 1 is 0.149; unweighted, 0.140 (4310 values)\nMean value of ADDERALL for level 2 is 0.115; unweighted, 0.114 (6934 values)\nMean value of ADDERALL for level 3 is 0.034; unweighted, 0.048 (43914 values)\nRatio between maximum value and minimum value 4.45754687876\nCategory COLLENR\nMean value of RITALIN for level 1 is 0.048; unweighted, 0.048 (4310 values)\nMean value of RITALIN for level 2 is 0.050; unweighted, 0.049 (6934 values)\nMean value of RITALIN for level 3 is 0.022; unweighted, 0.026 (43914 values)\nRatio between maximum value and minimum value 2.2055874138\n\n\n\n\n“This is far smaller than the difference between white 18 - 22 year olds and black 18 - 22 year olds (6x, 18% vs 3%) or the difference between 18 - 22 year olds whose families do not receive food stamps and those whose do (1.6x, 14% vs 9%).”\n\nyoung_people_idxs = nsduh_data['COLLENR'].map(lambda x:x in [1, 2])\nprint '\\nCategories for race:\\n1: white, 2: black, 3: Native Am; 4: Native HI/Other Pac Isl; 5: Asian; 6: Multiracial; 7: Hispanic'\nstratifyByCat(nsduh_data.loc[young_people_idxs], 'NEWRACE2')  \nstratifyByCat(nsduh_data.loc[young_people_idxs], 'NEWRACE2', drug_to_measure = 'RITALIN')\n\nprint 'Categories for food stamp: 1: respondent or family member received food stamp; 2: did not'\nstratifyByCat(nsduh_data.loc[young_people_idxs], 'IRFSTAMP')    \nstratifyByCat(nsduh_data.loc[young_people_idxs], 'IRFSTAMP', drug_to_measure = 'RITALIN') \n  \n\n\nCategories for race:\n1: white, 2: black, 3: Native Am; 4: Native HI/Other Pac Isl; 5: Asian; 6: Multiracial; 7: Hispanic\nCategory NEWRACE2\nMean value of ADDERALL for level 1 is 0.181; unweighted, 0.171 (6194 values)\nMean value of ADDERALL for level 2 is 0.031; unweighted, 0.034 (1602 values)\nMean value of ADDERALL for level 3 is 0.052; unweighted, 0.086 (163 values)\nMean value of ADDERALL for level 4 is 0.084; unweighted, 0.078 (64 values)\nMean value of ADDERALL for level 5 is 0.084; unweighted, 0.086 (532 values)\nMean value of ADDERALL for level 6 is 0.113; unweighted, 0.155 (491 values)\nMean value of ADDERALL for level 7 is 0.076; unweighted, 0.065 (2198 values)\nRatio between maximum value and minimum value 5.84203431864\nCategory NEWRACE2\nMean value of RITALIN for level 1 is 0.071; unweighted, 0.069 (6194 values)\nMean value of RITALIN for level 2 is 0.007; unweighted, 0.009 (1602 values)\nMean value of RITALIN for level 3 is 0.026; unweighted, 0.037 (163 values)\nMean value of RITALIN for level 4 is 0.013; unweighted, 0.016 (64 values)\nMean value of RITALIN for level 5 is 0.029; unweighted, 0.028 (532 values)\nMean value of RITALIN for level 6 is 0.054; unweighted, 0.067 (491 values)\nMean value of RITALIN for level 7 is 0.026; unweighted, 0.022 (2198 values)\nRatio between maximum value and minimum value 9.9793813036\nCategories for food stamp: 1: respondent or family member received food stamp; 2: did not\nCategory IRFSTAMP\nMean value of ADDERALL for level 1 is 0.088; unweighted, 0.089 (2564 values)\nMean value of ADDERALL for level 2 is 0.140; unweighted, 0.135 (8680 values)\nRatio between maximum value and minimum value 1.59577871899\nCategory IRFSTAMP\nMean value of RITALIN for level 1 is 0.041; unweighted, 0.043 (2564 values)\nMean value of RITALIN for level 2 is 0.051; unweighted, 0.050 (8680 values)\nRatio between maximum value and minimum value 1.24093935037\n\n\n\n\nGraphs on Adderall usage by age\n\ndef remapAges(x):\n    if x &lt;= 3:\n        return '12 - 14'\n    elif x &lt;= 6:\n        return '15 - 17'\n    elif x &lt;= 10:\n        return 'college age'\n    elif x &lt;= 12:\n        return '23 - 25'\n    elif x &lt;= 14:\n        return '26 - 34'\n    return '35+'\nnsduh_data['age_cat_to_plot'] = nsduh_data['AGE2'].map(remapAges)\nnsduh_data['age_cat_to_plot'].loc[nsduh_data['COLLENR'] != 3] = np.nan\n\n#This computes the values for Adderall usage stratified by age and college enrollment. \nstratifyByCat(nsduh_data, 'COLLENR')  \nstratifyByCat(nsduh_data, 'age_cat_to_plot') \n\n#This prints out the actual data in an easy form for making graphs. Kind of hacky. \ncol_vals = [0.4, 4.3, 11.5, 14.9,  14.5, 10.2, 1.1]\ncol_labels = ['12-14', '15-17', '18-22 (Not College)', '18-22 (In College)', '23-25', '26-34', '35+']\nprint 'Age\\tPercent Using Adderall'\nfor i in range(len(col_vals)):\n    print '%s\\t%2.1f' % (col_labels[i], col_vals[i])\n\nCategory COLLENR\nMean value of ADDERALL for level 1 is 0.149; unweighted, 0.140 (4310 values)\nMean value of ADDERALL for level 2 is 0.115; unweighted, 0.114 (6934 values)\nMean value of ADDERALL for level 3 is 0.034; unweighted, 0.048 (43914 values)\nRatio between maximum value and minimum value 4.45754687876\nCategory age_cat_to_plot\nMean value of ADDERALL for level 12 - 14 is 0.004; unweighted, 0.005 (8689 values)\nMean value of ADDERALL for level 15 - 17 is 0.043; unweighted, 0.043 (9047 values)\nMean value of ADDERALL for level 23 - 25 is 0.145; unweighted, 0.140 (6896 values)\nMean value of ADDERALL for level 26 - 34 is 0.102; unweighted, 0.089 (5446 values)\nMean value of ADDERALL for level 35+ is 0.011; unweighted, 0.015 (13836 values)\nRatio between maximum value and minimum value 33.1820207109\nAge Percent Using Adderall\n12-14   0.4\n15-17   4.3\n18-22 (Not College) 11.5\n18-22 (In College)  14.9\n23-25   14.5\n26-34   10.2\n35+ 1.1\n\n\n\n“Study drugs were most frequently used in New England schools”\n\n#Join Niche data with IPEDS data. \nniche_adderall_idxs = (niche_data['Response'] == 'Prescription \"study drugs\" (Adderall, Ritalin) ')\nniche_adderall_fracs = niche_data.loc[niche_adderall_idxs]\nat_least_ten_responses = niche_adderall_fracs['ResponseCt'] &gt;= 10#filter out schools with very few responses. \nniche_adderall_fracs = niche_adderall_fracs.loc[at_least_ten_responses]\nfor school_characteristic in ipeds_data.keys():\n    niche_adderall_fracs[school_characteristic] = niche_adderall_fracs['IPEDS_Id'].map(lambda x:ipeds_data[school_characteristic][x]\n                                                                      if x in ipeds_data[school_characteristic]\n                                                                      else None)\n    \n#Make geographic plot and run regression to confirm discrepancies are significant. \nregion_vals = []\nregion_names = []\nfor region in list(set(ipeds_data['HD2013_Geographic_region'].values())):\n    region_idxs = niche_adderall_fracs['HD2013_Geographic_region'] == region\n    if region_idxs.sum() &gt;= 10:\n        mean_val = niche_adderall_fracs.loc[region_idxs]['PctOfTotal'].mean()\n        if np.isnan(mean_val):\n            continue\n\n        region_vals.append(mean_val)\n        region_names.append(' '.join([a for a in region.split() if len(a) &gt; 2]))\nmodel = sm.OLS.from_formula('PctOfTotal ~ HD2013_Geographic_region', data = niche_adderall_fracs).fit()\nprint model.summary()\nregion_vals = np.array(region_vals)\nregion_names = np.array(region_names)\nsorted_idxs = np.argsort(region_vals)\nbarh(range(len(region_vals)), region_vals[sorted_idxs])\nyticks(np.arange(len(region_vals)) + .5, region_names[sorted_idxs])\nsubplots_adjust(left = .3)\nxlabel('Proportion of students reporting that prescription study drugs\\n are among the most popular on campus')\n\n        \n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             PctOfTotal   R-squared:                       0.126\nModel:                            OLS   Adj. R-squared:                  0.110\nMethod:                 Least Squares   F-statistic:                     7.700\nDate:                Tue, 03 Nov 2015   Prob (F-statistic):           1.18e-09\nTime:                        14:09:22   Log-Likelihood:                 350.49\nNo. Observations:                 436   AIC:                            -683.0\nDf Residuals:                     427   BIC:                            -646.3\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n=============================================================================================================================================\n                                                                                coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]\n---------------------------------------------------------------------------------------------------------------------------------------------\nIntercept                                                                     0.2824      0.016     17.881      0.000         0.251     0.313\nHD2013_Geographic_region[T.Great Lakes IL IN MI OH WI]                        0.0604      0.021      2.901      0.004         0.019     0.101\nHD2013_Geographic_region[T.Mid East DE DC MD NJ NY PA]                        0.1021      0.019      5.330      0.000         0.064     0.140\nHD2013_Geographic_region[T.New England CT ME MA NH RI VT]                     0.1152      0.022      5.283      0.000         0.072     0.158\nHD2013_Geographic_region[T.Outlying areas AS FM GU MH MP PR PW VI]            0.1047      0.111      0.946      0.344        -0.113     0.322\nHD2013_Geographic_region[T.Plains IA KS MN MO NE ND SD]                       0.0775      0.027      2.871      0.004         0.024     0.131\nHD2013_Geographic_region[T.Rocky Mountains CO ID MT UT WY]                   -0.0432      0.030     -1.457      0.146        -0.102     0.015\nHD2013_Geographic_region[T.Southeast AL AR FL GA KY LA MS NC SC TN VA WV]     0.0891      0.019      4.579      0.000         0.051     0.127\nHD2013_Geographic_region[T.Southwest AZ NM OK TX]                             0.0558      0.025      2.191      0.029         0.006     0.106\n==============================================================================\nOmnibus:                       10.358   Durbin-Watson:                   1.920\nProb(Omnibus):                  0.006   Jarque-Bera (JB):               10.752\nSkew:                           0.384   Prob(JB):                      0.00463\nKurtosis:                       2.941   Cond. No.                         22.7\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n#Create data for chart (this chart did not end up being included).\nprint 'Region\\tProportion'\nfor i in range(len(region_names)):\n    print '%s\\t%2.1f' % (region_names[sorted_idxs[::-1][i]], region_vals[sorted_idxs[::-1][i]] * 100)\n\nRegion  Proportion\nNew England 39.8\nMid East    38.5\nSoutheast   37.2\nPlains  36.0\nGreat Lakes 34.3\nSouthwest   33.8\nFar West    28.2\nRocky Mountains 23.9\n\n\n\n\n“Study drugs were also more frequently used at colleges that were more selective or had higher median SAT or ACT scores. In the graph below, each point represents one school; the horizontal axis is the school’s 75th percentile ACT score, and the vertical axis is the fraction of students responding that study drugs are popular. The correlation is highly statistically significant”\n\nachievement_columns = ['IC2013_SAT_Critical_Reading_75th_percentile_score', 'IC2013_SAT_Writing_75th_percentile_score', \n                       'IC2013_SAT_Math_75th_percentile_score', 'IC2013_ACT_Composite_75th_percentile_score', 'DRVIC2013_Percent_admitted_total']\ncontrol_columns = ['DRVEF2013_White', 'DRVEF2013_women', 'IC2013_Institutional_control_or_affiliation', \n                   'DRVEF2013_Asian', 'DRVEF2013_American_Indian_or_Alaska_Native', \n                   'DRVEF2013_Black_or_African_American', 'EFEST2013_Estimated_enrollment__total', \n                   'DRVEF2013_Hispanic_Latino', 'DRVEF2013_Native_Hawaiian_or_Other_Pacific_Islander']\nprint \"\"\"\nRegressing percentage of students at each college saying study drugs are popular on various measures of college selectivity. \nFor each measure of selectivity (first column) we run two regressions: \na simple model -- study_drugs_popular ~ selectivity_measure\na more complex model -- study_drugs_popular ~ selectivity_measure + college_covs\nwhere college_covs are college_racial_breakdown, college_gender_breakdown, college_type, and college_estimated_enrollment\n\nbelow the columns are selectivity_measure, simple_selectivity_measure_beta, simple_selectivity_measure_p, complex_selectivity_measure_beta, complex_selectivity_measure_p\n\"\"\"\n\nfor coef in achievement_columns:\n    model = sm.OLS.from_formula('PctOfTotal ~ %s' % coef, data = niche_adderall_fracs).fit()\n    model_controlling_for_covariates = sm.OLS.from_formula('PctOfTotal ~ %s + %s' % (coef, '+'.join(control_columns)), data = niche_adderall_fracs).fit()\n    print '%-60s %3.5f %2.3e %3.5f %2.3e' % (coef, model.params[coef], model.pvalues[coef], \n                                 model_controlling_for_covariates.params[coef], model_controlling_for_covariates.pvalues[coef])\nscatter(niche_adderall_fracs['IC2013_ACT_Composite_75th_percentile_score'], niche_adderall_fracs['PctOfTotal'])\nxlabel('75th Percentile ACT Score')\nylabel('Fraction of Students Reporting Adderall Popular')\nxlim([20, 36])\nylim([0, .7])\n\nshow()\n                   \n                       \n\n\nRegressing percentage of students at each college saying study drugs are popular on various measures of college selectivity. \nFor each measure of selectivity (first column) we run two regressions: \na simple model -- study_drugs_popular ~ selectivity_measure\na more complex model -- study_drugs_popular ~ selectivity_measure + college_covs\nwhere college_covs are college_racial_breakdown, college_gender_breakdown, college_type, and college_estimated_enrollment\n\nbelow the columns are selectivity_measure, simple_selectivity_measure_beta, simple_selectivity_measure_p, complex_selectivity_measure_beta, complex_selectivity_measure_p\n\nIC2013_SAT_Critical_Reading_75th_percentile_score            0.00068 1.352e-13 0.00057 9.374e-07\nIC2013_SAT_Writing_75th_percentile_score                     0.00059 3.890e-10 0.00062 2.139e-06\nIC2013_SAT_Math_75th_percentile_score                        0.00062 1.922e-13 0.00074 4.848e-10\nIC2013_ACT_Composite_75th_percentile_score                   0.01531 5.626e-18 0.01551 1.232e-09\nDRVIC2013_Percent_admitted_total                             -0.00129 2.996e-06 -0.00141 3.155e-06\n\n\n\n\n\n\n\n\n\n\n#Print out data for chart. \nmodel = sm.OLS.from_formula('PctOfTotal ~ IC2013_ACT_Composite_75th_percentile_score', data = niche_adderall_fracs).fit()\nones_to_plot = niche_adderall_fracs[['IC2013_ACT_Composite_75th_percentile_score', 'PctOfTotal']].dropna()\nones_to_plot['trendline'] = niche_adderall_fracs['IC2013_ACT_Composite_75th_percentile_score'] * model.params['IC2013_ACT_Composite_75th_percentile_score'] + model.params['Intercept']\nones_to_plot['trendline'] = ones_to_plot['trendline'] * 100\nprint 'ACT\\tAdderall\\tTrendline'\nfor i in range(len(ones_to_plot)):\n    print '%2.3f\\t%2.3f\\t%2.3f' % (ones_to_plot['IC2013_ACT_Composite_75th_percentile_score'].iloc[i],\n                                   ones_to_plot['PctOfTotal'].iloc[i] * 100, \n                                   ones_to_plot['trendline'].iloc[i])\n\nACT Adderall    Trendline\n27.000  31.480  35.021\n29.000  42.110  38.082\n30.000  47.960  39.613\n23.000  23.260  28.898\n28.000  36.960  36.552\n28.000  33.910  36.552\n23.000  27.120  28.898\n30.000  60.000  39.613\n27.000  53.130  35.021\n24.000  30.890  30.429\n29.000  34.040  38.082\n29.000  28.330  38.082\n30.000  45.160  39.613\n30.000  43.750  39.613\n23.000  28.260  28.898\n26.000  29.070  33.490\n33.000  64.710  44.205\n30.000  48.760  39.613\n33.000  68.970  44.205\n24.000  31.330  30.429\n28.000  51.350  36.552\n32.000  28.570  42.674\n31.000  7.720   41.144\n25.000  7.180   31.959\n34.000  29.170  45.736\n27.000  62.160  35.021\n32.000  50.000  42.674\n30.000  34.690  39.613\n26.000  40.000  33.490\n30.000  43.180  39.613\n26.000  15.180  33.490\n24.000  29.070  30.429\n22.000  10.680  27.367\n24.000  12.350  30.429\n24.000  22.360  30.429\n22.000  19.650  27.367\n22.000  13.890  27.367\n20.000  11.460  24.306\n23.000  28.000  28.898\n34.000  41.940  45.736\n26.000  23.260  33.490\n27.000  35.560  35.021\n33.000  37.140  44.205\n27.000  40.540  35.021\n25.000  21.150  31.959\n24.000  34.650  30.429\n23.000  26.090  28.898\n29.000  34.180  38.082\n28.000  25.000  36.552\n31.000  54.130  41.144\n25.000  20.310  31.959\n23.000  37.680  28.898\n32.000  48.000  42.674\n32.000  47.730  42.674\n28.000  55.130  36.552\n23.000  40.000  28.898\n32.000  40.630  42.674\n30.000  33.850  39.613\n32.000  39.390  42.674\n31.000  29.410  41.144\n27.000  32.370  35.021\n34.000  40.380  45.736\n29.000  37.930  38.082\n34.000  48.810  45.736\n30.000  37.210  39.613\n34.000  43.590  45.736\n31.000  68.750  41.144\n28.000  30.000  36.552\n31.000  54.170  41.144\n30.000  31.860  39.613\n34.000  42.550  45.736\n27.000  40.820  35.021\n24.000  48.480  30.429\n22.000  36.360  27.367\n24.000  30.770  30.429\n24.000  28.260  30.429\n24.000  25.420  30.429\n24.000  14.130  30.429\n23.000  28.070  28.898\n28.000  39.470  36.552\n22.000  29.550  27.367\n26.000  50.000  33.490\n30.000  25.640  39.613\n32.000  40.910  42.674\n25.000  53.330  31.959\n24.000  38.570  30.429\n25.000  38.890  31.959\n24.000  25.560  30.429\n24.000  30.610  30.429\n27.000  25.900  35.021\n27.000  28.570  35.021\n29.000  49.810  38.082\n30.000  40.540  39.613\n25.000  50.000  31.959\n28.000  25.810  36.552\n31.000  60.240  41.144\n33.000  47.690  44.205\n26.000  32.650  33.490\n32.000  44.590  42.674\n25.000  40.560  31.959\n25.000  26.200  31.959\n26.000  37.230  33.490\n32.000  38.460  42.674\n23.000  15.940  28.898\n27.000  55.000  35.021\n35.000  30.560  47.267\n26.000  47.460  33.490\n28.000  32.840  36.552\n29.000  39.020  38.082\n26.000  15.280  33.490\n24.000  13.510  30.429\n26.000  35.560  33.490\n22.000  23.610  27.367\n30.000  51.530  39.613\n22.000  28.710  27.367\n24.000  19.640  30.429\n25.000  24.620  31.959\n28.000  30.220  36.552\n27.000  47.320  35.021\n27.000  50.000  35.021\n34.000  52.830  45.736\n24.000  28.240  30.429\n25.000  34.420  31.959\n22.000  27.660  27.367\n25.000  33.330  31.959\n31.000  46.430  41.144\n26.000  48.280  33.490\n32.000  57.500  42.674\n26.000  7.270   33.490\n23.000  41.670  28.898\n28.000  52.890  36.552\n29.000  39.130  38.082\n29.000  37.000  38.082\n29.000  44.440  38.082\n28.000  33.330  36.552\n27.000  41.670  35.021\n29.000  47.140  38.082\n24.000  22.810  30.429\n26.000  40.740  33.490\n26.000  40.910  33.490\n27.000  50.000  35.021\n28.000  30.000  36.552\n23.000  13.890  28.898\n30.000  57.380  39.613\n28.000  41.880  36.552\n24.000  31.780  30.429\n33.000  57.690  44.205\n24.000  29.270  30.429\n24.000  40.540  30.429\n28.000  34.380  36.552\n26.000  28.570  33.490\n31.000  33.330  41.144\n25.000  52.380  31.959\n25.000  35.480  31.959\n27.000  23.290  35.021\n23.000  28.570  28.898\n24.000  16.380  30.429\n32.000  48.190  42.674\n30.000  38.390  39.613\n26.000  27.270  33.490\n33.000  40.000  44.205\n25.000  22.950  31.959\n24.000  27.000  30.429\n25.000  16.220  31.959\n25.000  33.330  31.959\n24.000  32.260  30.429\n34.000  32.760  45.736\n26.000  30.770  33.490\n31.000  37.380  41.144\n26.000  42.640  33.490\n27.000  57.140  35.021\n28.000  37.840  36.552\n23.000  30.970  28.898\n27.000  35.510  35.021\n25.000  36.230  31.959\n25.000  61.760  31.959\n29.000  36.840  38.082\n23.000  35.140  28.898\n23.000  33.330  28.898\n32.000  43.480  42.674\n28.000  43.590  36.552\n30.000  36.130  39.613\n27.000  53.230  35.021\n22.000  41.670  27.367\n26.000  35.290  33.490\n31.000  38.710  41.144\n22.000  35.290  27.367\n31.000  41.380  41.144\n25.000  24.390  31.959\n31.000  21.880  41.144\n29.000  50.980  38.082\n24.000  25.860  30.429\n26.000  30.230  33.490\n30.000  34.210  39.613\n28.000  41.670  36.552\n27.000  27.910  35.021\n24.000  36.960  30.429\n26.000  42.000  33.490\n26.000  41.380  33.490\n23.000  27.420  28.898\n26.000  42.940  33.490\n24.000  20.740  30.429\n25.000  27.160  31.959\n31.000  35.480  41.144\n27.000  20.800  35.021\n28.000  33.330  36.552\n27.000  31.250  35.021\n30.000  66.670  39.613\n22.000  38.100  27.367\n27.000  44.120  35.021\n30.000  32.350  39.613\n23.000  32.690  28.898\n25.000  26.190  31.959\n24.000  30.000  30.429\n22.000  22.640  27.367\n25.000  20.000  31.959\n25.000  16.950  31.959\n31.000  58.460  41.144\n26.000  18.570  33.490\n24.000  28.950  30.429\n26.000  38.100  33.490\n27.000  50.000  35.021\n27.000  27.350  35.021\n30.000  42.310  39.613\n24.000  29.270  30.429\n34.000  20.750  45.736\n24.000  28.410  30.429\n28.000  21.430  36.552\n30.000  33.650  39.613\n25.000  41.030  31.959\n26.000  35.710  33.490\n25.000  40.480  31.959\n26.000  25.580  33.490\n29.000  44.000  38.082\n27.000  22.220  35.021\n26.000  42.590  33.490\n24.000  34.210  30.429\n26.000  32.500  33.490\n27.000  35.710  35.021\n28.000  47.480  36.552\n23.000  23.210  28.898\n27.000  31.070  35.021\n26.000  28.210  33.490\n29.000  40.840  38.082\n23.000  30.770  28.898\n29.000  63.460  38.082\n25.000  43.200  31.959\n27.000  47.100  35.021\n24.000  39.220  30.429\n25.000  34.910  31.959\n29.000  51.520  38.082\n24.000  27.270  30.429\n30.000  29.730  39.613\n33.000  31.580  44.205\n32.000  64.620  42.674\n31.000  43.330  41.144\n26.000  50.000  33.490\n28.000  40.210  36.552\n25.000  27.030  31.959\n30.000  47.930  39.613\n28.000  28.570  36.552\n27.000  38.100  35.021\n28.000  49.170  36.552\n33.000  35.810  44.205\n29.000  17.050  38.082\n27.000  16.550  35.021\n31.000  35.100  41.144\n24.000  14.930  30.429\n25.000  23.530  31.959\n31.000  33.330  41.144\n29.000  47.620  38.082\n26.000  17.200  33.490\n26.000  37.500  33.490\n28.000  30.090  36.552\n35.000  38.100  47.267\n28.000  35.430  36.552\n29.000  38.380  38.082\n25.000  18.750  31.959\n30.000  51.970  39.613\n29.000  45.830  38.082\n29.000  44.700  38.082\n30.000  37.500  39.613\n31.000  43.010  41.144\n30.000  44.390  39.613\n26.000  35.090  33.490\n27.000  24.730  35.021\n27.000  26.940  35.021\n26.000  31.030  33.490\n31.000  40.000  41.144\n28.000  55.280  36.552\n28.000  47.790  36.552\n28.000  50.440  36.552\n25.000  32.690  31.959\n28.000  25.300  36.552\n26.000  31.340  33.490\n29.000  31.940  38.082\n29.000  38.130  38.082\n25.000  40.000  31.959\n28.000  28.890  36.552\n25.000  17.500  31.959\n32.000  46.360  42.674\n32.000  41.780  42.674\n27.000  22.920  35.021\n26.000  60.530  33.490\n30.000  31.030  39.613\n27.000  55.560  35.021\n28.000  52.800  36.552\n26.000  30.000  33.490\n28.000  27.470  36.552\n24.000  11.590  30.429\n26.000  27.340  33.490\n26.000  30.770  33.490\n25.000  23.380  31.959\n25.000  19.260  31.959\n25.000  32.260  31.959\n25.000  23.260  31.959\n26.000  45.900  33.490\n32.000  45.120  42.674\n25.000  40.830  31.959\n26.000  31.820  33.490\n26.000  28.890  33.490\n26.000  23.300  33.490\n24.000  28.240  30.429\n24.000  33.330  30.429\n34.000  19.300  45.736\n29.000  44.860  38.082\n27.000  42.860  35.021\n34.000  38.100  45.736\n30.000  35.970  39.613\n30.000  35.480  39.613\n26.000  42.570  33.490\n31.000  54.290  41.144\n32.000  31.250  42.674\n30.000  46.250  39.613\n27.000  34.090  35.021\n27.000  40.000  35.021\n26.000  21.740  33.490\n29.000  47.480  38.082\n26.000  34.380  33.490\n28.000  30.120  36.552\n33.000  30.970  44.205\n27.000  23.810  35.021\n28.000  40.000  36.552\n26.000  39.290  33.490\n29.000  51.430  38.082\n25.000  44.230  31.959\n25.000  25.640  31.959\n26.000  22.790  33.490\n31.000  44.700  41.144\n31.000  18.920  41.144\n21.000  19.790  25.837\n25.000  25.000  31.959\n25.000  22.730  31.959\n32.000  47.830  42.674\n27.000  23.130  35.021\n29.000  49.300  38.082\n33.000  40.660  44.205\n30.000  28.160  39.613\n26.000  28.380  33.490\n22.000  34.020  27.367\n30.000  52.590  39.613\n26.000  25.490  33.490\n26.000  24.070  33.490\n24.000  34.860  30.429\n24.000  32.760  30.429\n25.000  25.640  31.959\n24.000  31.580  30.429\n27.000  22.080  35.021\n27.000  9.870   35.021\n24.000  30.160  30.429\n34.000  52.080  45.736\n33.000  34.380  44.205\n31.000  50.850  41.144\n26.000  25.630  33.490\n27.000  43.480  35.021\n33.000  62.500  44.205\n29.000  41.380  38.082\n25.000  28.480  31.959\n34.000  27.140  45.736\n26.000  21.650  33.490\n33.000  36.670  44.205\n26.000  51.580  33.490\n24.000  32.610  30.429\n23.000  21.670  28.898\n25.000  31.430  31.959\n25.000  42.150  31.959\n28.000  27.030  36.552\n22.000  45.450  27.367\n28.000  27.500  36.552\n25.000  37.040  31.959\n29.000  25.640  38.082\n25.000  32.350  31.959\n25.000  22.730  31.959\n27.000  48.840  35.021\n35.000  27.910  47.267\n25.000  34.620  31.959\n\n\n\n\n“When I looked at the NSDUH data, I found that college students who had used Adderall non-medically reported significantly higher levels of depression and were more likely to have considered suicide.”\n\ndef lookAtDepressionLevels(nsduh_data, idxs):\n    nsduh_data['seriously_thought_about_killing_oneself_last_year'] = 1.* (nsduh_data['MHSUITHK'] == 1)\n    nsduh_data['worst_K6_score_past_year'] = 1.*nsduh_data['K6SCMAX']\n    data_to_use = nsduh_data.loc[idxs]\n    weights = data_to_use['ANALWT_C']\n    stratifyByCat(data_to_use, 'seriously_thought_about_killing_oneself_last_year')  \n\n    model1 = sm.Logit.from_formula('ADDERALL ~ seriously_thought_about_killing_oneself_last_year', \n                                 weights = weights, \n                                 data = data_to_use).fit()\n    model2 = sm.Logit.from_formula('ADDERALL ~ seriously_thought_about_killing_oneself_last_year +  IRSEX + C(IRFSTAMP, Sum) + C(NEWRACE2, Sum)', \n                                 weights = weights, \n                                 data = data_to_use).fit()\n    model3 = sm.Logit.from_formula('ADDERALL ~ worst_K6_score_past_year', \n                                 weights = weights, \n                                 data = data_to_use).fit()\n    model4 = sm.Logit.from_formula('ADDERALL ~ worst_K6_score_past_year + IRSEX + C(IRFSTAMP, Sum) + C(NEWRACE2, Sum)', \n                                 weights = weights, \n                                 data = data_to_use).fit()\n\n\n    models = [model1, model2, model3, model4]\n    print summary_col(models, model_names = range(len(models)), stars = True,\n                      regressor_order = ['seriously_thought_about_killing_oneself_last_year', 'worst_K6_score_past_year'],\n                      float_format='%0.3f',\n                      info_dict={'N':lambda x: \"{0:d}\".format(int(x.nobs))})\ncollege_idxs = nsduh_data['COLLENR'] == 1\nadult_idxs = nsduh_data['CATAG6'] &gt;= 3\n\nprint '\\n\\nLooking at depression levels in college students'\nlookAtDepressionLevels(nsduh_data, college_idxs)\nprint '\\n\\nLooking at depression levels in adults'\nlookAtDepressionLevels(nsduh_data, adult_idxs)\n\n\n\nLooking at depression levels in college students\nCategory seriously_thought_about_killing_oneself_last_year\nMean value of ADDERALL for level 0.0 is 0.143; unweighted, 0.136 (3944 values)\nMean value of ADDERALL for level 1.0 is 0.213; unweighted, 0.191 (366 values)\nRatio between maximum value and minimum value 1.48723023239\nOptimization terminated successfully.\n         Current function value: 0.404718\n         Iterations 6\nOptimization terminated successfully.\n         Current function value: 0.389079\n         Iterations 7\nOptimization terminated successfully.\n         Current function value: 0.403017\n         Iterations 6\nOptimization terminated successfully.\n         Current function value: 0.386725\n         Iterations 7\n\n=========================================================================================\n                                                      0         1         2         3    \n-----------------------------------------------------------------------------------------\nseriously_thought_about_killing_oneself_last_year 0.410***  0.435***                     \n                                                  (0.141)   (0.144)                      \nworst_K6_score_past_year                                              0.033***  0.039*** \n                                                                      (0.007)   (0.007)  \nC(IRFSTAMP, Sum)[S.1]                                       -0.059              -0.076   \n                                                            (0.089)             (0.089)  \nC(NEWRACE2, Sum)[S.1]                                       0.746***            0.754*** \n                                                            (0.186)             (0.186)  \nC(NEWRACE2, Sum)[S.2]                                       -0.693***           -0.676***\n                                                            (0.248)             (0.249)  \nC(NEWRACE2, Sum)[S.3]                                       0.152               0.209    \n                                                            (0.549)             (0.552)  \nC(NEWRACE2, Sum)[S.4]                                       -0.499              -0.554   \n                                                            (0.892)             (0.893)  \nC(NEWRACE2, Sum)[S.5]                                       -0.038              -0.056   \n                                                            (0.245)             (0.245)  \nC(NEWRACE2, Sum)[S.6]                                       0.562**             0.533**  \n                                                            (0.249)             (0.250)  \nIRSEX                                                       -0.373***           -0.439***\n                                                            (0.090)             (0.091)  \nIntercept                                         -1.852*** -1.781*** -2.058*** -1.948***\n                                                  (0.047)   (0.236)   (0.070)   (0.240)  \nN                                                 4310      4310      4310      4310     \n=========================================================================================\nStandard errors in parentheses.\n* p&lt;.1, ** p&lt;.05, ***p&lt;.01\n\n\nLooking at depression levels in adults\nCategory seriously_thought_about_killing_oneself_last_year\nMean value of ADDERALL for level 0.0 is 0.026; unweighted, 0.034 (18526 values)\nMean value of ADDERALL for level 1.0 is 0.067; unweighted, 0.093 (756 values)\nRatio between maximum value and minimum value 2.54581324356\nOptimization terminated successfully.\n         Current function value: 0.153638\n         Iterations 7\nOptimization terminated successfully.\n         Current function value: 0.148860\n         Iterations 8\nOptimization terminated successfully.\n         Current function value: 0.147275\n         Iterations 8\nOptimization terminated successfully.\n         Current function value: 0.142961\n         Iterations 8\n\n=========================================================================================\n                                                      0         1         2         3    \n-----------------------------------------------------------------------------------------\nseriously_thought_about_killing_oneself_last_year 1.074***  0.908***                     \n                                                  (0.132)   (0.135)                      \nworst_K6_score_past_year                                              0.098***  0.097*** \n                                                                      (0.005)   (0.006)  \nC(IRFSTAMP, Sum)[S.1]                                       0.292***            0.157*** \n                                                            (0.046)             (0.048)  \nC(NEWRACE2, Sum)[S.1]                                       0.565***            0.529*** \n                                                            (0.111)             (0.112)  \nC(NEWRACE2, Sum)[S.2]                                       -0.848***           -0.825***\n                                                            (0.190)             (0.191)  \nC(NEWRACE2, Sum)[S.3]                                       0.490*              0.433    \n                                                            (0.263)             (0.267)  \nC(NEWRACE2, Sum)[S.4]                                       0.225               0.309    \n                                                            (0.446)             (0.448)  \nC(NEWRACE2, Sum)[S.5]                                       -0.461*             -0.432   \n                                                            (0.268)             (0.270)  \nC(NEWRACE2, Sum)[S.6]                                       0.750***            0.668*** \n                                                            (0.199)             (0.201)  \nIRSEX                                                       -0.262***           -0.415***\n                                                            (0.078)             (0.080)  \nIntercept                                         -3.357*** -3.090*** -3.948*** -3.505***\n                                                  (0.041)   (0.158)   (0.061)   (0.163)  \nN                                                 19282     19282     19282     19282    \n=========================================================================================\nStandard errors in parentheses.\n* p&lt;.1, ** p&lt;.05, ***p&lt;.01\n\n\n\n\n“I found that Google searches for Adderall in college towns spiked during exam months and drop during summer months.”\n\ncollege_towns_to_percentages = {}\n#read in college towns. \nfor line in open('college_town_percentages'):\n    line = ' '.join(line.split()[1:])\n    city_name = line.split('(')[0].strip()\n    percentage = float(line.split('(')[1].replace('%)', ''))\n    if city_name in d.index:\n        college_towns_to_percentages[city_name] = percentage\n#reprocess google data into easier-to-analyze format. \ndef createGoogleSearchDataframe(all_searches, college_towns_to_percentages):\n    df = {'month':[], 'year':[], 'town':[]}\n    for k in all_searches:\n        df[k] = []\n        df[k + '_normalized_by_town'] = []\n        df[k + '_zero_meaned_by_town'] = []\n    \n    for i, town_name in enumerate(college_towns_to_percentages.keys()):\n        if i % 10 == 0:\n            print i,'towns processed out of', len(d.index)\n        \n        mean_vals_by_town = {}\n        bad_town = False\n        for k in all_searches:\n            mean_vals_by_town[k] = {}\n            if town_name not in all_searches[k].index:\n                print town_name, 'not in index'\n                bad_town = True\n            if town_name in all_searches[k].index:\n                mean_vals_by_town[k]['mu'] = all_searches[k].loc[town_name].mean()\n                mean_vals_by_town[k]['sigma'] = all_searches[k].loc[town_name].std()\n        if bad_town:\n            continue\n        for date in all_searches['Adderall'].columns:\n            month = date.split('-')[1]\n            year = date.split('-')[0]\n            df['month'].append(month)\n            df['year'].append(year)\n            df['town'].append(town_name)\n            for k in all_searches.keys():\n                search_volume = float(all_searches[k].loc[town_name][date])\n                df[k].append(search_volume)\n                df[k + '_normalized_by_town'].append((search_volume - mean_vals_by_town[k]['mu']) / mean_vals_by_town[k]['sigma'])\n                df[k + '_zero_meaned_by_town'].append((search_volume - mean_vals_by_town[k]['mu']))\n\n    for a in df.keys():\n        print a, len(df[a])\n    df = pd.DataFrame(df)\n    return df\ngoogle_search_dataframe = createGoogleSearchDataframe(google_data, college_towns_to_percentages)\n\n#Do regressions for various adjustments to the Google data to confirm we see the same pattern. \n#Eg, if we subtract off the mean for each town, do we still see it? What if we divide by each town's standard deviation...? Pattern is robust to all these things. \nfor adjustment_to_use in ['', '_normalized_by_town', '_zero_meaned_by_town']:\n    figure(figsize = [5, 5])\n    for k in ['Adderall', 'ADHD']:\n        model = sm.OLS.from_formula('%s ~ C(month, Sum) + C(year, Sum)' \n                                    % (k + adjustment_to_use), data = google_search_dataframe).fit()\n        month_coefs = []\n        month_ticks = []\n        for param in model.params.index:\n            if 'month' in param:\n                month_coefs.append(model.params[param])\n                month_ticks.append(param.split('S.')[1].replace(']', ''))\n        month_coefs.append(- np.mean(month_coefs))\n        month_ticks.append('12')\n        plot(range(len(month_coefs)), month_coefs, label = k)\n        xticks(range(len(month_coefs)), month_ticks, rotation = 90)\n    ylim([-1, 1])\n    legend(loc = 3, fontsize = 8)\n\n    xlabel('Month')\n    ylabel('Normalized Search Rate in College Towns')\n    title('Adjustment:' +adjustment_to_use)\n\n0 towns processed out of 2084\nHamilton, New York not in index\nFarmville, Virginia not in index\n10 towns processed out of 2084\nCanton, New York not in index\nOrono, Maine not in index\n20 towns processed out of 2084\nMenomonie, Wisconsin not in index\nLexington, Virginia not in index\n30 towns processed out of 2084\n40 towns processed out of 2084\nGeneseo, New York not in index\nKutztown, Pennsylvania not in index\n50 towns processed out of 2084\n60 towns processed out of 2084\nFrostburg, Maryland not in index\n70 towns processed out of 2084\ntown 4355\nADHD 4355\nADHD_zero_meaned_by_town 4355\nmonth 4355\nADHD_normalized_by_town 4355\nyear 4355\nAdderall 4355\nAdderall_normalized_by_town 4355\nAdderall_zero_meaned_by_town 4355\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“In fact, NSDUH data showed that adults whose family incomes were below 10,000 had the highest rates of nonmedical Adderall use, and those whose family incomes were greater than 75,000 had the lowest.”\n\n#run regressions + stratify by income levels to confirm that we observe income effects.\nadult_idxs = nsduh_data['CATAG6'] &gt;= 3\n\ndata_to_use = nsduh_data.loc[adult_idxs]\nweights = data_to_use['ANALWT_C']\nprint 'Examining Adderall use frequency in adults as a function of family income'\nprint 'Ritalin results are qualitatively similar although the effects seem to be weaker'\nprint '\\nCategories for total family income:\\n1: &lt;10k, 2: 10-20, 3: 20-30; 4: 30-40; 5: 40-50; 6: 50-75; 7: 75'\nstratifyByCat(data_to_use, 'IRFAMIN3')  \nstratifyByCat(data_to_use, 'IRFAMIN3', drug_to_measure = 'RITALIN')\n\n\nmodel1 = sm.Logit.from_formula('ADDERALL ~ C(IRFAMIN3)', data = data_to_use, weights = weights).fit()#treat income as categorical variable\nmodel2 = sm.Logit.from_formula('ADDERALL ~ IRFAMIN3', data = data_to_use, weights = weights).fit()#treat as continuous variable (bad, bad, bad)\nmodel3 = sm.Logit.from_formula('ADDERALL ~ IRSEX + C(CATAG6, Sum) + C(NEWRACE2, Sum) + C(IRFAMIN3)', data = data_to_use, weights = weights).fit()#control for other covariates: age, race, sex. \nmodel4 = sm.Logit.from_formula('RITALIN ~ C(IRFAMIN3)', data = data_to_use, weights = weights).fit()\nmodel5 = sm.Logit.from_formula('RITALIN ~ IRFAMIN3', data = data_to_use, weights = weights).fit()\nmodel6 = sm.Logit.from_formula('RITALIN ~ IRSEX + C(CATAG6, Sum) + C(NEWRACE2, Sum) + C(IRFAMIN3)', data = data_to_use, weights = weights).fit()\n\nmodels = [model1, model2, model3, model4, model5, model6]\nprint summary_col(models, model_names = ['ADDERALL', 'ADDERALL', 'ADDERALL', 'RITALIN', 'RITALIN', 'RITALIN'], stars = True,\n                  float_format='%0.3f',\n                  info_dict={'N':lambda x: \"{0:d}\".format(int(x.nobs))})\n\nExamining Adderall use frequency in adults as a function of family income\nRitalin results are qualitatively similar although the effects seem to be weaker\n\nCategories for total family income:\n1: &lt;10k, 2: 10-20, 3: 20-30; 4: 30-40; 5: 40-50; 6: 50-75; 7: 75\nCategory IRFAMIN3\nMean value of ADDERALL for level 1 is 0.037; unweighted, 0.053 (1132 values)\nMean value of ADDERALL for level 2 is 0.025; unweighted, 0.036 (2308 values)\nMean value of ADDERALL for level 3 is 0.032; unweighted, 0.042 (2102 values)\nMean value of ADDERALL for level 4 is 0.032; unweighted, 0.040 (2100 values)\nMean value of ADDERALL for level 5 is 0.029; unweighted, 0.036 (2085 values)\nMean value of ADDERALL for level 6 is 0.028; unweighted, 0.033 (3418 values)\nMean value of ADDERALL for level 7 is 0.024; unweighted, 0.031 (6137 values)\nRatio between maximum value and minimum value 1.52577800068\nCategory IRFAMIN3\nMean value of RITALIN for level 1 is 0.030; unweighted, 0.038 (1132 values)\nMean value of RITALIN for level 2 is 0.018; unweighted, 0.025 (2308 values)\nMean value of RITALIN for level 3 is 0.024; unweighted, 0.031 (2102 values)\nMean value of RITALIN for level 4 is 0.026; unweighted, 0.031 (2100 values)\nMean value of RITALIN for level 5 is 0.018; unweighted, 0.022 (2085 values)\nMean value of RITALIN for level 6 is 0.022; unweighted, 0.026 (3418 values)\nMean value of RITALIN for level 7 is 0.020; unweighted, 0.025 (6137 values)\nRatio between maximum value and minimum value 1.67773828665\nOptimization terminated successfully.\n         Current function value: 0.154529\n         Iterations 7\nOptimization terminated successfully.\n         Current function value: 0.154656\n         Iterations 7\nOptimization terminated successfully.\n         Current function value: 0.132304\n         Iterations 11\nOptimization terminated successfully.\n         Current function value: 0.124139\n         Iterations 8\nOptimization terminated successfully.\n         Current function value: 0.124316\n         Iterations 8\nOptimization terminated successfully.\n         Current function value: 0.111418\n         Iterations 10\n\n==========================================================================================\n                      ADDERALL I ADDERALL II ADDERALL III RITALIN I RITALIN II RITALIN III\n------------------------------------------------------------------------------------------\nC(CATAG6, Sum)[S.3]                          2.208***                          1.373***   \n                                             (0.186)                           (0.104)    \nC(CATAG6, Sum)[S.4]                          0.742***                          0.467***   \n                                             (0.191)                           (0.110)    \nC(CATAG6, Sum)[S.5]                          -0.150                            -0.244*    \n                                             (0.215)                           (0.139)    \nC(IRFAMIN3)[T.2]      -0.406**               -0.387**     -0.426**             -0.426**   \n                      (0.173)                (0.181)      (0.205)              (0.210)    \nC(IRFAMIN3)[T.3]      -0.236                 -0.365**     -0.213               -0.360*    \n                      (0.171)                (0.179)      (0.200)              (0.206)    \nC(IRFAMIN3)[T.4]      -0.283                 -0.458**     -0.196               -0.401*    \n                      (0.173)                (0.181)      (0.200)              (0.205)    \nC(IRFAMIN3)[T.5]      -0.405**               -0.650***    -0.560***            -0.837***  \n                      (0.177)                (0.185)      (0.215)              (0.221)    \nC(IRFAMIN3)[T.6]      -0.484***              -0.825***    -0.390**             -0.785***  \n                      (0.163)                (0.172)      (0.189)              (0.195)    \nC(IRFAMIN3)[T.7]      -0.572***              -0.834***    -0.421**             -0.811***  \n                      (0.152)                (0.161)      (0.175)              (0.182)    \nC(NEWRACE2, Sum)[S.1]                        0.795***                          1.001***   \n                                             (0.114)                           (0.139)    \nC(NEWRACE2, Sum)[S.2]                        -0.798***                         -1.727***  \n                                             (0.191)                           (0.346)    \nC(NEWRACE2, Sum)[S.3]                        0.424                             0.441      \n                                             (0.269)                           (0.334)    \nC(NEWRACE2, Sum)[S.4]                        0.156                             0.411      \n                                             (0.451)                           (0.516)    \nC(NEWRACE2, Sum)[S.5]                        -0.606**                          -0.480     \n                                             (0.270)                           (0.329)    \nC(NEWRACE2, Sum)[S.6]                        0.858***                          1.182***   \n                                             (0.203)                           (0.226)    \nIRFAMIN3                         -0.068***                          -0.042*               \n                                 (0.019)                            (0.022)               \nIRSEX                                        -0.264***                         -0.326***  \n                                             (0.080)                           (0.090)    \nIntercept             -2.883***  -2.963***   -3.964***    -3.232*** -3.379***  -3.748***  \n                      (0.133)    (0.096)     (0.274)      (0.155)   (0.112)    (0.256)    \nN                     19282      19282       19282        19282     19282      19282      \n==========================================================================================\nStandard errors in parentheses.\n* p&lt;.1, ** p&lt;.05, ***p&lt;.01"
  },
  {
    "objectID": "ml-models-exercise/ml-model-exercise.html",
    "href": "ml-models-exercise/ml-model-exercise.html",
    "title": "Machine Learning Model Exercise",
    "section": "",
    "text": "Load packages and choose base seed number.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(ranger)\nlibrary(here)\n\nhere() starts at /Users/annalisecramer/MADA/annalisecramer-MADA-portfolio-main/annalisecramer-MADA-portfolio\n\nrnseed &lt;- 1234\n\nStart by loading data that we left off on in the fitting exercise.\n\nhere()\n\n[1] \"/Users/annalisecramer/MADA/annalisecramer-MADA-portfolio-main/annalisecramer-MADA-portfolio\"\n\ndata4 &lt;- readRDS(here(\"fitting-exercise\", \"data\", \"data4.rds\"))\n\nI can’t find what the race variable repsonses encoded for. I kind of rememebr from a few weeks ago that there was a multiple races option. Below, I’ll recode 7 and 88 into 3.\n\ndata5 &lt;- data4 %&gt;%\n  mutate(RACE = ifelse(RACE %in% c(7, 88), 3, RACE))\n         \nunique(data5$RACE)\n\n[1] 2 1 3\n\n\nNext, let’s make a pairwise correlation plot for the continuous variables, which are age, weight, height. The correlations don’t look too high/low, so I’m not going to remove any of them.\n\n#pull out continuous variables\ncor_matrix &lt;- cor(data5[, c(\"AGE\", \"WT\", \"HT\")], use = \"complete.obs\")\n\n#make correlation plot\ncorrplot(cor_matrix, method = \"color\", type = \"upper\", tl.col = \"black\", tl.cex = 1.2, addCoef.col = \"black\")\n\n\n\n\n\n\n\n\nNow, I will create a BMI variable using height and weight, which are in metric units.\n\ndata6 &lt;- data5 %&gt;%\n  mutate(BMI = WT / (HT)^2) #bmi formula\n\nunique(data6$BMI) #checking\n\n  [1] 30.10 25.96 21.92 28.43 26.42 22.13 25.68 20.68 23.99 30.57 26.20 23.77\n [13] 26.46 25.95 23.60 26.97 30.25 25.53 24.66 24.68 26.33 26.47 25.66 31.17\n [25] 25.96 28.49 29.29 27.48 23.23 27.35 24.23 26.17 26.06 31.27 26.81 27.47\n [37] 31.75 30.29 28.51 30.25 21.12 32.21 22.78 29.04 29.83 30.40 29.06 18.69\n [49] 22.31 29.97 28.28 29.69 25.06 24.56 23.07 24.18 24.46 26.01 28.44 30.80\n [61] 21.19 20.95 23.87 25.99 27.79 30.65 26.32 31.35 25.30 30.47 30.19 21.38\n [73] 23.25 22.93 28.10 24.86 30.03 23.89 26.45 20.43 24.69 28.46 29.99 22.18\n [85] 30.21 27.88 20.34 21.50 21.94 29.60 24.56 25.73 26.14 25.96 21.00 25.56\n [97] 28.93 30.16 31.89 23.59 27.81 30.95 24.98 24.98 25.91 23.40 31.28 30.07\n[109] 25.18 29.74 28.12 26.78 25.78 28.20 30.50 31.03 26.58 31.69 29.07\n\n\nLet’s construct 3 models. For the first model, we’ll revisit the one we had previously, namely a linear model with all predictors. For our second model, we’ll use LASSO regression. For our third model, we’ll do a random forest (RF).\n\n########## Linear Model ########################################################\n# Define the model specification using lm\nlm_all &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n# Create a workflow\nlm_all_wf &lt;- workflow() %&gt;%\n  add_model(lm_all) %&gt;%\n  add_formula(Y ~ .)\n\n########## LASSO Regression ####################################################\nset.seed(rnseed)\n\n# Define a LASSO model with a fixed penalty\nlasso_all &lt;- linear_reg(penalty = 0.1) %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_mode(\"regression\")\n\n# Create a workflow\nlasso_all_wf &lt;- workflow() %&gt;% \n  add_model(lasso_all) %&gt;% \n  add_formula(Y ~ .)\n\n########## Random Forest Model #################################################\nset.seed(rnseed)\n\n# Define the model specification using the ranger engine\nrf_all &lt;- rand_forest(mode = \"regression\") %&gt;%\n  set_engine(\"ranger\", seed = rnseed)\n\n# Create the workflow\nrf_all_wf &lt;- workflow() %&gt;% \n  add_model(rf_all) %&gt;% \n  add_formula(Y ~ .)\n\nNow, we will fit all 3 models, and calculate the RMSE for them to compare.\n\n########## Linear Model ########################################################\n# Fit the model on training data\nlm_all_fit &lt;- lm_all_wf %&gt;% fit(data6)\n\n# Compute predictions\npreds_lm_all &lt;- predict(lm_all_fit, data6) %&gt;% bind_cols(data6)\n\n# Compute RMSE\nrmse_lm_all &lt;- rmse(preds_lm_all, truth = Y, estimate = .pred)\npaste0(\"RMSE Linear Model: \", rmse_lm_all$.estimate)\n\n[1] \"RMSE Linear Model: 581.417690122579\"\n\n########## LASSO Regression ####################################################\n# Fit the model on training data\nlasso_all_fit &lt;- lasso_all_wf %&gt;% fit(data6)\n\n# Compute predictions on training data\npreds_lasso_all &lt;- predict(lasso_all_fit, data6) %&gt;% bind_cols(data6)\n\n# Compute RMSE\nrmse_lasso_all &lt;- rmse(preds_lasso_all, truth = Y, estimate = .pred)\npaste0(\"RMSE LASSO Model: \", rmse_lasso_all$.estimate)\n\n[1] \"RMSE LASSO Model: 581.46647664791\"\n\n########## Random Forest Model #################################################\n# Fit the model on the training data\nrf_all_fit &lt;- rf_all_wf %&gt;% fit(data6)\n\n# Compute predictions on training data\npreds_rf_all &lt;- predict(rf_all_fit, data6) %&gt;% bind_cols(data6)\n\n# Compute RMSE\nrmse_rf_all &lt;- rmse(preds_rf_all, truth = Y, estimate = .pred)\npaste0(\"RMSE Random Forest Model: \", rmse_rf_all$.estimate)\n\n[1] \"RMSE Random Forest Model: 357.645737679099\"\n\n\nWe will tune the LASSO and random forest models’ hyperparameters. The linear model from earlier doesn’t have hyperparamters to tune.\nIn the LASSO results plot, we see the RMSE is lowest when the penalty is lowest, which means it’s behaving more like a regular linear model (OLS model), in which LASSO isn’t doing a lot of regularization. With higher penalties, LASSO will cut coefficients to reduce complexity. However, doing this increases bias which increaes RMSE.\n\n########## LASSO Tuning ####################################################\nset.seed(rnseed)\n\n# Define a grid of penalty values on a log scale\nlasso_grid &lt;- 10^seq(log10(1E-5), log10(1E2), length.out = 50)\n\n# Update the LASSO model to use tune() for the penalty\nlasso_tune &lt;- linear_reg(penalty = tune()) %&gt;% \n  set_engine(\"glmnet\") %&gt;% \n  set_mode(\"regression\")\n\n# Create a workflow\nlasso_tune_wf &lt;- workflow() %&gt;% \n  add_model(lasso_tune) %&gt;% \n  add_formula(Y ~ .)\n\n# Create resampling object\nresamples &lt;- apparent(data6)\n\n# Perform tuning using apparent resampling\nlasso_tune_res &lt;- tune_grid(\n  lasso_tune_wf,\n  resamples = resamples,\n  grid = tibble(penalty = lasso_grid),\n  metrics = metric_set(rmse),\n  control = control_grid(save_pred = TRUE)\n)\n\n# Collect results\nlasso_results &lt;- collect_metrics(lasso_tune_res)\nlasso_rmse &lt;- lasso_tune_res$.metrics[[1]]\nglimpse(lasso_rmse)\n\nRows: 50\nColumns: 5\n$ penalty    &lt;dbl&gt; 1.000000e-05, 1.389495e-05, 1.930698e-05, 2.682696e-05, 3.7…\n$ .metric    &lt;chr&gt; \"rmse\", \"rmse\", \"rmse\", \"rmse\", \"rmse\", \"rmse\", \"rmse\", \"rm…\n$ .estimator &lt;chr&gt; \"standard\", \"standard\", \"standard\", \"standard\", \"standard\",…\n$ .estimate  &lt;dbl&gt; 581.4664, 581.4664, 581.4664, 581.4664, 581.4664, 581.4664,…\n$ .config    &lt;chr&gt; \"Preprocessor1_Model01\", \"Preprocessor1_Model02\", \"Preproce…\n\n# Plot, autoplot isn't working, chatGPT says to plot it manually, lasso_results not plotting\nggplot(lasso_rmse, aes(x = penalty, y = .estimate)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"LASSO Tuning Results\", \n       x = \"Penalty (log scale)\", \n       y = \"RMSE\")\n\n\n\n\n\n\n\n########## Random Forest Tuning ############################################\nset.seed(rnseed)\n\n# Define the RF model with tunable hyperparameters\nrf_tune &lt;- rand_forest(mtry = tune(), min_n = tune(), trees = 300) %&gt;%\n  set_engine(\"ranger\", seed = rnseed) %&gt;%\n  set_mode(\"regression\")\n\n# Create a workflow\nrf_tune_wf &lt;- workflow() %&gt;%\n  add_model(rf_tune) %&gt;%\n  add_formula(Y ~ .)\n\n# Define tuning grid for mtry (1 to 7) and min_n (1 to 21)\nrf_grid &lt;- grid_regular(\n  mtry(range = c(1, 7)),\n  min_n(range = c(1, 21)),\n  levels = c(7, 7)\n)\n\n# Create resampling object\nresamples &lt;- apparent(data6)\n\n# Perform tuning\nrf_tune_res &lt;- tune_grid(\n  rf_tune_wf,\n  resamples = resamples,\n  grid = rf_grid,\n  metrics = metric_set(rmse),\n  control = control_grid(save_pred = TRUE)\n)\n\n# Collect tuning results\nrf_results &lt;- rf_tune_res$.metrics[[1]]\nglimpse(rf_results)\n\nRows: 49\nColumns: 6\n$ mtry       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6,…\n$ min_n      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7,…\n$ .metric    &lt;chr&gt; \"rmse\", \"rmse\", \"rmse\", \"rmse\", \"rmse\", \"rmse\", \"rmse\", \"rm…\n$ .estimator &lt;chr&gt; \"standard\", \"standard\", \"standard\", \"standard\", \"standard\",…\n$ .estimate  &lt;dbl&gt; 499.6873, 283.0590, 254.8678, 254.3069, 249.9474, 250.8785,…\n$ .config    &lt;chr&gt; \"Preprocessor1_Model01\", \"Preprocessor1_Model02\", \"Preproce…\n\n# Plot, autoplot isn't working, chatGPT says to plot it manually, lasso_results not plotting\nggplot(rf_results, aes(x = min_n, y = .estimate, color = factor(mtry))) +\n  geom_line() +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"RF Tuning\",\n       x = \"min_n\",\n       y = \"RMSE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUp next, we will do this with CV, tuning both the LASSO and RF. We’ll now create real samples. Let’s do 5-fold cross-validation, 5 times repeated. Then redo the tuning of both models.\nComparing the plots to the plots from earlier, we can see the LASSO plots look the same. The RF looks completely different now, with very few predicitors the RMSE is very high but as the number of predictors increaes, the RMSE lowers and flattens out at a vlaue that is still higher than prior to using CV. RF has a atendency to overfit when given a lot of options (like we did with CV), so that’s my guess as to why RMSE is higher here than before. Overall, I think the LASSO model preforms better.\n\n#### LASSO Regression ##########################################################\nset.seed(rnseed)  # Set seed\n\n# Create a 5-fold cross-validation repeated 5 times\ncv_folds &lt;- vfold_cv(data6, v = 5, repeats = 5)\n\n# Define the LASSO model with tuning\nlasso_tune &lt;- linear_reg(penalty = tune()) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Create a workflow\nlasso_tune_wf &lt;- workflow() %&gt;%\n  add_model(lasso_tune) %&gt;%\n  add_formula(Y ~ .)\n\n# Perform tuning with real cross-validation\nlasso_tune_res &lt;- tune_grid(\n  lasso_tune_wf,\n  resamples = cv_folds,  # Use real resamples now\n  grid = tibble(penalty = lasso_grid),\n  metrics = metric_set(rmse),\n  control = control_grid(save_pred = TRUE)\n)\n\n# Collect results and visualize\nautoplot(lasso_tune_res)\n\n\n\n\n\n\n\nlasso_results &lt;- collect_metrics(lasso_tune_res)\n\nggplot(lasso_results, aes(x = penalty, y = mean)) +\n  geom_point() +\n  geom_line() +\n  scale_x_log10() +\n  labs(title = \"LASSO Tuning Results (Using 5x5 CV)\", \n       x = \"Penalty (log scale)\", \n       y = \"RMSE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n#### Random Forest Model #######################################################\nset.seed(rnseed)  # Set seed\n\n# Create a 5-fold cross-validation, repeated 5 times\ncv_folds &lt;- vfold_cv(data6, v = 5, repeats = 5)\n\n# Define the RF model with tunable hyperparameters\nrf_tune &lt;- rand_forest(mtry = tune(), min_n = tune(), trees = 300) %&gt;%\n  set_engine(\"ranger\", seed = rnseed) %&gt;%\n  set_mode(\"regression\")\n\n# Create a workflow\nrf_tune_wf &lt;- workflow() %&gt;%\n  add_model(rf_tune) %&gt;%\n  add_formula(Y ~ .)\n\n# Define tuning grid for mtry (1 to 7) and min_n (1 to 21)\nrf_grid &lt;- grid_regular(\n  mtry(range = c(1, 7)),\n  min_n(range = c(1, 21)),\n  levels = c(7, 7)  # This defines a 7x7 grid\n)\n\n# Perform tuning with real cross-validation\nrf_tune_res &lt;- tune_grid(\n  rf_tune_wf,\n  resamples = cv_folds,  # Use real cross-validation samples\n  grid = rf_grid,\n  metrics = metric_set(rmse),\n  control = control_grid(save_pred = TRUE)\n)\n\n# Collect tuning results\nautoplot(rf_tune_res)  # Automatically plot tuning results\n\n\n\n\n\n\n\nrf_results &lt;- collect_metrics(rf_tune_res)\n\nggplot(rf_results, aes(x = min_n, y = mean, color = factor(mtry))) +\n  geom_point() +\n  geom_line() +\n  scale_x_log10() +\n  labs(title = \"Random Forest Tuning Results (Using 5x5 CV)\", \n       x = \"min_n\", \n       y = \"RMSE\") +\n  theme_minimal()"
=======
    "text": "I wanted to challenge myself to become better at making maps, since this is an area that I need improvement on. I chose a fun article about Americans’ favorite thanksgiving side dishes. Here’s the original plot I will attempt to recreate.\n To start, I’ll load packages and data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at C:/Users/mz13171/Downloads/MADA-course/annalisecramer-MADA-portfolio\n\nhere()\n\n[1] \"C:/Users/mz13171/Downloads/MADA-course/annalisecramer-MADA-portfolio\"\n\nthanksgiving &lt;- read_csv(here(\"presentation-exercise/thanksgiving-2015/thanksgiving-2015-poll-data.csv\"))\n\nNew names:\nRows: 1058 Columns: 65\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(64): Do you celebrate Thanksgiving?, What is typically the main dish at... dbl\n(1): RespondentID\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `Which of these side dishes aretypically served at your Thanksgiving dinner?\n  Please select all that apply. - Other (please specify)` -&gt; `Which of these\n  side dishes aretypically served at your Thanksgiving dinner? Please select\n  all that apply. - Other (please specify)...25`\n• `Which of these side dishes aretypically served at your Thanksgiving dinner?\n  Please select all that apply. - Other (please specify)` -&gt; `Which of these\n  side dishes aretypically served at your Thanksgiving dinner? Please select\n  all that apply. - Other (please specify)...26`\n• `Which type of pie is typically served at your Thanksgiving dinner? Please\n  select all that apply. - Other (please specify)` -&gt; `Which type of pie is\n  typically served at your Thanksgiving dinner? Please select all that apply. -\n  Other (please specify)...38`\n• `Which type of pie is typically served at your Thanksgiving dinner? Please\n  select all that apply. - Other (please specify)` -&gt; `Which type of pie is\n  typically served at your Thanksgiving dinner? Please select all that apply. -\n  Other (please specify)...39`\n• `Which of these desserts do you typically have at Thanksgiving dinner? Please\n  select all that apply.  - Other (please specify)` -&gt; `Which of these desserts\n  do you typically have at Thanksgiving dinner? Please select all that apply.\n  - Other (please specify)...50`\n• `Which of these desserts do you typically have at Thanksgiving dinner? Please\n  select all that apply.  - Other (please specify)` -&gt; `Which of these desserts\n  do you typically have at Thanksgiving dinner? Please select all that apply.\n  - Other (please specify)...51`\n\n\nHere’s my final plot. I used a color picker app to get the right colors, but the yellow and tealstill looks a little bit too pale.\nAfter several hours, I cannot figure out how to get Alaska and Hawaii. Some of my attempts are pictured further down in this document.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(mapdata)\n\n# Define region colors and labels\nregion_data &lt;- data.frame(\n  state = tolower(c(\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Rhode Island\", \"Connecticut\", \n                    \"New York\", \"New Jersey\", \"Pennsylvania\", \n                    \"Wisconsin\", \"Michigan\", \"Illinois\", \"Indiana\", \"Ohio\", \n                    \"North Dakota\", \"South Dakota\", \"Nebraska\", \"Kansas\", \"Minnesota\", \"Iowa\", \"Missouri\", \n                    \"Delaware\", \"Maryland\", \"Virginia\", \"West Virginia\", \"North Carolina\", \"South Carolina\", \"Georgia\", \"Florida\", \n                    \"Kentucky\", \"Tennessee\", \"Mississippi\", \"Alabama\", \n                    \"Oklahoma\", \"Texas\", \"Arkansas\", \"Louisiana\", \n                    \"Montana\", \"Idaho\", \"Wyoming\", \"Nevada\", \"Utah\", \"Colorado\", \"Arizona\", \"New Mexico\", \n                    \"Washington\", \"Oregon\", \"California\", \"Alaska\", \"Hawaii\")),\n\n  region = c(rep(\"SQUASH\", 6),  # New England\n             rep(\"SQUASH\", 3),  # Middle Atlantic\n             rep(\"ROLLS/BISCUITS\", 5),  # East North Central\n             rep(\"GREEN BEANS/CASSEROLE\", 7),  # West North Central\n             rep(\"MAC & CHEESE\", 8),  # South Atlantic\n             rep(\"MAC & CHEESE\", 4),  # East South Central\n             rep(\"CORNBREAD\", 4),  # West South Central\n             rep(\"SALAD\", 13)),  # Mountain & Pacific\n\n  color = c(rep(\"#D9A2C4\", 6),  # Pink (SQUASH)\n            rep(\"#D9A2C4\", 3),  # Pink (SQUASH)\n            rep(\"#B6D382\", 5),  # Lime Green (ROLLS/BISCUITS)\n            rep(\"#F8D867\", 7),  # Yellow (GREEN BEANS/CASSEROLE)\n            rep(\"#90A9D5\", 8),  # Blue (MAC & CHEESE)\n            rep(\"#90A9D5\", 4),  # Blue (MAC & CHEESE)\n            rep(\"#EBA482\", 4),  # Red-Orange (CORNBREAD)\n            rep(\"#77C2BE\", 13))  # Teal (SALAD)\n)\n\n# Get US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with region data\nmap_data &lt;- left_join(us_states, region_data, by = c(\"region\" = \"state\"))\n\n# Update legend_data to wrap text\nlegend_data &lt;- data.frame(\n  dish = c(\"SQUASH\", \"MAC & CHEESE\", \"ROLLS/ BISCUITS\", \"GREENBEANS/ CASSEROLE\", \"CORNBREAD\", \"SALAD\"),\n  color = c(\"#D9A2C4\", \"#90A9D5\", \"#B6D382\", \"#F8D867\", \"#EBA482\", \"#77C2BE\"),\n  x = c(-73, -80, -83, -104, -96, -132),  # Customize x positions for each label\n  y = c(49, 31, 48, 51, 27.5, 44)  # Customize y positions for each label\n)\n\n# Wrap the dish names to two lines\nlegend_data$dish &lt;- str_wrap(legend_data$dish, width = 10)  # Adjust 'width' to control wrapping\n\n# Plot the map with wrapped labels\nggplot(map_data, aes(long, lat, group = group, fill = color)) +\n  geom_polygon(color = \"grey65\", size = 0.3) +\n  scale_fill_identity() +\n  coord_map(\"albers\", lat0 = 25, lat1 = 50) +  # Curved map projection\n  geom_text(data = legend_data, \n            aes(x = x, y = y, label = dish, color = color),\n            inherit.aes = FALSE,\n            hjust = 0,\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_color_identity() +\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = expression(atop(\n    \"Most disproportionately common Thanksgiving side dish by region\",\n    atop(\"\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\",  \n    atop(scriptstyle(\"SURVEY DATES 11/17/2015                                               NO. OF RESPONDENTS 931\"),\n         \"\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\\u2015\")  \n    ))\n  )) +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5),\n        panel.background = element_rect(fill = \"grey95\", color = NA),  # Light grey background\n        plot.background = element_rect(fill = \"grey95\", color = NA))   # Light grey background \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nI’m going to find some basic info to help feed into AI.\n\nunique(thanksgiving$`US Region`)\n\n [1] \"Middle Atlantic\"    \"East South Central\" \"Mountain\"          \n [4] \"Pacific\"            \"East North Central\" \"West North Central\"\n [7] \"West South Central\" \"South Atlantic\"     \"New England\"       \n[10] NA                  \n\n\nHere’s my initial prompt to chatGPT:\nI need code to make a figure in R of a map of the united states by state. Title will be “Side Dishes Of America’s Regions”, and the subtitle will be “Most disproportionately common Thanksgiving side dish by region”. Middle Atlantic region should be pale pink and be “SQUASH”. East South Central region should be soft blue and should be “MAC & CHEESE”. Mountain region should be light teal and be “SALAD”. Pacfic region should be light teal and be “SALAD”. East North Central region should be pale lime green and be “ROLLS/BISCUITS”. West North Central should be yellow and be “GREEN BEANS/CASSEROLE”. West South Central should be soft orange-red and be “CORNBREAD”. South Atlantic should be soft blue and be “MAC & CHEESE”. New England should be pale pink and be “SQUASH”. Do you have any questions? Do you know which states are in these regions?\nChatGPT confirmed that it had the states in each of the nine regions correct and provided this code.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(maps)\nlibrary(mapdata)\n\n# Define region colors and labels\nregion_data &lt;- data.frame(\n  state = tolower(c(\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Rhode Island\", \"Connecticut\", \n                    \"New York\", \"New Jersey\", \"Pennsylvania\", \n                    \"Wisconsin\", \"Michigan\", \"Illinois\", \"Indiana\", \"Ohio\", \n                    \"North Dakota\", \"South Dakota\", \"Nebraska\", \"Kansas\", \"Minnesota\", \"Iowa\", \"Missouri\", \n                    \"Delaware\", \"Maryland\", \"Virginia\", \"West Virginia\", \"North Carolina\", \"South Carolina\", \"Georgia\", \"Florida\", \n                    \"Kentucky\", \"Tennessee\", \"Mississippi\", \"Alabama\", \n                    \"Oklahoma\", \"Texas\", \"Arkansas\", \"Louisiana\", \n                    \"Montana\", \"Idaho\", \"Wyoming\", \"Nevada\", \"Utah\", \"Colorado\", \"Arizona\", \"New Mexico\", \n                    \"Washington\", \"Oregon\", \"California\", \"Alaska\", \"Hawaii\")),\n  region = c(rep(\"SQUASH\", 6), rep(\"SQUASH\", 3), rep(\"ROLLS/BISCUITS\", 5), rep(\"GREEN BEANS/CASSEROLE\", 7),\n             rep(\"MAC & CHEESE\", 8), rep(\"MAC & CHEESE\", 4), rep(\"CORNBREAD\", 4), rep(\"SALAD\", 8), rep(\"SALAD\", 5)),\n  color = c(rep(\"#f4c2c2\", 6), rep(\"#f4c2c2\", 3), rep(\"#bfff00\", 5), rep(\"#ffff66\", 7),\n            rep(\"#aec6cf\", 8), rep(\"#aec6cf\", 4), rep(\"#ff6347\", 4), rep(\"#20b2aa\", 8), rep(\"#20b2aa\", 5))\n)\n\n# Get US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with region data\nmap_data &lt;- left_join(us_states, region_data, by = c(\"region\" = \"state\"))\n\n# Plot map\nggplot(map_data, aes(long, lat, group = group, fill = color)) +\n  geom_polygon(color = \"black\", size = 0.3) +\n  scale_fill_identity() +\n  coord_fixed(1.3) +\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = \"Most disproportionately common Thanksgiving side dish by region\") +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5))\n\n\n\n\n\n\n\n\nI provided the following prompt to ChatGPT to update this figure:\nThis map needs a few adjustments. Alaska and Hawaii need to be included in the Pacific region. The pink color should be D9A2C4. The blue color should be 90A9D5. The lime green color should be B6D382. The yellow color should be F8D867. The red-orange color should be EBA482. The teal color should be 77C2BE. Next to each color area, there should be words for each type of food, the text should be the same color as each region and not overlapping the map. The map need to have the latitude curves, not be flat and straight.\nHere’s the reply with the updated figure.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(maps)\nlibrary(mapdata)\n\n# Define region colors and labels\nregion_data &lt;- data.frame(\n  state = tolower(c(\"maine\", \"new hampshire\", \"vermont\", \"massachusetts\", \"rhode island\", \"connecticut\", \n                    \"new york\", \"new jersey\", \"pennsylvania\", \n                    \"wisconsin\", \"michigan\", \"illinois\", \"indiana\", \"ohio\", \n                    \"north dakota\", \"south dakota\", \"nebraska\", \"kansas\", \"minnesota\", \"iowa\", \"missouri\", \n                    \"delaware\", \"maryland\", \"virginia\", \"west virginia\", \"north carolina\", \"south carolina\", \"georgia\", \"florida\", \n                    \"kentucky\", \"tennessee\", \"mississippi\", \"alabama\", \n                    \"oklahoma\", \"texas\", \"arkansas\", \"louisiana\", \n                    \"montana\", \"idaho\", \"wyoming\", \"nevada\", \"utah\", \"colorado\", \"arizona\", \"new mexico\", \n                    \"washington\", \"oregon\", \"california\", \"alaska\", \"hawaii\")),\n\n  region = c(rep(\"SQUASH\", 6),  # New England\n             rep(\"SQUASH\", 3),  # Middle Atlantic\n             rep(\"ROLLS/BISCUITS\", 5),  # East North Central\n             rep(\"GREEN BEANS/CASSEROLE\", 7),  # West North Central\n             rep(\"MAC & CHEESE\", 8),  # South Atlantic\n             rep(\"MAC & CHEESE\", 4),  # East South Central\n             rep(\"CORNBREAD\", 4),  # West South Central\n             rep(\"SALAD\", 13)),  # Mountain & Pacific\n\n  color = c(rep(\"#D9A2C4\", 6),  # Pink (SQUASH)\n            rep(\"#D9A2C4\", 3),  # Pink (SQUASH)\n            rep(\"#B6D382\", 5),  # Lime Green (ROLLS/BISCUITS)\n            rep(\"#F8D867\", 7),  # Yellow (GREEN BEANS/CASSEROLE)\n            rep(\"#90A9D5\", 8),  # Blue (MAC & CHEESE)\n            rep(\"#90A9D5\", 4),  # Blue (MAC & CHEESE)\n            rep(\"#EBA482\", 4),  # Red-Orange (CORNBREAD)\n            rep(\"#77C2BE\", 13))  # Teal (SALAD)\n)\n\n# Get US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with region data\nmap_data &lt;- left_join(us_states, region_data, by = c(\"region\" = \"state\"))\n\n# Compute label positions\nlabel_positions &lt;- map_data %&gt;%\n  group_by(region) %&gt;%\n  summarise(long = mean(range(long)), lat = mean(range(lat)), color = first(color), .groups = \"drop\")\n\n# Plot map\nggplot(map_data, aes(long, lat, fill = color)) +  # Remove group = group\n  geom_polygon(color = \"black\", size = 0.3) +\n  scale_fill_identity() +\n  coord_map(\"albers\", lat0 = 25, lat1 = 50) +  # Adjust for latitude curves\n  geom_text(data = label_positions, aes(x = long, y = lat, label = region, color = color), size = 4, fontface = \"bold\") +\n  scale_color_identity() +\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = \"Most disproportionately common Thanksgiving side dish by region\") +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5))\n\n\n\n\n\n\n\n\nThis plot is a mess! I’m going to try using Claude instead of chatGPT, giving it the same two prompts as above.\n\nlibrary(ggplot2)\nlibrary(maps)\nlibrary(mapproj)\nlibrary(dplyr)\n\n# Define regions and their states\nregions &lt;- list(\n  \"New England\" = c(\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Rhode Island\", \"Connecticut\"),\n  \"Middle Atlantic\" = c(\"New York\", \"Pennsylvania\", \"New Jersey\"),\n  \"East North Central\" = c(\"Wisconsin\", \"Michigan\", \"Illinois\", \"Indiana\", \"Ohio\"),\n  \"West North Central\" = c(\"Missouri\", \"North Dakota\", \"South Dakota\", \"Nebraska\", \"Kansas\", \"Minnesota\", \"Iowa\"),\n  \"South Atlantic\" = c(\"Delaware\", \"Maryland\", \"District of Columbia\", \"Virginia\", \"West Virginia\", \"North Carolina\", \"South Carolina\", \"Georgia\", \"Florida\"),\n  \"East South Central\" = c(\"Kentucky\", \"Tennessee\", \"Mississippi\", \"Alabama\"),\n  \"West South Central\" = c(\"Oklahoma\", \"Texas\", \"Arkansas\", \"Louisiana\"),\n  \"Mountain\" = c(\"Idaho\", \"Montana\", \"Wyoming\", \"Nevada\", \"Utah\", \"Colorado\", \"Arizona\", \"New Mexico\"),\n  \"Pacific\" = c(\"Alaska\", \"Washington\", \"Oregon\", \"California\", \"Hawaii\")\n)\n\n# Create a data frame with state-region mappings\nstate_regions &lt;- data.frame(\n  state = unlist(regions),\n  region = rep(names(regions), sapply(regions, length))\n)\n\n# Define colors and foods by region\nregion_colors &lt;- c(\n  \"New England\" = \"#D9A2C4\",\n  \"Middle Atlantic\" = \"#D9A2C4\",\n  \"East North Central\" = \"#B6D382\",\n  \"West North Central\" = \"#F8D867\",\n  \"South Atlantic\" = \"#90A9D5\",\n  \"East South Central\" = \"#90A9D5\",\n  \"West South Central\" = \"#EBA482\",\n  \"Mountain\" = \"#77C2BE\",\n  \"Pacific\" = \"#77C2BE\"\n)\n\nregion_foods &lt;- c(\n  \"New England\" = \"SQUASH\",\n  \"Middle Atlantic\" = \"SQUASH\",\n  \"East North Central\" = \"ROLLS/BISCUITS\",\n  \"West North Central\" = \"GREEN BEANS/CASSEROLE\",\n  \"South Atlantic\" = \"MAC & CHEESE\",\n  \"East South Central\" = \"MAC & CHEESE\",\n  \"West South Central\" = \"CORNBREAD\",\n  \"Mountain\" = \"SALAD\",\n  \"Pacific\" = \"SALAD\"\n)\n\n# Get state map data\nstates_map &lt;- map_data(\"state\")\n\n# Convert state names to title case to match our data\nstates_map$region &lt;- tools::toTitleCase(states_map$region)\n\n# Add region information to the map data\nstates_map &lt;- left_join(states_map, state_regions, by = c(\"region\" = \"state\"))\n\n# Create label data frame\nlabel_data &lt;- data.frame(\n  long = c(-80, -82, -98, -110, -120, -85, -93, -103, -70),\n  lat = c(45, 38, 45, 45, 45, 32, 35, 38, 47),\n  region = names(region_foods),\n  label = region_foods\n)\n\n# Create the map\nggplot(states_map, aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = region), color = \"white\", size = 0.2) +\n  coord_map(\"albers\", lat0 = 39, lat1 = 45) +\n  scale_fill_manual(values = region_colors) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    axis.title = element_blank()\n  ) +\n  # Add labels for each region\n  geom_text(data = label_data, \n    aes(label = label, color = region, group = NULL),\n    size = 3, fontface = \"bold\") +\n  scale_color_manual(values = region_colors) +\n  labs(\n    title = \"Side Dishes Of America's Regions\",\n    subtitle = \"Most disproportionately common Thanksgiving side dish by region\"\n  )\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\n\n\n\n\n\n\n\n\n\nThat’s even worse. Back to chatGPT. I’m giving it the same prompt but taking out the part with the food labels next to the regions, because that seems to be where it’s getting confused.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(maps)\nlibrary(mapdata)\n\n# Define region colors and labels\nregion_data &lt;- data.frame(\n  state = tolower(c(\"maine\", \"new hampshire\", \"vermont\", \"massachusetts\", \"rhode island\", \"connecticut\", \n                    \"new york\", \"new jersey\", \"pennsylvania\", \n                    \"wisconsin\", \"michigan\", \"illinois\", \"indiana\", \"ohio\", \n                    \"north dakota\", \"south dakota\", \"nebraska\", \"kansas\", \"minnesota\", \"iowa\", \"missouri\", \n                    \"delaware\", \"maryland\", \"virginia\", \"west virginia\", \"north carolina\", \"south carolina\", \"georgia\", \"florida\", \n                    \"kentucky\", \"tennessee\", \"mississippi\", \"alabama\", \n                    \"oklahoma\", \"texas\", \"arkansas\", \"louisiana\", \n                    \"montana\", \"idaho\", \"wyoming\", \"nevada\", \"utah\", \"colorado\", \"arizona\", \"new mexico\", \n                    \"washington\", \"oregon\", \"california\", \"alaska\", \"hawaii\")),\n\n  region = c(rep(\"SQUASH\", 6),  # New England\n             rep(\"SQUASH\", 3),  # Middle Atlantic\n             rep(\"ROLLS/BISCUITS\", 5),  # East North Central\n             rep(\"GREEN BEANS/CASSEROLE\", 7),  # West North Central\n             rep(\"MAC & CHEESE\", 8),  # South Atlantic\n             rep(\"MAC & CHEESE\", 4),  # East South Central\n             rep(\"CORNBREAD\", 4),  # West South Central\n             rep(\"SALAD\", 13)),  # Mountain & Pacific\n\n  color = c(rep(\"#D9A2C4\", 6),  # Pink (SQUASH)\n            rep(\"#D9A2C4\", 3),  # Pink (SQUASH)\n            rep(\"#B6D382\", 5),  # Lime Green (ROLLS/BISCUITS)\n            rep(\"#F8D867\", 7),  # Yellow (GREEN BEANS/CASSEROLE)\n            rep(\"#90A9D5\", 8),  # Blue (MAC & CHEESE)\n            rep(\"#90A9D5\", 4),  # Blue (MAC & CHEESE)\n            rep(\"#EBA482\", 4),  # Red-Orange (CORNBREAD)\n            rep(\"#77C2BE\", 13))  # Teal (SALAD)\n)\n\n# Get US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with region data\nmap_data &lt;- left_join(us_states, region_data, by = c(\"region\" = \"state\"))\n\n# Plot map with curved projection\nggplot(map_data, aes(long, lat, group = group, fill = color)) +\n  geom_polygon(color = \"black\", size = 0.3) +\n  scale_fill_identity() +\n  coord_map(\"albers\", lat0 = 25, lat1 = 50) +  # Curved map projection to adjust for latitude lines\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = \"Most disproportionately common Thanksgiving side dish by region\") +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5))\n\n\n\n\n\n\n\n\nNot bad. ChatGPT, we need to add labels of which type of food these represent.\n\n# Get US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with region data\nmap_data &lt;- left_join(us_states, region_data, by = c(\"region\" = \"state\"))\n\n# Create labels data frame for the legend\nlegend_data &lt;- data.frame(\n  dish = c(\"SQUASH\", \"MAC & CHEESE\", \"ROLLS/BISCUITS\", \"GREEN BEANS/CASSEROLE\", \"CORNBREAD\", \"SALAD\"),\n  color = c(\"#D9A2C4\", \"#90A9D5\", \"#B6D382\", \"#F8D867\", \"#EBA482\", \"#77C2BE\"),\n  x = rep(-65, 6),  # Position on right side of map\n  y = seq(45, 25, length.out = 6)  # Spread vertically\n)\n\n# Plot map with curved projection and labels\nggplot(map_data, aes(long, lat, group = group, fill = color)) +\n  geom_polygon(color = \"grey65\", size = 0.3) +\n  scale_fill_identity() +\n  coord_map(\"albers\", lat0 = 25, lat1 = 50) +  # Curved map projection\n  geom_text(data = legend_data, \n            aes(x = x, y = y, label = dish, color = color),\n            inherit.aes = FALSE,\n            hjust = 0,\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_color_identity() +\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = \"Most disproportionately common Thanksgiving side dish by region\") +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5))\n\n\n\n\n\n\n\n\n\n# Get US map data\nus_states &lt;- map_data(\"state\")\n\n# Merge map data with region data\nmap_data &lt;- left_join(us_states, region_data, by = c(\"region\" = \"state\"))\n\n# Create labels data frame for the legend\nlegend_data &lt;- data.frame(\n  dish = c(\"SQUASH\", \"MAC & CHEESE\", \"ROLLS/BISCUITS\", \"GREEN BEANS/CASSEROLE\", \"CORNBREAD\", \"SALAD\"),\n  color = c(\"#D9A2C4\", \"#90A9D5\", \"#B6D382\", \"#F8D867\", \"#EBA482\", \"#77C2BE\"),\n  x = rep(-65, 6),  # Position on right side of map\n  y = seq(45, 25, length.out = 6)  # Spread vertically\n)\n\n# Add Alaska and Hawaii labels manually\nalaska_hawaii_labels &lt;- data.frame(\n  dish = c(\"ALASKA\", \"HAWAII\"),\n  color = c(\"#77C2BE\", \"#77C2BE\"),  # Same color as Pacific region (SALAD)\n  x = c(-140, -155),  # Position outside the map for Alaska and Hawaii\n  y = c(65, 55)  # Adjust vertical placement for clarity\n)\n\n# Plot map with curved projection, labels, and separate Alaska/Hawaii positions\nggplot(map_data, aes(long, lat, group = group, fill = color)) +\n  geom_polygon(color = \"grey65\", size = 0.3) +\n  scale_fill_identity() +\n  coord_map(\"albers\", lat0 = 25, lat1 = 50) +  # Curved map projection\n  geom_text(data = legend_data, \n            aes(x = x, y = y, label = dish, color = color),\n            inherit.aes = FALSE,\n            hjust = 0,\n            size = 3.5,\n            fontface = \"bold\") +\n  geom_text(data = alaska_hawaii_labels, \n            aes(x = x, y = y, label = dish, color = color),\n            inherit.aes = FALSE,\n            hjust = 0,\n            size = 4,\n            fontface = \"bold\") +\n  scale_color_identity() +\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = \"Most disproportionately common Thanksgiving side dish by region\") +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5))\n\n\n\n\n\n\n\n\nI asked to move Alaska nad Hawaii closer.\n\nregion_data &lt;- data.frame(\n  state = tolower(c(\"maine\", \"new hampshire\", \"vermont\", \"massachusetts\", \"rhode island\", \"connecticut\", \n                    \"new york\", \"new jersey\", \"pennsylvania\", \n                    \"wisconsin\", \"michigan\", \"illinois\", \"indiana\", \"ohio\", \n                    \"north dakota\", \"south dakota\", \"nebraska\", \"kansas\", \"minnesota\", \"iowa\", \"missouri\", \n                    \"delaware\", \"maryland\", \"virginia\", \"west virginia\", \"north carolina\", \"south carolina\", \"georgia\", \"florida\", \n                    \"kentucky\", \"tennessee\", \"mississippi\", \"alabama\", \n                    \"oklahoma\", \"texas\", \"arkansas\", \"louisiana\", \n                    \"montana\", \"idaho\", \"wyoming\", \"nevada\", \"utah\", \"colorado\", \"arizona\", \"new mexico\", \n                    \"washington\", \"oregon\", \"california\", \"alaska\", \"hawaii\")),\n  region = c(rep(\"SQUASH\", 6),  # New England\n             rep(\"SQUASH\", 3),  # Middle Atlantic\n             rep(\"ROLLS/BISCUITS\", 5),  # East North Central\n             rep(\"GREEN BEANS/CASSEROLE\", 7),  # West North Central\n             rep(\"MAC & CHEESE\", 8),  # South Atlantic\n             rep(\"MAC & CHEESE\", 4),  # East South Central\n             rep(\"CORNBREAD\", 4),  # West South Central\n             rep(\"SALAD\", 13)),  # Mountain & Pacific\n  color = c(rep(\"#D9A2C4\", 6),  # Pink (SQUASH)\n            rep(\"#D9A2C4\", 3),  # Pink (SQUASH)\n            rep(\"#B6D382\", 5),  # Lime Green (ROLLS/BISCUITS)\n            rep(\"#F8D867\", 7),  # Yellow (GREEN BEANS/CASSEROLE)\n            rep(\"#90A9D5\", 8),  # Blue (MAC & CHEESE)\n            rep(\"#90A9D5\", 4),  # Blue (MAC & CHEESE)\n            rep(\"#EBA482\", 4),  # Red-Orange (CORNBREAD)\n            rep(\"#77C2BE\", 13))  # Teal (SALAD)\n)\n\n# Get US map data including Alaska and Hawaii\nus_states &lt;- map_data(\"state\")\nalaska &lt;- map_data(\"world\", \"USA:alaska\")\nhawaii &lt;- map_data(\"world\", \"USA:hawaii\")\n\n# Transform Alaska coordinates\nalaska$long &lt;- alaska$long + 50\nalaska$lat &lt;- alaska$lat + 35\nalaska &lt;- alaska[alaska$long &gt; -130 & alaska$lat &gt; 51,]\n\n# Transform Hawaii coordinates\nhawaii$long &lt;- hawaii$long + 50\nhawaii$lat &lt;- hawaii$lat + 6\n\n# Combine all map data\nmap_data &lt;- bind_rows(\n  left_join(us_states, region_data, by = c(\"region\" = \"state\")),\n  left_join(alaska %&gt;% mutate(region = \"alaska\"), region_data, by = c(\"region\" = \"state\")),\n  left_join(hawaii %&gt;% mutate(region = \"hawaii\"), region_data, by = c(\"region\" = \"state\"))\n)\n\n# Create labels data frame with specific coordinates for each label\nlegend_data &lt;- data.frame(\n  dish = c(\"SQUASH\", \"MAC & CHEESE\", \"ROLLS/BISCUITS\", \"GREEN BEANS/CASSEROLE\", \"CORNBREAD\", \"SALAD\"),\n  color = c(\"#D9A2C4\", \"#90A9D5\", \"#B6D382\", \"#F8D867\", \"#EBA482\", \"#77C2BE\"),\n  x = c(-63, -63, -63, -63, -63, -63),  # Consistent x alignment\n  y = c(45, 41, 37, 33, 29, 25)  # Individual y coordinates\n)\n\n# Plot map with curved projection and labels\nggplot(map_data, aes(long, lat, group = group, fill = color)) +\n  geom_polygon(color = \"#666666\", size = 0.3) +\n  scale_fill_identity() +\n  coord_map(\"albers\", lat0 = 25, lat1 = 50) +\n  geom_text(data = legend_data, \n            aes(x = x, y = y, label = dish, color = color),\n            inherit.aes = FALSE,\n            hjust = 0,\n            size = 3.5,\n            fontface = \"bold\") +\n  scale_color_identity() +\n  ggtitle(\"Side Dishes Of America's Regions\") +\n  labs(subtitle = \"Most disproportionately common Thanksgiving side dish by region\") +\n  theme_void() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5))\n\n\n\n\n\n\n\n\nI’ve spent quite long working on this with ChatGPT and Claude. I suppose these tools aren’t the best for working the map plots. One of them wasn’t too terrible!\nNext, onto the table. First I need to process my data. I used AI to help with this.\n\nnames(thanksgiving)\n\n [1] \"RespondentID\"                                                                                                                                \n [2] \"Do you celebrate Thanksgiving?\"                                                                                                              \n [3] \"What is typically the main dish at your Thanksgiving dinner?\"                                                                                \n [4] \"What is typically the main dish at your Thanksgiving dinner? - Other (please specify)\"                                                       \n [5] \"How is the main dish typically cooked?\"                                                                                                      \n [6] \"How is the main dish typically cooked? - Other (please specify)\"                                                                             \n [7] \"What kind of stuffing/dressing do you typically have?\"                                                                                       \n [8] \"What kind of stuffing/dressing do you typically have? - Other (please specify)\"                                                              \n [9] \"What type of cranberry saucedo you typically have?\"                                                                                          \n[10] \"What type of cranberry saucedo you typically have? - Other (please specify)\"                                                                 \n[11] \"Do you typically have gravy?\"                                                                                                                \n[12] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Brussel sprouts\"                 \n[13] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Carrots\"                         \n[14] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Cauliflower\"                     \n[15] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Corn\"                            \n[16] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Cornbread\"                       \n[17] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Fruit salad\"                     \n[18] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Green beans/green bean casserole\"\n[19] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Macaroni and cheese\"             \n[20] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Mashed potatoes\"                 \n[21] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Rolls/biscuits\"                  \n[22] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Squash\"                          \n[23] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Vegetable salad\"                 \n[24] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Yams/sweet potato casserole\"     \n[25] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Other (please specify)...25\"     \n[26] \"Which of these side dishes aretypically served at your Thanksgiving dinner? Please select all that apply. - Other (please specify)...26\"     \n[27] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Apple\"                                    \n[28] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Buttermilk\"                               \n[29] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Cherry\"                                   \n[30] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Chocolate\"                                \n[31] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Coconut cream\"                            \n[32] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Key lime\"                                 \n[33] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Peach\"                                    \n[34] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Pecan\"                                    \n[35] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Pumpkin\"                                  \n[36] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Sweet Potato\"                             \n[37] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - None\"                                     \n[38] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Other (please specify)...38\"              \n[39] \"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Other (please specify)...39\"              \n[40] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Apple cobbler\"                       \n[41] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Blondies\"                            \n[42] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Brownies\"                            \n[43] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Carrot cake\"                         \n[44] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Cheesecake\"                          \n[45] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Cookies\"                             \n[46] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Fudge\"                               \n[47] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Ice cream\"                           \n[48] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Peach cobbler\"                       \n[49] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - None\"                                \n[50] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Other (please specify)...50\"         \n[51] \"Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply.   - Other (please specify)...51\"         \n[52] \"Do you typically pray before or after the Thanksgiving meal?\"                                                                                \n[53] \"How far will you travel for Thanksgiving?\"                                                                                                   \n[54] \"Will you watch any of the following programs on Thanksgiving? Please select all that apply. - Macy's Parade\"                                 \n[55] \"What's the age cutoff at your \\\"kids' table\\\" at Thanksgiving?\"                                                                              \n[56] \"Have you ever tried to meet up with hometown friends on Thanksgiving night?\"                                                                 \n[57] \"Have you ever attended a \\\"Friendsgiving?\\\"\"                                                                                                 \n[58] \"Will you shop any Black Friday sales on Thanksgiving Day?\"                                                                                   \n[59] \"Do you work in retail?\"                                                                                                                      \n[60] \"Will you employer make you work on Black Friday?\"                                                                                            \n[61] \"How would you describe where you live?\"                                                                                                      \n[62] \"Age\"                                                                                                                                         \n[63] \"What is your gender?\"                                                                                                                        \n[64] \"How much total combined money did all members of your HOUSEHOLD earn last year?\"                                                             \n[65] \"US Region\"                                                                                                                                   \n\nthanksgiving2 &lt;- thanksgiving %&gt;%\n  select(`US Region`, `Do you celebrate Thanksgiving?`, `What is your gender?`, `Have you ever attended a \\\"Friendsgiving?\\\"`, `What's the age cutoff at your \\\"kids' table\\\" at Thanksgiving?`, `How much total combined money did all members of your HOUSEHOLD earn last year?`)\n\n\nunique(thanksgiving2$`What is your gender?`)\n\n[1] \"Male\"   \"Female\" NA      \n\nunique(thanksgiving2$`Do you celebrate Thanksgiving?`)\n\n[1] \"Yes\" \"No\" \n\nunique(thanksgiving2$`Have you ever attended a \\\"Friendsgiving?\\\"`)\n\n[1] \"No\"  \"Yes\" NA   \n\nunique(thanksgiving2$`What's the age cutoff at your \\\"kids' table\\\" at Thanksgiving?`)\n\n [1] \"12\"            \"19\"            \"13\"            \"10 or younger\"\n [5] \"20\"            \"21 or older\"   NA              \"15\"           \n [9] \"14\"            \"18\"            \"16\"            \"17\"           \n[13] \"11\"           \n\nunique(thanksgiving2$`How much total combined money did all members of your HOUSEHOLD earn last year?`)\n\n [1] \"$75,000 to $99,999\"   \"$50,000 to $74,999\"   \"$0 to $9,999\"        \n [4] \"$200,000 and up\"      \"$100,000 to $124,999\" \"$25,000 to $49,999\"  \n [7] \"Prefer not to answer\" \"$10,000 to $24,999\"   \"$150,000 to $174,999\"\n[10] \"$175,000 to $199,999\" \"$125,000 to $149,999\" NA                    \n\n# Function to handle age cutoff values and convert to numeric, including NA handling\nget_age_cutoff_value &lt;- function(age_cutoff) {\n  if (is.na(age_cutoff)) {\n    return(NA)  # Return NA if the value is missing\n  } else if (age_cutoff == \"10 or younger\") {\n    return(10)  # Assign 10 for \"10 or younger\"\n  } else if (age_cutoff == \"21 or older\") {\n    return(21)  # Assign 21 for \"21 or older\"\n  } else {\n    return(as.numeric(age_cutoff))  # Convert other values to numeric\n  }\n}\n\nget_income_value &lt;- function(income_range) {\n  if (is.na(income_range) | income_range == \"Prefer not to answer\") {\n    return(NA)  # Return NA for missing values or 'Prefer not to answer'\n  } else if (income_range == \"$0 to $9,999\") {\n    return(5000)\n  } else if (income_range == \"$10,000 to $24,999\") {\n    return(17500)\n  } else if (income_range == \"$25,000 to $49,999\") {\n    return(37500)\n  } else if (income_range == \"$50,000 to $74,999\") {\n    return(62500)\n  } else if (income_range == \"$75,000 to $99,999\") {\n    return(87500)\n  } else if (income_range == \"$100,000 to $124,999\") {\n    return(112500)\n  } else if (income_range == \"$125,000 to $149,999\") {\n    return(137500)\n  } else if (income_range == \"$150,000 to $174,999\") {\n    return(162500)\n  } else if (income_range == \"$175,000 to $199,999\") {\n    return(187500)\n  } else if (income_range == \"$200,000 and up\") {\n    return(200000)  # You can adjust this value if needed\n  } else {\n    return(NA)\n  }\n}\n\nthanksgiving2 %&gt;%\n  filter(!is.na(`US Region`)) %&gt;%  # Exclude rows where `US Region` is NA\n   mutate(Household_Income_Numeric = sapply(`How much total combined money did all members of your HOUSEHOLD earn last year?`, get_income_value)) %&gt;%\n  mutate(Age_Cutoff_Numeric = sapply(`What's the age cutoff at your \"kids' table\" at Thanksgiving?`, get_age_cutoff_value)) %&gt;%\n  group_by(`US Region`) %&gt;%\n  summarise(\n    Percent_Female = mean(`What is your gender?` == \"Female\", na.rm = TRUE) * 100,\n    Percent_Celebrate_Thanksgiving = mean(`Do you celebrate Thanksgiving?` == \"Yes\", na.rm = TRUE) * 100,\n    Percent_Attended_Friendsgiving = mean(`Have you ever attended a \"Friendsgiving?\"` == \"Yes\", na.rm = TRUE) * 100,\n    Age_Cutoff_Avg = mean(Age_Cutoff_Numeric, na.rm = TRUE),\n    Household_Income_Avg = mean(Household_Income_Numeric, na.rm = TRUE)\n  )\n\n# A tibble: 9 × 6\n  `US Region`       Percent_Female Percent_Celebrate_Th…¹ Percent_Attended_Fri…²\n  &lt;chr&gt;                      &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1 East North Centr…           52.7                   96.7                   20.7\n2 East South Centr…           55                     93.3                   25  \n3 Middle Atlantic             52.2                   91.2                   23.4\n4 Mountain                    68.1                   87.2                   46.3\n5 New England                 55.2                   94.8                   23.6\n6 Pacific                     50.7                   89.0                   40.8\n7 South Atlantic              55.1                   94.9                   27.1\n8 West North Centr…           55.4                   95.9                   12.7\n9 West South Centr…           46.2                   93.4                   35.3\n# ℹ abbreviated names: ¹​Percent_Celebrate_Thanksgiving,\n#   ²​Percent_Attended_Friendsgiving\n# ℹ 2 more variables: Age_Cutoff_Avg &lt;dbl&gt;, Household_Income_Avg &lt;dbl&gt;\n\n\nNow, making the table. Statistics are automatically calcualted before feeding into the table.\n\nlibrary(gt)\n\n#data manipulation code\nsummary_data &lt;- thanksgiving2 %&gt;%\n  filter(!is.na(`US Region`)) %&gt;%  # Exclude rows where `US Region` is NA\n  mutate(Household_Income_Numeric = sapply(`How much total combined money did all members of your HOUSEHOLD earn last year?`, get_income_value)) %&gt;%\n  mutate(Age_Cutoff_Numeric = sapply(`What's the age cutoff at your \"kids' table\" at Thanksgiving?`, get_age_cutoff_value)) %&gt;%\n  group_by(`US Region`) %&gt;%\n  summarise(\n    Percent_Female = mean(`What is your gender?` == \"Female\", na.rm = TRUE) * 100,\n    Percent_Celebrate_Thanksgiving = mean(`Do you celebrate Thanksgiving?` == \"Yes\", na.rm = TRUE) * 100,\n    Percent_Attended_Friendsgiving = mean(`Have you ever attended a \"Friendsgiving?\"` == \"Yes\", na.rm = TRUE) * 100,\n    Age_Cutoff_Avg = mean(Age_Cutoff_Numeric, na.rm = TRUE),\n    Household_Income_Avg = mean(Household_Income_Numeric, na.rm = TRUE)\n  )\n\nsummary_data %&gt;%\n  mutate(\n    Percent_Female = Percent_Female / 100,\n    Percent_Celebrate_Thanksgiving = Percent_Celebrate_Thanksgiving / 100,\n    Percent_Attended_Friendsgiving = Percent_Attended_Friendsgiving / 100\n  ) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Thanksgiving Statistics by US Region\",\n  ) %&gt;%\n  cols_label(\n    `US Region` = \"Region\",\n    Percent_Female = \"Percent Female\",\n    Percent_Celebrate_Thanksgiving = \"Percent Who Celebrate Thanksgiving\",\n    Percent_Attended_Friendsgiving = \"Percent Who Have Attended Friendsgiving\",\n    Age_Cutoff_Avg = \"Average Kids' Table Age Cutoff\",\n    Household_Income_Avg = \"Average Household Income\"\n  ) %&gt;%\n  fmt_percent(\n    columns = c(Percent_Female, Percent_Celebrate_Thanksgiving, Percent_Attended_Friendsgiving),\n    decimals = 2\n  ) %&gt;%\n  fmt_number(\n    columns = c(Age_Cutoff_Avg),\n    decimals = 1\n  ) %&gt;%\n  fmt_number(\n    columns = c(Household_Income_Avg),\n    decimals = 1\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body(columns = \"US Region\")\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels(columns = c(Percent_Female, Percent_Celebrate_Thanksgiving, Percent_Attended_Friendsgiving, Age_Cutoff_Avg, Household_Income_Avg))\n  ) %&gt;%\n  tab_spanner(\n    label = \"Behavioral Data\",\n    columns = c(Percent_Female, Percent_Celebrate_Thanksgiving, Percent_Attended_Friendsgiving)\n  ) %&gt;%\n  tab_spanner(\n    label = \"Demographic Data\",\n    columns = c(Age_Cutoff_Avg, Household_Income_Avg)\n  ) %&gt;%\n  tab_style(\n    style = cell_borders(sides = c(\"left\", \"right\"), color = \"grey\", weight = px(0.5)),\n    locations = cells_column_labels(columns = c(Percent_Female, Percent_Celebrate_Thanksgiving, Percent_Attended_Friendsgiving, Age_Cutoff_Avg, Household_Income_Avg))\n  ) %&gt;%\n  tab_style(\n    style = cell_borders(sides = c(\"left\", \"right\"), color = \"grey\", weight = px(0.5)),\n    locations = cells_body(columns = c(Percent_Female, Percent_Celebrate_Thanksgiving, Percent_Attended_Friendsgiving, Age_Cutoff_Avg, Household_Income_Avg))\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#B6D382\"),  # Color for the first row\n    locations = cells_body(rows = 1)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#90A9D5\"),  # Color for the second row\n    locations = cells_body(rows = 2)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#D9A2C4\"),  # Color for the third row\n    locations = cells_body(rows = 3)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#77C2BE\"),  # Color for the fourth row\n    locations = cells_body(rows = 4)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#D9A2C4\"),  # Color for the fifth row\n    locations = cells_body(rows = 5)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#77C2BE\"),  # Color for the sixth row\n    locations = cells_body(rows = 6)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#90A9D5\"),  # Color for the seventh row\n    locations = cells_body(rows = 7)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color =  \"#F8D867\"),  # Color for the eighth row\n    locations = cells_body(rows = 8)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#EBA482\"),  # Color for the ninth row\n    locations = cells_body(rows = 9)\n  ) %&gt;%\n  # Add the dollar sign using a custom format\n  tab_style(\n    style = cell_text(align = \"right\"),\n    locations = cells_body(columns = \"Household_Income_Avg\")\n  ) %&gt;%\n  text_transform(\n    locations = cells_body(columns = \"Household_Income_Avg\"),\n    fn = function(x) paste0(\"$\", formatC(x, format = \"f\", big.mark = \",\"))\n  ) %&gt;%\n  tab_caption(\n    caption = \"Table 1. Summary statistics of 931 Americans answering a poll regarding their Thanksgiving traditions and plans, aquired 11/17/2015. Respondants who did not answer their US geographical region are not included. \"\n  )\n\n\n\n\n\nTable 1. Summary statistics of 931 Americans answering a poll regarding their Thanksgiving traditions and plans, aquired 11/17/2015. Respondants who did not answer their US geographical region are not included.\n\n\n\n\n\n\n\n\n\n\nThanksgiving Statistics by US Region\n\n\nRegion\n\nBehavioral Data\n\n\nDemographic Data\n\n\n\nPercent Female\nPercent Who Celebrate Thanksgiving\nPercent Who Have Attended Friendsgiving\nAverage Kids' Table Age Cutoff\nAverage Household Income\n\n\n\n\nEast North Central\n52.67%\n96.67%\n20.69%\n12.8\n$85,808.8\n\n\nEast South Central\n55.00%\n93.33%\n25.00%\n12.9\n$82,053.6\n\n\nMiddle Atlantic\n52.20%\n91.19%\n23.45%\n13.4\n$96,532.8\n\n\nMountain\n68.09%\n87.23%\n46.34%\n14.0\n$87,000.0\n\n\nNew England\n55.17%\n94.83%\n23.64%\n13.5\n$97,606.4\n\n\nPacific\n50.68%\n89.04%\n40.77%\n13.2\n$80,600.8\n\n\nSouth Atlantic\n55.14%\n94.86%\n27.09%\n12.9\n$81,426.6\n\n\nWest North Central\n55.41%\n95.95%\n12.68%\n12.2\n$76,475.4\n\n\nWest South Central\n46.15%\n93.41%\n35.29%\n12.7\n$87,120.3\n\n\n\n\n\n\n\ntesting"
>>>>>>> origin/main
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats.\nDoreen Kalembe contributed to this exercise.\nFigure 1: Height and weight stratified by gender.\nAge vs Weight plot.\nData summary table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n181.6927374\n50.360795\n3.6078211\n0.0365598\n\n\nAge\n-0.5027933\n2.057257\n-0.2443998\n0.8226832\n\n\nAllergiesPeanuts\n4.3715084\n12.192611\n0.3585375\n0.7436873\n\n\nAllergiesPollen\n-13.1229050\n15.463677\n-0.8486277\n0.4584358\n\n\nAllergiesShellfish\n-35.6201117\n15.802091\n-2.2541391\n0.1095178\n\n\nAllergiesTreenuts\n5.3743017\n15.395102\n0.3490917\n0.7500858"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section. The data set came with Height, Weight, and Gender. on 1/16/25 I added numerical vairable Age (0-110) and categorical vairable Allergies (None, Soy, Wheat, Egg, Milk, Peanuts, Shellfish, Treenuts, Pollen, Latex)."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\ntbl-summarytable shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nAllergies\n0\n1\n4\n9\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.976545\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.245261\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nAge\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n24.22222\n2.538591\n20\n23\n25\n26\n28\n▃▂▂▇▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\n#fig result shows a scatterplot figure produced by one of the R scripts.\n#{r} #| label: fig-result #| fig-cap: \"Height and weight stratified by gender.\" #| echo: FALSE #knitr::include_graphics(here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")) #"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample table results shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda - Copy.html",
    "href": "starter-analysis-exercise/code/eda-code/eda - Copy.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/mz13171/Downloads/MADA-course/annalisecramer-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Allergies             0             1   4   9     0        5          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean    sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0  133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2   45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  24.2  2.54  20  23  25  26   28 ▃▂▂▇▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nAdding code that creates a box plot and the scatter plots.\n\n# Create boxplot\np5&lt;- ggplot(mydata, aes(x = Gender, y = Height)) +\n  geom_boxplot(fill = \"orange\", alpha = 0.5) +\n  labs(title = \"Height by Gender\", x = \"Gender\", y = \"Height\") +\n  theme_minimal()\nplot(p5)\n\n\n\n\n\n\n\n# Save boxplot as an image file\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"heightvsgender-stratified.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\nlets also make a scatter plot of the new numerical variable that we have. That is age with weight.\n\n# Create scatterplot\np6&lt;- ggplot(mydata, aes(x = Weight, y = Age)) +\n  geom_point(color = \"red\", alpha = 0.7) +\n  labs(title = \"Weight vs Age\", x = \"Weight\", y = \"Age\") +\n  theme_minimal()\nplot(p6)\n\n\n\n\n\n\n\n# Save scatterplot as an image file\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"agevsweight.png\")\nggsave(filename = figure_file, plot=p6)\n\nSaving 7 x 5 in image\n\n\nI have done the two visualisations.\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/annalisecramer/MADA/annalisecramer-MADA-portfolio-main/annalisecramer-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at C:/Users/mz13171/Downloads/MADA-course/annalisecramer-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata2 &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ndim(rawdata2) #finding the dimensions of the data.\n\n[1] 14  5\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata2)\n\nRows: 14\nColumns: 5\n$ Height    &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155…\n$ Weight    &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender    &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\",…\n$ Age       &lt;dbl&gt; 28, 24, 23, 21, 19, 20, 25, 25, 24, 20, 27, 26, 26, 23\n$ Allergies &lt;chr&gt; \"None\", \"Treenuts\", \"None\", \"None\", \"None\", \"Peanuts\", \"Poll…\n\nsummary(rawdata2)\n\n    Height              Weight          Gender               Age       \n Length:14          Min.   :  45.0   Length:14          Min.   :19.00  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:21.50  \n Mode  :character   Median :  70.0   Mode  :character   Median :24.00  \n                    Mean   : 602.7                      Mean   :23.64  \n                    3rd Qu.:  90.0                      3rd Qu.:25.75  \n                    Max.   :7000.0                      Max.   :28.00  \n                    NA's   :1                                          \n  Allergies        \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata2)\n\n# A tibble: 6 × 5\n  Height Weight Gender   Age Allergies\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    \n1 180        80 M         28 None     \n2 175        70 O         24 Treenuts \n3 sixty      60 F         23 None     \n4 178        76 F         21 None     \n5 192        90 NA        19 None     \n6 6          55 F         20 Peanuts  \n\nskimr::skim(rawdata2)\n\n\nData summary\n\n\nName\nrawdata2\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nAllergies\n0\n1\n4\n9\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.0\n70\n90.00\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n23.64\n2.79\n19\n21.5\n24\n25.75\n28\n▆▂▇▇▃\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata2 %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nAllergies\n0\n1\n4\n9\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n23.69\n2.90\n19\n21.00\n24\n26\n28\n▆▂▆▇▃\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nAllergies\n0\n1\n4\n9\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n23.69\n2.90\n19\n21.00\n24\n26\n28\n▆▂▆▇▃\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nAllergies\n0\n1\n4\n9\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n23.73\n2.76\n19\n22.0\n24\n25.5\n28\n▃▂▆▇▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nAllergies\n0\n1\n4\n9\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n23.73\n2.76\n19\n22.0\n24\n25.5\n28\n▃▂▆▇▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nAllergies\n0\n1\n4\n9\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n24.22\n2.54\n20\n23\n25\n26\n28\n▃▂▂▇▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  }
]