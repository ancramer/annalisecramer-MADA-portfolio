[
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at C:/Pooja/Pooja/Spring 2025/EPID-BIOS-8060E-MADA/annalisecramer-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Pooja/Pooja/Spring 2025/EPID-BIOS-8060E-MADA/annalisecramer-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Allergies             0             1   4   9     0        5          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean    sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0  133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2   45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  24.2  2.54  20  23  25  26   28 ▃▂▂▇▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "To start, I’ll load packages.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nHere’s the prompt I gave chatGPT:\nWrite me code for R that generates data. I want the data to have 500 people who are in a clinical trial for a cancer drug. There should be roughly the same amount of males and females. Ages are 60-90, with more people in the mid to late 70s. Average BMI should be 28 for males, 30 for females. Age and BMI are slightly correlated, as age goes up, BMI slightly goes down. The study takes place over 2 years, with checkpoints every 3 months. There are 3 treatment groups, randomly assigned: placebo, 10mg, 20mg dose. Include a tumor variable that is measured in mm. Over time, the 20mg treatment group should show significant reduction in tumor size, the 10mg tumor group showing tumor staying the same size or slightly shrinking, and the placebo group can show anything in tumor size. Add a patient ID for each individual patient.\n\nset.seed(123)  # For reproducibility\n\n# Number of participants\nn &lt;- 500  \n\n# Generate patient IDs\npatient_ids &lt;- sprintf(\"P%03d\", 1:n)  # P001, P002, ..., P500\n\n# Generate gender\ngender &lt;- sample(c(\"Male\", \"Female\"), n, replace = TRUE)\n\n# Generate ages (60-90) with more in mid to late 70s\nages &lt;- round(rnorm(n, mean = 76, sd = 6))\nages &lt;- pmax(pmin(ages, 90), 60)  # Keep within bounds\n\n# Generate BMI (correlated with age, decreasing slightly as age increases)\nbmi_male &lt;- round(rnorm(n, mean = 28 - (ages - 76) * 0.1, sd = 2), 1)\nbmi_female &lt;- round(rnorm(n, mean = 30 - (ages - 76) * 0.1, sd = 2), 1)\nbmi &lt;- ifelse(gender == \"Male\", bmi_male, bmi_female)\n\n# Treatment groups (random assignment)\ntreatment &lt;- sample(c(\"Placebo\", \"10mg\", \"20mg\"), n, replace = TRUE)\n\n# Number of time points (every 3 months for 2 years)\ntime_points &lt;- seq(0, 24, by = 3)  # Months\n\n# Baseline tumor size (randomly generated between 20mm-60mm)\nbaseline_tumor &lt;- round(runif(n, min = 20, max = 60), 1)\n\n# Function to model tumor size change based on treatment\ntumor_reduction &lt;- function(base_size, treatment, time) {\n  if (treatment == \"Placebo\") {\n    return(base_size + rnorm(1, mean = 0, sd = 5))  # Tumor size fluctuates\n  } else if (treatment == \"10mg\") {\n    return(base_size * (1 - 0.005 * time) + rnorm(1, mean = 0, sd = 2))  # Small reduction\n  } else {  # 20mg group\n    return(base_size * (1 - 0.02 * time) + rnorm(1, mean = 0, sd = 2))  # Significant reduction\n  }\n}\n\n# Create dataset with repeated measures\ntrial_data &lt;- data.frame()\nfor (i in 1:n) {\n  for (t in time_points) {\n    new_row &lt;- data.frame(\n      Patient_ID = patient_ids[i],\n      Gender = gender[i],\n      Age = ages[i],\n      BMI = bmi[i],\n      Treatment = treatment[i],\n      Time = t,\n      Tumor_Size = max(tumor_reduction(baseline_tumor[i], treatment[i], t), 0)  # Ensure no negative values\n    )\n    trial_data &lt;- rbind(trial_data, new_row)\n  }\n}\n\n# View first few rows\nhead(trial_data)\n\n  Patient_ID Gender Age  BMI Treatment Time Tumor_Size\n1       P001   Male  74 31.3      20mg    0   55.43543\n2       P001   Male  74 31.3      20mg    3   48.42171\n3       P001   Male  74 31.3      20mg    6   49.58208\n4       P001   Male  74 31.3      20mg    9   43.98880\n5       P001   Male  74 31.3      20mg   12   42.12862\n6       P001   Male  74 31.3      20mg   15   37.27120\n\n# Save to CSV\n#write.csv(trial_data, \"clinical_trial_data.csv\", row.names = FALSE)\n\nOkay. First to test this code, I’m going to make a plot to look at the gender distribution, including making a dataset of only the starting values. They look alright.\n\nunique(trial_data$Gender)\n\n[1] \"Male\"   \"Female\"\n\nstart &lt;- trial_data %&gt;%\n  filter(Time==0)\nggplot(data=start, aes(x=Gender)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNow, let’s look at BMI at the start of the trial between males and females, and then at the end of the trial. It doesn’t look like it changes over time. Looks like the code given by AI kept everyone’s weight exactly the same, which isn’t totally realistic. However, I didn’t specify it to do this. We can see that it does show at the start of the trial that older people have slightly lower BMIs in both genders than younger people, as I asked.\n\nunique(trial_data$Time)\n\n[1]  0  3  6  9 12 15 18 21 24\n\nend &lt;- trial_data %&gt;%\n  filter(Time==24)\nggplot(data=start, aes(x=BMI, col=Gender)) +\n  geom_density(size=2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nggplot(data=end, aes(x=BMI, col=Gender)) +\n  geom_density(size=2)\n\n\n\n\n\n\n\nggplot(data = trial_data, aes(x = Time, y = BMI, group = Patient_ID, color = Gender)) +\n  geom_line(alpha = 0.2) +  # Individual spaghetti lines with transparency\n  stat_summary(fun = mean, geom = \"line\", aes(group = Gender), size = 1.2, color = \"black\") +  # Mean lines\n  stat_summary(fun = mean, geom = \"point\", aes(group = Gender), size = 3, color = \"black\")  # Mean points\n\n\n\n\n\n\n\nggplot(data = start, aes(x = Age, y = BMI, color = Gender)) +\n  geom_point(alpha = 0.6) +  # Scatter plot with transparency\n  geom_smooth(method = \"lm\", aes(group = Gender), se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNext, let’s look at the distribution of treatment groups. They don’t look perfectly even, but I asked for them to be randomly assignened and I guess I should have specified I meant for them to be even.\n\nggplot(data=start, aes(x=Treatment, fill=Treatment)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNow, I’ll look at the distribution of tumor size by treatment group. Looks good.\n\nggplot(data=start, aes(x=Treatment, y=Tumor_Size, fill=Treatment)) +\n  geom_violin()\n\n\n\n\n\n\n\nggplot(data=end, aes(x=Treatment, y=Tumor_Size, fill=Treatment)) +\n  geom_violin()\n\n\n\n\n\n\n\n\nNext I will make a linear model to see if treatment group has an effect on tumor size. All parameters have p-values below an significance level of alpha=0.05, so we conclude that treatment group is a significant predictor of tumor size.\n\nmodel1 &lt;- lm(data=trial_data, Tumor_Size~Treatment)\nsummary(model1)\n\n\nCall:\nlm(formula = Tumor_Size ~ Treatment, data = trial_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.513  -8.991  -0.547   8.725  33.123 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       37.0048     0.2808 131.789  &lt; 2e-16 ***\nTreatment20mg     -7.0365     0.4023 -17.491  &lt; 2e-16 ***\nTreatmentPlacebo   2.0495     0.4172   4.912 9.33e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.3 on 4497 degrees of freedom\nMultiple R-squared:  0.1049,    Adjusted R-squared:  0.1045 \nF-statistic: 263.5 on 2 and 4497 DF,  p-value: &lt; 2.2e-16\n\n\nI’m going to build a model that explores whther gender is a predictor for tumor size. I didn’t ask the data to contain this association, so we shouldn’t see a significant p-value. With a p-value of 0.434, we see that Gender is found to be a significant predictor of Tumor Size. Interesting.\n\nmodel2 &lt;- lm(data=trial_data, Tumor_Size~Gender)\nsummary(model2)\n\n\nCall:\nlm(formula = Tumor_Size ~ Gender, data = trial_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.456  -9.398  -0.489   9.040  36.625 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  34.8333     0.2574  135.32   &lt;2e-16 ***\nGenderMale    0.7197     0.3563    2.02   0.0434 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.94 on 4498 degrees of freedom\nMultiple R-squared:  0.0009063, Adjusted R-squared:  0.0006842 \nF-statistic:  4.08 on 1 and 4498 DF,  p-value: 0.04345"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "This data is from the CDC website, link here. It shows information from information from recent serological tests on SARS-CoV-2 samples that examine positivity, reactivity, and anitbody counts. Specifically, it examines whether different tests determine if samples test positive or negative using different types of covid tests. The mNT titer level represents the most dilute possible solution of anitbody to neutralize the virus.\nTo start, I’ll load packages and the data. Looking at the structure of the data, we can see there’s 204 obersvatinos and 14 variables. We will not look at all the vairables, so let’s choose the ones we want.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at C:/Pooja/Pooja/Spring 2025/EPID-BIOS-8060E-MADA/annalisecramer-MADA-portfolio\n\nhere()\n\n[1] \"C:/Pooja/Pooja/Spring 2025/EPID-BIOS-8060E-MADA/annalisecramer-MADA-portfolio\"\n\ndata &lt;- read_csv(here(\"cdcdata-exercise\", \"Examination_of_SARS-CoV-2_serological_test_results_from_multiple_commercial_and_laboratory_platforms_with_an_in-house_serum_panel_20250205.csv\"))\n\nRows: 204 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): PCR confirmed, Abbott Reactivity, Ortho Reactivity, In-House CDC E...\ndbl (10): Sample #, Abbott S/C Values, Abbott S/C Values (Log10), Ortho Inde...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata2 &lt;- data %&gt;%\n  select(`Sample #`, `PCR confirmed`, `Abbott Reactivity`, `Ortho Reactivity`, `In-House CDC ELISA Reactivity`, `mNT TITER (Log10)`)\n\nFirst, let’s look at the Sample # variable. It’s just integers from 1 to 204, each occuring one time.\n\nunique(data2$`Sample #`)\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204\n\ndata2$`Sample #`\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204\n\nggplot(data=data2, aes(x=`Sample #`)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNext, let’s explore the variable for PCR confirmed. This variable is either negative or positive.\n\nunique(data2$`PCR confirmed`)\n\n[1] \"Pos\" \"Neg\"\n\ncount_table &lt;- table(data2$`PCR confirmed`) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n    count_table      \nNeg         117 57.35\nPos          87 42.65\n\n\nNow we will look at the Abbott Reactivity vairable. There are two values, Reactive and Non-Reactive.\n\nunique(data2$`Abbott Reactivity`)\n\n[1] \"Reactive\"     \"Non-reactive\"\n\ncount_table &lt;- table(data2$`Abbott Reactivity`) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n             count_table      \nNon-reactive         120 58.82\nReactive              84 41.18\n\n\nNext, let us examine Ortho Reactivity. There are two values, Reactive and Non-Reactive.\n\nunique(data2$`Ortho Reactivity`)\n\n[1] \"Reactive\"     \"Non-reactive\"\n\ncount_table &lt;- table(data2$`Ortho Reactivity`) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n             count_table      \nNon-reactive         120 58.82\nReactive              84 41.18\n\n\nOne more reactivity test to examine. There are two responses for In-House CDC ELISA Reactivity, Reactive and Non-Reactive.\n\nunique(data2$`In-House CDC ELISA Reactivity`)\n\n[1] \"Reactive\"     \"Non-reactive\"\n\ncount_table &lt;- table(data2$`In-House CDC ELISA Reactivity`) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n             count_table      \nNon-reactive         118 57.84\nReactive              86 42.16\n\n\nNow, let us examine mNT TITER (Log10). This is not a character variable, it is numerical with continuous responses that ranger between 1 and 4. Many samples have a value of 1.\n\nunique(data2$`mNT TITER (Log10)`)\n\n [1] 1.2553 2.3674 3.3420 1.4314 2.7193 1.7559 2.5855 2.4914 2.1987 2.2455\n[11] 2.6776 2.9547 1.5051 3.7748 2.4265 2.5315 1.0000 3.0128 2.7945 3.7083\n[21] 1.4771 2.7284 1.6232 3.5465 1.6532 2.8062 2.8865 1.3010 3.1035 3.1414\n[31] 1.6721 2.7938 2.8357 2.5933 2.6653 2.9513 2.1000 1.7064 2.2310 2.2175\n[41] 2.5038 2.7372 3.1380 2.8169 2.8014 3.3162 2.7177 1.2788 2.8370 3.2240\n[51] 3.1562 2.2095 2.1903 3.3185 2.5441 2.5416 2.5877 2.0414 3.7676 2.6454\n[61] 2.8993 2.0755 1.8921 3.2582 2.4150 2.7649 1.3617 1.9685 2.7267 3.2577\n[71] 2.5955 3.4260 3.0026 2.1790 3.4156 1.3222 2.5490 3.0030 3.4298 3.4492\n[81] 2.3032 1.2041\n\nggplot(data=data2, aes(x=`mNT TITER (Log10)`)) +\n  geom_histogram(binwidth=0.1)\n\n\n\n\n\n\n\n\nReturning to the different types of tests, we know most of these have the same results across samples. However, a few are different. I used AI to help write the code for reshpaing and plotting the data. We can see they arre very close.\n\ndata3 &lt;- data2 %&gt;% #change the test data to all the same answers\n  mutate(`PCR confirmed` = recode(`PCR confirmed`, \n                                     \"Pos\" = \"yes\", \n                                     \"Neg\" = \"no\")) %&gt;%\n  mutate(`Abbott Reactivity` = recode(`Abbott Reactivity`, \n                                     \"Reactive\" = \"yes\", \n                                     \"Non-reactive\" = \"no\")) %&gt;%\n  mutate(`Ortho Reactivity` = recode(`Ortho Reactivity`, \n                                     \"Reactive\" = \"yes\", \n                                     \"Non-reactive\" = \"no\")) %&gt;%\n  mutate(`In-House CDC ELISA Reactivity` = recode(`In-House CDC ELISA Reactivity`, \n                                    \"Reactive\" = \"yes\", \n                                     \"Non-reactive\" = \"no\"))\n\n# Reshape the dataset to long format for easy plotting\ndata3_long &lt;- data3 %&gt;%\n  gather(key = \"Test\", value = \"Outcome\", \n         `PCR confirmed`, `Abbott Reactivity`, \n         `Ortho Reactivity`, `In-House CDC ELISA Reactivity`)\n\n# Ensure that Outcome is a factor with the correct levels\ndata3_long$Outcome &lt;- factor(data3_long$Outcome, levels = c(\"no\", \"yes\"))\n\n# Create a stacked bar plot\nggplot(data3_long, aes(x = Test, fill = Outcome)) + \n  geom_bar(position = \"fill\") +  # Position 'fill' normalizes the height to proportions\n  labs(\n    title = \"Comparison of Test Reactivity\",\n    x = \"Test\",\n    y = \"Proportion of Outcomes\",\n    fill = \"Reactivity Outcome\"\n  ) +\n  scale_fill_manual(values = c(\"no\" = \"red3\", \"yes\" = \"forestgreen\")) + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\nThis section contributed by Pooja Gokhale. Based on the give data, I used an AI prompt to give me the code to generate synthetic data. I used the counts and percentages from the original data. For mNT Titer Log10, we will compute the mean and range.\n\nmean(data2$`mNT TITER (Log10)`)\n\n[1] 1.625027\n\nsd(data2$`mNT TITER (Log10)`)\n\n[1] 0.8635054\n\nrange(data2$`mNT TITER (Log10)`)\n\n[1] 1.0000 3.7748\n\n\nUsing these, I got the folllowing code. The code was set to produce 200 observations. In the original dataset, about 125 out of 204 observations were in the range of 1, with others in the range of 2-3.7. So in the synthetic data, 60% of the values were assumed to be near 1 for mNT Titer Log10.\n\nset.seed(123)  # For reproducibility\n\n# Number of samples\nn_samples &lt;- 200\n\n# Generate Sample Number\nsample_number &lt;- 1:n_samples\n\n# Assign PCR Confirmed based on proportions\npcr_confirmed &lt;- sample(c(\"Pos\", \"Neg\"), n_samples, replace = TRUE, prob = c(0.4265, 0.5735))\n\n# Assign Abbott Reactivity based on proportions\nabbott_reactivity &lt;- sample(c(\"Reactive\", \"Non-reactive\"), n_samples, replace = TRUE, prob = c(0.4118, 0.5882))\n\n# Assign Ortho Reactivity based on proportions\northo_reactivity &lt;- sample(c(\"Reactive\", \"Non-reactive\"), n_samples, replace = TRUE, prob = c(0.4118, 0.5882))\n\n# Assign In-house CDC ELISA Reactivity based on proportions\ninhouse_cdc_elisa &lt;- sample(c(\"Reactive\", \"Non-reactive\"), n_samples, replace = TRUE, prob = c(0.4216, 0.5784))\n\n# Generate mNT Titer (log10) values following the required distribution\nmnt_titer &lt;- c(\n  rnorm(120, mean = 1.2, sd = 0.2),  # ~125 values near 1\n  rnorm(80, mean = 2.8, sd = 0.4)    # ~79 values in the 2-3.77 range\n)\n\n# Ensure values stay within the expected range\nmnt_titer[mnt_titer &lt; 1] &lt;- 1        # Clamp minimum to 1\nmnt_titer[mnt_titer &gt; 3.7748] &lt;- 3.7748  # Clamp maximum to 3.7748\n\n# Adjust mean and SD to match the target values\nwhile (abs(mean(mnt_titer) - 1.625) &gt; 0.01 || abs(sd(mnt_titer) - 0.8635) &gt; 0.01) {\n  mnt_titer &lt;- scale(mnt_titer) * 0.8635 + 1.625  # Rescale to match mean & SD\n  mnt_titer[mnt_titer &lt; 1] &lt;- 1\n  mnt_titer[mnt_titer &gt; 3.7748] &lt;- 3.7748\n}\n\n# Shuffle the values to randomize order\nmnt_titer &lt;- sample(mnt_titer, n_samples, replace = FALSE)\n\n# Combine into a data frame\nsynthetic_data &lt;- data.frame(\n  Sample_Number = sample_number,\n  PCR_Confirmed = pcr_confirmed,\n  Abbott_Reactivity = abbott_reactivity,\n  Ortho_Reactivity = ortho_reactivity,\n  Inhouse_CDC_ELISA = inhouse_cdc_elisa,\n  mNT_Titer_Log10 = mnt_titer\n)\n\nwrite.csv(synthetic_data, \"synthetic_data.csv\", row.names = FALSE)\n\nNow let us look at the summary of this synthetic data.\nFirst. let’s look at PCR confirmed\n\ncount_table &lt;- table(synthetic_data$PCR_Confirmed) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2))\n\n    count_table     \nNeg         117 58.5\nPos          83 41.5\n\n\nAbout 58% are Negative, and 42% are Positive, compared to 57.35% and 42.65% in the original dataset.\nNow, let’s look at Abbott reactivity\n\ncount_table &lt;- table(synthetic_data$Abbott_Reactivity) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n             count_table     \nNon-reactive         121 60.5\nReactive              79 39.5\n\n\nHere, 60.5% are non-reactive and 39.5% are Reactive, compared to 58.82% and 41.18% respectively in the original dataset.\nNow, let’s look at Ortho reactivity\n\ncount_table &lt;- table(synthetic_data$Ortho_Reactivity) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n             count_table     \nNon-reactive         121 60.5\nReactive              79 39.5\n\n\nThis is similar to Abbott reactivity, like the original dataset.\nNow, let’s look at In-House CDC Elisa reactivity\n\ncount_table &lt;- table(synthetic_data$Inhouse_CDC_ELISA) #table of counts\npercentage_table &lt;- prop.table(count_table) * 100 #table of percentages\ncbind(count_table, round(percentage_table, 2)) #combine them\n\n             count_table     \nNon-reactive         115 57.5\nReactive              85 42.5\n\n\nHere, 57.5% are Non-reactive and 42.5% are Reactive, compared to 57.84% and 42.16% respectively in the original dataset.\nNow, let’s look at mNT Titer Log10.\n\nggplot(synthetic_data, aes(x=mNT_Titer_Log10)) +\n  geom_histogram(binwidth=0.1)\n\n\n\n\n\n\n\n\nWe can see that the distribution of the mNT Titer Log10 in the synthetic data is very similar to that of the original dataset.\nNow, let’s plot the data.\n\nsynthetic_data2 &lt;- synthetic_data %&gt;% #change the test data to all the same answers\n  mutate(PCR_Confirmed = recode(PCR_Confirmed, \n                                     \"Pos\" = \"yes\", \n                                     \"Neg\" = \"no\")) %&gt;%\n  mutate(Abbott_Reactivity = recode(Abbott_Reactivity, \n                                     \"Reactive\" = \"yes\", \n                                     \"Non-reactive\" = \"no\")) %&gt;%\n  mutate(Ortho_Reactivity = recode(Ortho_Reactivity, \n                                     \"Reactive\" = \"yes\", \n                                     \"Non-reactive\" = \"no\")) %&gt;%\n  mutate(Inhouse_CDC_ELISA = recode(Inhouse_CDC_ELISA, \n                                    \"Reactive\" = \"yes\", \n                                     \"Non-reactive\" = \"no\"))\n\n# Reshape the dataset to long format for easy plotting\nsynthetic_data_long &lt;- synthetic_data2 %&gt;%\n  gather(key = \"Test\", value = \"Outcome\", \n         PCR_Confirmed, Abbott_Reactivity, \n         Ortho_Reactivity, Inhouse_CDC_ELISA)\n\n# Ensure that Outcome is a factor with the correct levels\nsynthetic_data_long$Outcome &lt;- factor(synthetic_data_long$Outcome, levels = c(\"no\", \"yes\"))\n\n# Create a stacked bar plot\nggplot(synthetic_data_long, aes(x = Test, fill = Outcome)) + \n  geom_bar(position = \"fill\") +  # Position 'fill' normalizes the height to proportions\n  labs(\n    title = \"Comparison of Test Reactivity\",\n    x = \"Test\",\n    y = \"Proportion of Outcomes\",\n    fill = \"Reactivity Outcome\"\n  ) +\n  scale_fill_manual(values = c(\"no\" = \"red3\", \"yes\" = \"forestgreen\")) + \n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\nSimilar to the original dataset, approximately 38% of the outcome was Yes, while 62% was No.\nUsing the AI prompt, I was able to produce a synthetic dataset which was very similar to the original."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Hi! My name is Annalise, but everyone calls me Annie. I use she/her pronouns.\nI’m a second year PhD student in the Epidemiology and Biostatistics Department. I work with Dr. Spencer Fox, and I broadly do stuff related to flu evolution– currently, we’re working on developing county by county risk for H5N1 based on childhood immunological imprinting. It’s a work in progress :)"
  },
  {
    "objectID": "aboutme.html#introduction",
    "href": "aboutme.html#introduction",
    "title": "About me",
    "section": "",
    "text": "Hi! My name is Annalise, but everyone calls me Annie. I use she/her pronouns.\nI’m a second year PhD student in the Epidemiology and Biostatistics Department. I work with Dr. Spencer Fox, and I broadly do stuff related to flu evolution– currently, we’re working on developing county by county risk for H5N1 based on childhood immunological imprinting. It’s a work in progress :)"
  },
  {
    "objectID": "aboutme.html#academic-background",
    "href": "aboutme.html#academic-background",
    "title": "About me",
    "section": "Academic Background",
    "text": "Academic Background\nFor undergrad, I attended Westfield State University in Westfield, MA (about half an hour south of UMass, if you’re familiar). My bachelors are in biotechnology and general mathematics, I did half pure and half applied math. Working in a hospital lab during the height of covid developed my interest in respiratory viruses, and completing an REU in UGA’s Odum School of Ecology fine tuned my infectious disease interests towards mathematical modeling. After graduating, I moved to Athens to begin my PhD.\nI have a strong preference for R, and I’ve used SAS for a few classes. In undergrad we used VBA/excel, but that’s not very helpful for this field. I’m a stronger mathematician than programmer, but that’s been shifting as I move through this degree program, and that’s something that I hope this course will continue to push me towards. I’ve taken the department required courses (BIOS8010, BIOS8020, BIOS8030, BIOS8200) and a handful of STAT department courses to fill out my skillset (Bayesian statistics, Time series analysis). I’m making progress on building my data analysis skills on top of my mathematical background, but there’s still plenty more to learn!\nI’ve completed several data analysis projects throughout my academic career, using data from phylogenetic sequences to survey data to case counts. I hope to get more into case count data and eventually move into forecasting. I’m hoping this course will help me gain more confidence working with data and staying organized."
  },
  {
    "objectID": "aboutme.html#non-academic-background",
    "href": "aboutme.html#non-academic-background",
    "title": "About me",
    "section": "Non-Academic background",
    "text": "Non-Academic background\nI grew up in rural Western Massachusetts, leading to my hobby for hiking. I especially like mountainy hikes with great views. I do a little day trip once or twice a month to go hiking somewhere in Northern GA/NC/TN. My all time favorite hike was with my brother in Death Valley. It’s the only place I’ve been to where there’s no encouragement to stay on the trails– the climate is so extreme, humans can’t do much damage, so we just walked across the salt flats for miles. We both preferred less busy environments, so we planned our trip for December to avoid crowds. Along the way, we found this tool that helped us plan the trip. It’s interesting to me to see the differences during 2020-2021, as well as effects of climate variation on visit counts. Additionally, the site provides visitation data sets.\nSince I’m newish to the area still, let me know your trail recommendations!\n\n\n\nMe at Charlie’s Bunion in Great Smoky Mountains NP, this past June. It was too cloudy to see the view."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "To start, I’ll load packages.\n#install.packages(\"dslabs\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dslabs)\nThen, check out the package info and data.\nhelp(gapminder)\n\nstarting httpd help server ... done\n\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\nclass(gapminder) #data frame, nice, I can use tidyverse on it\n\n[1] \"data.frame\"\nafricadata &lt;- gapminder |&gt; #pull out the African countries' data\n  filter(continent == \"Africa\")\nNext, I start making smaller dataframes with only specific variables within them.\ninfants1 &lt;- africadata |&gt; #create object with just infant mortality and life expectancy\n  select(infant_mortality, life_expectancy)\nstr(infants1) #its a data frame\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(infants1)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\npopulation1 &lt;- africadata |&gt; #create object with just population and life expectancy\n  select(population, life_expectancy)\nstr(population1) #its a data frame\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(population1)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51\nI’ll make plots using ggplot. We can see that lower infant mortality is negatively correlated with higher life expectancy. Higher population is positively correlated with life expectancy. The data appears in “streaks” in both plots as this shows year by year data within each African country. I’ll add a second set of plots colored by country to show this more clearly.\nggplot(data=infants1, aes(x=infant_mortality, y=life_expectancy)) +\n  geom_point()\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nggplot(data=population1, aes(x=population, y=life_expectancy)) +\n  geom_point() +\n  scale_x_log10()\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nggplot(data=africadata, aes(x=infant_mortality, y=life_expectancy, col=country)) +\n  geom_point()\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nggplot(data=africadata, aes(x=population, y=life_expectancy, col=country)) +\n  geom_point() +\n  scale_x_log10()\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\nmissing_data &lt;- africadata |&gt; #by year, count number of NAs AKA missings\n  group_by(year) |&gt;\n  summarize(missing = sum(is.na(infant_mortality)))\n\nggplot(data=missing_data, aes(x=year, y=missing)) + #make a bar plot to easily see when/where the NAs are\n  geom_col()\nTo avoid missingness, lets’ pull only rhe data from 2000.\nafricadata2000 &lt;- africadata |&gt; #pull out year 200 only\n  filter(year==2000)\n\nggplot(data=africadata2000, aes(x=infant_mortality, y=life_expectancy, col=country)) + #make same plots as above, but only usiing year 2000\n  geom_point()\n\n\n\n\n\n\n\nggplot(data=africadata2000, aes(x=population, y=life_expectancy, col=country)) +\n  geom_point()\nFinally, I fit two simple linear models to the 2000 data, life expectency explained by infant mortality and then by population. Using alpha=0.05 as a cutoff, I will assume the null hypotheses to be each predictor is not associated with life expectancy. With a p-value of 2.83e-08, we reject the null hypothesis to conclude that infant moratlity is a significant predictor of life expectancy in African countries in 2000. With a p-value of 0.616, we fail to reject the null hypothesis to conlcude that population is not a significant predictor of life expectency in African countries in 2000.\nfit1 &lt;- lm(data=africadata2000, life_expectancy~infant_mortality)\nfit2 &lt;- lm(data=africadata2000, life_expectancy~population)\n\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = africadata2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#this-section-contributed-by-mohammed-zuber",
    "href": "coding-exercise/coding-exercise.html#this-section-contributed-by-mohammed-zuber",
    "title": "R Coding Exercise",
    "section": "This section contributed by Mohammed Zuber",
    "text": "This section contributed by Mohammed Zuber"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#loading-and-checking-data",
    "href": "coding-exercise/coding-exercise.html#loading-and-checking-data",
    "title": "R Coding Exercise",
    "section": "Loading and Checking Data",
    "text": "Loading and Checking Data\n\n# help() function pulls up the help page for the data to see what it contains.\nhelp(research_funding_rates) \n# str () give us the overview of the datasets\nstr(research_funding_rates) \n\n'data.frame':   9 obs. of  10 variables:\n $ discipline         : chr  \"Chemical sciences\" \"Physical sciences\" \"Physics\" \"Humanities\" ...\n $ applications_total : num  122 174 76 396 251 183 282 834 505\n $ applications_men   : num  83 135 67 230 189 105 156 425 245\n $ applications_women : num  39 39 9 166 62 78 126 409 260\n $ awards_total       : num  32 35 20 65 43 29 56 112 75\n $ awards_men         : num  22 26 18 33 30 12 38 65 46\n $ awards_women       : num  10 9 2 32 13 17 18 47 29\n $ success_rates_total: num  26.2 20.1 26.3 16.4 17.1 15.8 19.9 13.4 14.9\n $ success_rates_men  : num  26.5 19.3 26.9 14.3 15.9 11.4 24.4 15.3 18.8\n $ success_rates_women: num  25.6 23.1 22.2 19.3 21 21.8 14.3 11.5 11.2\n\n# summary () gives summary of data\nsummary(research_funding_rates) \n\n  discipline        applications_total applications_men applications_women\n Length:9           Min.   : 76.0      Min.   : 67.0    Min.   :  9       \n Class :character   1st Qu.:174.0      1st Qu.:105.0    1st Qu.: 39       \n Mode  :character   Median :251.0      Median :156.0    Median : 78       \n                    Mean   :313.7      Mean   :181.7    Mean   :132       \n                    3rd Qu.:396.0      3rd Qu.:230.0    3rd Qu.:166       \n                    Max.   :834.0      Max.   :425.0    Max.   :409       \n  awards_total      awards_men     awards_women   success_rates_total\n Min.   : 20.00   Min.   :12.00   Min.   : 2.00   Min.   :13.4       \n 1st Qu.: 32.00   1st Qu.:22.00   1st Qu.:10.00   1st Qu.:15.8       \n Median : 43.00   Median :30.00   Median :17.00   Median :17.1       \n Mean   : 51.89   Mean   :32.22   Mean   :19.67   Mean   :18.9       \n 3rd Qu.: 65.00   3rd Qu.:38.00   3rd Qu.:29.00   3rd Qu.:20.1       \n Max.   :112.00   Max.   :65.00   Max.   :47.00   Max.   :26.3       \n success_rates_men success_rates_women\n Min.   :11.4      Min.   :11.20      \n 1st Qu.:15.3      1st Qu.:14.30      \n Median :18.8      Median :21.00      \n Mean   :19.2      Mean   :18.89      \n 3rd Qu.:24.4      3rd Qu.:22.20      \n Max.   :26.9      Max.   :25.60      \n\n#class() function to check what type of object research_funding_rates is\nclass(research_funding_rates) # obtaining the type of object research_funding_rates is\n\n[1] \"data.frame\""
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-data",
    "href": "coding-exercise/coding-exercise.html#processing-data",
    "title": "R Coding Exercise",
    "section": "Processing Data",
    "text": "Processing Data\n\n# Filtering the research funding data (I am not subsetting any data since the dataset is already very small)\nfundingdata &lt;- research_funding_rates \n\n# Getting an overview of the data structure\nstr(fundingdata) \n\n'data.frame':   9 obs. of  10 variables:\n $ discipline         : chr  \"Chemical sciences\" \"Physical sciences\" \"Physics\" \"Humanities\" ...\n $ applications_total : num  122 174 76 396 251 183 282 834 505\n $ applications_men   : num  83 135 67 230 189 105 156 425 245\n $ applications_women : num  39 39 9 166 62 78 126 409 260\n $ awards_total       : num  32 35 20 65 43 29 56 112 75\n $ awards_men         : num  22 26 18 33 30 12 38 65 46\n $ awards_women       : num  10 9 2 32 13 17 18 47 29\n $ success_rates_total: num  26.2 20.1 26.3 16.4 17.1 15.8 19.9 13.4 14.9\n $ success_rates_men  : num  26.5 19.3 26.9 14.3 15.9 11.4 24.4 15.3 18.8\n $ success_rates_women: num  25.6 23.1 22.2 19.3 21 21.8 14.3 11.5 11.2\n\n# Summary of fundingdata\nsummary(fundingdata) \n\n  discipline        applications_total applications_men applications_women\n Length:9           Min.   : 76.0      Min.   : 67.0    Min.   :  9       \n Class :character   1st Qu.:174.0      1st Qu.:105.0    1st Qu.: 39       \n Mode  :character   Median :251.0      Median :156.0    Median : 78       \n                    Mean   :313.7      Mean   :181.7    Mean   :132       \n                    3rd Qu.:396.0      3rd Qu.:230.0    3rd Qu.:166       \n                    Max.   :834.0      Max.   :425.0    Max.   :409       \n  awards_total      awards_men     awards_women   success_rates_total\n Min.   : 20.00   Min.   :12.00   Min.   : 2.00   Min.   :13.4       \n 1st Qu.: 32.00   1st Qu.:22.00   1st Qu.:10.00   1st Qu.:15.8       \n Median : 43.00   Median :30.00   Median :17.00   Median :17.1       \n Mean   : 51.89   Mean   :32.22   Mean   :19.67   Mean   :18.9       \n 3rd Qu.: 65.00   3rd Qu.:38.00   3rd Qu.:29.00   3rd Qu.:20.1       \n Max.   :112.00   Max.   :65.00   Max.   :47.00   Max.   :26.3       \n success_rates_men success_rates_women\n Min.   :11.4      Min.   :11.20      \n 1st Qu.:15.3      1st Qu.:14.30      \n Median :18.8      Median :21.00      \n Mean   :19.2      Mean   :18.89      \n 3rd Qu.:24.4      3rd Qu.:22.20      \n Max.   :26.9      Max.   :25.60      \n\n\n\n# Load necessary package\nlibrary(dplyr)\n\n# Create proportion_women_applicant and success rate difference\nfundingdata &lt;- research_funding_rates %&gt;%\n  mutate(\n    success_rate_difference = success_rates_men - success_rates_women, # Difference in success rates\n    proportion_women_applicants = applications_women / applications_total # Proportion of female applicants\n  )\n\n\n# Check if the new variables exist\nstr(fundingdata)\n\n'data.frame':   9 obs. of  12 variables:\n $ discipline                 : chr  \"Chemical sciences\" \"Physical sciences\" \"Physics\" \"Humanities\" ...\n $ applications_total         : num  122 174 76 396 251 183 282 834 505\n $ applications_men           : num  83 135 67 230 189 105 156 425 245\n $ applications_women         : num  39 39 9 166 62 78 126 409 260\n $ awards_total               : num  32 35 20 65 43 29 56 112 75\n $ awards_men                 : num  22 26 18 33 30 12 38 65 46\n $ awards_women               : num  10 9 2 32 13 17 18 47 29\n $ success_rates_total        : num  26.2 20.1 26.3 16.4 17.1 15.8 19.9 13.4 14.9\n $ success_rates_men          : num  26.5 19.3 26.9 14.3 15.9 11.4 24.4 15.3 18.8\n $ success_rates_women        : num  25.6 23.1 22.2 19.3 21 21.8 14.3 11.5 11.2\n $ success_rate_difference    : num  0.9 -3.8 4.7 -5 -5.1 ...\n $ proportion_women_applicants: num  0.32 0.224 0.118 0.419 0.247 ...\n\n# Creating two new objects for further analysis\nfunding_variable_one &lt;- fundingdata %&gt;% select(applications_total, success_rates_total)\n\n# Obtaining the structure of funding_variable_one\nstr(funding_variable_one) \n\n'data.frame':   9 obs. of  2 variables:\n $ applications_total : num  122 174 76 396 251 183 282 834 505\n $ success_rates_total: num  26.2 20.1 26.3 16.4 17.1 15.8 19.9 13.4 14.9\n\n# Summarizing the details about funding_variable_one\nsummary(funding_variable_one) \n\n applications_total success_rates_total\n Min.   : 76.0      Min.   :13.4       \n 1st Qu.:174.0      1st Qu.:15.8       \n Median :251.0      Median :17.1       \n Mean   :313.7      Mean   :18.9       \n 3rd Qu.:396.0      3rd Qu.:20.1       \n Max.   :834.0      Max.   :26.3       \n\n# Creating another dataset with only two variables as listed\nfunding_variable_two &lt;- fundingdata %&gt;% select(proportion_women_applicants, success_rate_difference)\n\n# Obtaining the structure of funding_variable_two\nstr(funding_variable_two) \n\n'data.frame':   9 obs. of  2 variables:\n $ proportion_women_applicants: num  0.32 0.224 0.118 0.419 0.247 ...\n $ success_rate_difference    : num  0.9 -3.8 4.7 -5 -5.1 ...\n\n# Obtaining the summary about the dataset funding_variable_two\nsummary(funding_variable_two)  \n\n proportion_women_applicants success_rate_difference\n Min.   :0.1184              Min.   :-10.4000       \n 1st Qu.:0.2470              1st Qu.: -5.0000       \n Median :0.4192              Median :  0.9000       \n Mean   :0.3563              Mean   :  0.3111       \n 3rd Qu.:0.4468              3rd Qu.:  4.7000       \n Max.   :0.5149              Max.   : 10.1000"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#plotting",
    "href": "coding-exercise/coding-exercise.html#plotting",
    "title": "R Coding Exercise",
    "section": "Plotting",
    "text": "Plotting\n\nFigure 1 Scatterplot of Total Applications vs. Success Rate\nThe scatterplot shows a negative trend, where disciplines with more applications tend to have lower success rates. Fields with fewer applications have higher success rates, possibly due to lower competition. In contrast, disciplines with over 500 applications see success rates drop below 15%, likely due to increased competition for limited funding. One outlier with 800+ applications and the lowest success rate suggests that some fields are highly competitive with stricter selection criteria.\n\n# Scatterplot of Total Applications vs. Success Rate\nggplot(funding_variable_one, aes(x = applications_total, y = success_rates_total)) + \n  geom_point(color = \"blue\", alpha = 0.7) +\n  labs(\n    title = \"Total Applications vs. Success Rate\",\n    x = \"Total Applications\",\n    y = \"Overall Success Rate\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 2: Proportion of Female Applicants vs. Gender Success Rate Difference\nThe scatterplot shows no clear correlation between the proportion of female applicants and the gender success rate difference, as points are scattered across both positive and negative values. Some disciplines with higher female representation still exhibit gender disparities in success rates. In certain fields, men have a higher success rate, while in others, women do better, indicating variability across disciplines. This suggests that factors beyond applicant proportion, such as funding policies or selection criteria, may influence gender differences in research funding success.\n\n#Scatterplot Proportion of Female Applicants vs. Gender Success Rate Difference\nggplot(funding_variable_two, aes(x = proportion_women_applicants, y = success_rate_difference)) + \n  geom_point(color = \"red\", alpha = 0.7) +\n  labs(\n    title = \"Proportion of Female Applicants vs. Gender Success Rate Difference\",\n    x = \"Proportion of Female Applicants\",\n    y = \"Success Rate Difference (Men - Women)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nTable 1: Top Disciplines Based on Total Applications\n\n# Extracting the top disciplines based on total applications\ntop_disciplines &lt;- fundingdata %&gt;%\n  select(discipline, applications_total, awards_total, awards_men, awards_women) %&gt;%\n  arrange(desc(applications_total)) %&gt;%  # Sorting by total applications\n  head(5)  # Displaying top 5 disciplines\n\n# Print the summary table\nprint(top_disciplines)\n\n           discipline applications_total awards_total awards_men awards_women\n1     Social sciences                834          112         65           47\n2    Medical sciences                505           75         46           29\n3          Humanities                396           65         33           32\n4 Earth/life sciences                282           56         38           18\n5  Technical sciences                251           43         30           13\n\n\n\n\nFigure 3: Total awards By Discipline\n\n# Creating a bar plot for total awards by discipline \nggplot(fundingdata, aes(x = reorder(discipline, awards_total), y = awards_total)) + \n  geom_bar(stat = \"identity\", fill = \"steelblue\", color = \"black\") + \n  labs(\n    title = \"Total Awards by Discipline\",\n    x = \"Discipline\",\n    y = \"Total Awards\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate text for better readability"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#fitting-a-linear-model",
    "href": "coding-exercise/coding-exercise.html#fitting-a-linear-model",
    "title": "R Coding Exercise",
    "section": "Fitting a linear model",
    "text": "Fitting a linear model\nIn the first model, the p-value is less than 0.05 (0.0197). Therefore, this suggests that the number of applications is a significant predictor of success rates. The negative coefficient (-0.0149) indicates that as the number of applications increases, the success rate decreases, likely due to increased competition. The R-squared of 0.564 means that 56.4% of the variation in success rates is explained by the number of applications.\nIn the second model, the p-value is greater than 0.05 (0.6013). Therefore, there is no strong statistical evidence to suggest that the proportion of female applicants is a significant predictor of the gender success rate difference. The R-squared of 0.041 means that only 4.1% of the variation in gender success rate differences is explained by the proportion of female applicants, which is weak.\n\n# Linear model: Success Rate vs. Total Applications\nfit1 &lt;- lm(success_rates_total ~ applications_total, data = fundingdata)\n# Linear model: Success Rate Difference vs. Proportion of Female Applicants\nfit2 &lt;- lm(success_rate_difference ~ proportion_women_applicants, data = fundingdata)\n# Summary of the model\nsummary(fit1)\n\n\nCall:\nlm(formula = success_rates_total ~ applications_total, data = fundingdata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0423 -1.2761 -0.8761  2.2346  4.4509 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        23.562590   1.901882  12.389 5.13e-06 ***\napplications_total -0.014865   0.004939  -3.009   0.0197 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.309 on 7 degrees of freedom\nMultiple R-squared:  0.564, Adjusted R-squared:  0.5018 \nF-statistic: 9.057 on 1 and 7 DF,  p-value: 0.01968\n\nsummary(fit2)\n\n\nCall:\nlm(formula = success_rate_difference ~ proportion_women_applicants, \n    data = fundingdata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.4201  -4.3029   0.9603   5.6813   8.8712 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                   -3.302      7.017  -0.471    0.652\nproportion_women_applicants   10.140     18.534   0.547    0.601\n\nResidual standard error: 7.118 on 7 degrees of freedom\nMultiple R-squared:  0.041, Adjusted R-squared:  -0.096 \nF-statistic: 0.2993 on 1 and 7 DF,  p-value: 0.6013"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Annie’s website and data analysis portfolio for MADA",
    "section": "",
    "text": "Hello!\nWelcome to my website. Read about me in the About Me tab."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda - Copy.html",
    "href": "starter-analysis-exercise/code/eda-code/eda - Copy.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Pooja/Pooja/Spring 2025/EPID-BIOS-8060E-MADA/annalisecramer-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Allergies             0             1   4   9     0        5          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean    sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0  133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2   45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  24.2  2.54  20  23  25  26   28 ▃▂▂▇▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nAdding code that creates a box plot and the scatter plots.\n\n# Create boxplot\np5&lt;- ggplot(mydata, aes(x = Gender, y = Height)) +\n  geom_boxplot(fill = \"orange\", alpha = 0.5) +\n  labs(title = \"Height by Gender\", x = \"Gender\", y = \"Height\") +\n  theme_minimal()\nplot(p5)\n\n\n\n\n\n\n\n# Save boxplot as an image file\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"heightvsgender-stratified.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\nlets also make a scatter plot of the new numerical variable that we have. That is age with weight.\n\n# Create scatterplot\np6&lt;- ggplot(mydata, aes(x = Weight, y = Age)) +\n  geom_point(color = \"red\", alpha = 0.7) +\n  labs(title = \"Weight vs Age\", x = \"Weight\", y = \"Age\") +\n  theme_minimal()\nplot(p6)\n\n\n\n\n\n\n\n# Save scatterplot as an image file\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"agevsweight.png\")\nggsave(filename = figure_file, plot=p6)\n\nSaving 7 x 5 in image\n\n\nI have done the two visualisations.\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at C:/Pooja/Pooja/Spring 2025/EPID-BIOS-8060E-MADA/annalisecramer-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata2 &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ndim(rawdata2) #finding the dimensions of the data.\n\n[1] 14  5\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata2)\n\nRows: 14\nColumns: 5\n$ Height    &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155…\n$ Weight    &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender    &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\",…\n$ Age       &lt;dbl&gt; 28, 24, 23, 21, 19, 20, 25, 25, 24, 20, 27, 26, 26, 23\n$ Allergies &lt;chr&gt; \"None\", \"Treenuts\", \"None\", \"None\", \"None\", \"Peanuts\", \"Poll…\n\nsummary(rawdata2)\n\n    Height              Weight          Gender               Age       \n Length:14          Min.   :  45.0   Length:14          Min.   :19.00  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:21.50  \n Mode  :character   Median :  70.0   Mode  :character   Median :24.00  \n                    Mean   : 602.7                      Mean   :23.64  \n                    3rd Qu.:  90.0                      3rd Qu.:25.75  \n                    Max.   :7000.0                      Max.   :28.00  \n                    NA's   :1                                          \n  Allergies        \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata2)\n\n# A tibble: 6 × 5\n  Height Weight Gender   Age Allergies\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    \n1 180        80 M         28 None     \n2 175        70 O         24 Treenuts \n3 sixty      60 F         23 None     \n4 178        76 F         21 None     \n5 192        90 NA        19 None     \n6 6          55 F         20 Peanuts  \n\nskimr::skim(rawdata2)\n\n\nData summary\n\n\nName\nrawdata2\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nAllergies\n0\n1\n4\n9\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.0\n70\n90.00\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n23.64\n2.79\n19\n21.5\n24\n25.75\n28\n▆▂▇▇▃\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata2 %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nAllergies\n0\n1\n4\n9\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n23.69\n2.90\n19\n21.00\n24\n26\n28\n▆▂▆▇▃\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nAllergies\n0\n1\n4\n9\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n23.69\n2.90\n19\n21.00\n24\n26\n28\n▆▂▆▇▃\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nAllergies\n0\n1\n4\n9\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n23.73\n2.76\n19\n22.0\n24\n25.5\n28\n▃▂▆▇▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nAllergies\n0\n1\n4\n9\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n23.73\n2.76\n19\n22.0\n24\n25.5\n28\n▃▂▆▇▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nAllergies\n0\n1\n4\n9\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n24.22\n2.54\n20\n23\n25\n26\n28\n▃▂▂▇▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats.\nDoreen Kalembe contributed to this exercise.\nFigure 1: Height and weight stratified by gender.\nAge vs Weight plot.\nData summary table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n181.6927374\n50.360795\n3.6078211\n0.0365598\n\n\nAge\n-0.5027933\n2.057257\n-0.2443998\n0.8226832\n\n\nAllergiesPeanuts\n4.3715084\n12.192611\n0.3585375\n0.7436873\n\n\nAllergiesPollen\n-13.1229050\n15.463677\n-0.8486277\n0.4584358\n\n\nAllergiesShellfish\n-35.6201117\n15.802091\n-2.2541391\n0.1095178\n\n\nAllergiesTreenuts\n5.3743017\n15.395102\n0.3490917\n0.7500858"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section. The data set came with Height, Weight, and Gender. on 1/16/25 I added numerical vairable Age (0-110) and categorical vairable Allergies (None, Soy, Wheat, Egg, Milk, Peanuts, Shellfish, Treenuts, Pollen, Latex)."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\ntbl-summarytable shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nAllergies\n0\n1\n4\n9\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.976545\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.245261\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nAge\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n24.22222\n2.538591\n20\n23\n25\n26\n28\n▃▂▂▇▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\n#fig result shows a scatterplot figure produced by one of the R scripts.\n#{r} #| label: fig-result #| fig-cap: \"Height and weight stratified by gender.\" #| echo: FALSE #knitr::include_graphics(here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")) #"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample table results shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Annie's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  }
]