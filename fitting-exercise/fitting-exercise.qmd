---
title: "Fitting Exercise"
---

To start, I'll load packages.

```{r}
library(tidyverse)
library(tidymodels)
library(nlmixr2data)
library(corrplot)
library(here)
```

Load the data and explore variable names, then plot. Then write code to make a plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. Stratify by dose (e.g., use a different color for each dose, or facets).

```{r}
data <- read_csv(here("fitting-exercise", "data", "Mavoglurant_A2121_nmpk.csv"))
names(data) #WT=weight, CMP=compartment number, EVID=eventID, MDV=missingDV, AMT=DoseAmountKeyword, DV=dependentvariableMavoglurant, OCC=occasion, HT=height
ggplot(data = data, aes(x = TIME, y = DV, color = factor(DOSE), group = ID)) +
  geom_line() +
  labs(
    title = "Mavoglurant concentration over time by dose group",
    x = "Time in hours",
    y = "Mavoglurant Concentration (DV)",
    color = "Dose")
```

Let us keep patients who only recived one dose.

```{r}
data2 <- data %>%
  filter(OCC == 1)
```

Next, we will exclude observations with TIME=0, and compute the sum of each DV variable. We will select observations where time=0, too. Data3 combines sums of time $\ne$ 0 observations and time = 0 observations.

```{r}
data_no0 <- data2 %>%
  filter(TIME != 0) #get observations where time is not 0

data_sum <- data_no0 %>% #from those non time=0, sum by dose
  group_by(ID) %>%
  summarize(Y = sum(DV, na.rm = TRUE)) %>%
  ungroup()

data_with0 <- data2 %>%
  filter(TIME == 0)

data3 <- left_join(data_sum, data_with0, by="ID")
```

Below is code that converts RACE and SEX to factor variables and keeps only certain variables that we want to work with: Y,DOSE,AGE,SEX,RACE,WT,HT

```{r}
data4 <- data3 %>%
  mutate(RACE = as.factor(RACE)) %>% #convert to factors
  mutate(SEX = as.factor(SEX)) %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT)

here()
saveRDS(data4, file = here("fitting-exercise", "data", "data4.rds"))
```

Let's explore the data with tables and plots.

First, we'll make a correlation plot, I used chatGPT to help with this. We can see most of these variables have weak correlations with each other, with the exception of Y/Dose and Weight/Height, both of which have moderately steong negative correlations.

```{r}
corr_matrix <- cor(data4[sapply(data4, is.numeric)], use = "complete.obs")

# Plot the correlation matrix
corrplot(corr_matrix, method = "circle", type = "upper", 
         tl.col = "black", tl.srt = 45, 
         main = "Correlation Plot for data4")
```

We will next plot the distributions for our variables to check if they're reasonable. I used chatGPT to help with this becuase I was having trouble getting the data into the right format (long).

I think most of these distributions look good. Dose shows a smallers group for the moderate dose, we will keep that in mind. Age shows a that people in their 30s are underrepresented, and perhaps those in their late 20s are overrepresented.

```{r}
# Select numeric columns only
numeric_data4 <- data4 %>% select(where(is.numeric))

# Pivot data to long format for ggplot
long_data4 <- pivot_longer(numeric_data4, cols = everything(), names_to = "Variable", values_to = "Value")

# Plot the distributions
ggplot(long_data4, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~ Variable, scales = "free") +
  labs(title = "Distributions of Numeric Variables in data4", x = "Value", y = "Count") +
  theme_minimal()
```

Now, let's make some scatterplots and boxplots.

Notes:

-   Not much trend with Age or Weight

-   Height shows fewer individuals with low height/high dose

-   Dose doesn't tell us much that we didn't already knkow from previous figures

-   Interested to know what races 7 and 88 are, I don't want to mess with them without access to a codebook-- could be unknown, don't know, NA, or multiple, some of those are valid answers

-   Sex has a few outliers but I am not going to remove them

```{r}
#scatterplots
ggplot(data4, aes(x = AGE, y = Y)) +
  geom_point(color = "dodgerblue2") +
  labs(title = "Y vs Age", x = "Age", y = "Total Drug (Y)")

ggplot(data4, aes(x = WT, y = Y)) +
  geom_point(color = "indianred2") +
  labs(title = "Y vs Weight", x = "Weight", y = "Total Drug (Y)")

ggplot(data4, aes(x = HT, y = Y)) +
  geom_point(color = "olivedrab3") +
  labs(title = "Y vs Height", x = "Height", y = "Total Drug (Y)")

ggplot(data4, aes(x = DOSE, y = Y)) +
  geom_point(color = "darkorchid2") +
  labs(title = "Y vs Dose", x = "Dose", y = "Total Drug (Y)")

#boxplots
ggplot(data4, aes(x = factor(RACE), y = Y)) +
  geom_boxplot(fill = "hotpink2") +
  labs(title = "Y vs Race", x = "Race", y = "Total Drug (Y)")

ggplot(data4, aes(x = factor(SEX), y = Y)) +
  geom_boxplot(fill = "orange2") +
  labs(title = "Y vs Sex", x = "Sex", y = "Total Drug (Y)")
```

Next, a summary table with usefule summary statistics. ChatGPT helped with this.

Means and medians are pretty close for all categories-- this tells me there's no crazy outliers.

```{r}
summary_table <- data4 %>%
  summarise(across(where(is.numeric), 
                   list(mean = ~ mean(.x, na.rm = TRUE),
                        median = ~ median(.x, na.rm = TRUE),
                        min = ~ min(.x, na.rm = TRUE),
                        max = ~ max(.x, na.rm = TRUE)))) %>%
  pivot_longer(everything(), names_to = c("variable", ".value"), names_sep = "_") %>%
  mutate(range = paste0(min, " - ", max)) %>%
  select(variable, mean, median, range)

print(summary_table)
```

With our exploratory analysis complete, we will begin building models.

Fit a linear model to the continuous outcome (Y) using the main predictor of interest, which we’ll assume here to be DOSE.

```{r}
model1 <- linear_reg()

model1_fit <- 
  model1 %>% 
  fit(Y ~ DOSE, data = data4)

model1_fit
```

Fit a linear model to the continuous outcome (Y) using all predictors.

```{r}
model2 <- linear_reg()

model2_fit <- 
  model2 %>% 
  fit(Y ~ DOSE + AGE + SEX + RACE + WT + HT, data = data4)

model2_fit
```

For both models, compute RMSE and R-squared and print them.

```{r}
#predictions for model1
model1_pred <- predict(
  model1_fit,
  new_data = data4
)

#put predictions with data
model1_result <- data4 %>%
  select(Y, DOSE) %>%
  bind_cols(model1_pred)

#get RMSE
model1_result %>%
  yardstick::rmse(
    truth = Y, 
    estimate = .pred
  )

#get R^2
model1_result %>%
  yardstick::rsq(
    truth = Y, 
    estimate = .pred
  )

#do the same for model2 (full model)
#predictions for model2
model2_pred <- predict(
  model2_fit,
  new_data = data4
)

#put predictions with data, add other vairables
model2_result <- data4 %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT) %>%
  bind_cols(model2_pred)

#get RMSE
model2_result %>%
  yardstick::rmse(
    truth = Y, 
    estimate = .pred
  )

#get R^2
model2_result %>%
  yardstick::rsq(
    truth = Y, 
    estimate = .pred
  )

```

We will make the same reduced and full model, but this time SEX will be the outcome instead of Y.

Fit a logistic model to the categorical/binary outcome (SEX) using the main predictor of interest, which we’ll again assume here to be DOSE.

```{r}
model3 <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

model3_fit <- model3 %>%
  fit(SEX ~ DOSE, data = data4)

```

Fit a logistic model to SEX using all predictors.

```{r}
model4 <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

model4_fit <- model4 %>%
  fit(SEX ~ DOSE + AGE + RACE + HT + WT + Y, data = data4)

```

For both models, compute accuracy and ROC-AUC and print them.

```{r}
#make predicitons for class
model3_pred <- predict(model3_fit, new_data = data4)
model4_pred <- predict(model4_fit, new_data = data4)

#put predictions with data
model3_result <- data4 %>%
  bind_cols(model3_pred)
model4_result <- data4 %>%
  bind_cols(model4_pred)

#calculate accuracy
model3_accuracy <- metrics(data = tibble(truth = data4$SEX, predicted = model3_pred$.pred_class), 
                    truth = truth, estimate = predicted)
model4_accuracy <- metrics(data = tibble(truth = data4$SEX, predicted = model4_pred$.pred_class), 
                    truth = truth, estimate = predicted)

#print accuracy
print(model3_accuracy)
print(model4_accuracy)

#make predictions for non-class
model3_pred <- predict(model3_fit, new_data = data4, type = "prob")
model4_pred <- predict(model4_fit, new_data = data4, type = "prob")

#calculate ROC_AUC
model3_roc_auc <- roc_auc(data = tibble(truth = data4$SEX,
                                 .pred_1 = model3_pred$.pred_1), 
                   truth = truth, .pred_1)

model4_roc_auc <- roc_auc(data = tibble(truth = data4$SEX, 
                                 .pred_1 = model4_pred$.pred_1), 
                   truth = truth, .pred_1)

print(model3_roc_auc)
print(model4_roc_auc)

```

This section marks the beginning of the modeule 10 exercise.

```{r}
rngseed = 1234 #use this later to set seed

data5 <- data4 %>% #remove race vairbale because we won't be using it
  select(Y, DOSE, AGE, SEX, WT, HT)

set.seed(rngseed) #set seed

data_split <- initial_split(data5, prop = 3/4) #3/4 of the data is for training

train_data <- training(data_split) #create dataframes for the sets
test_data  <- testing(data_split) #based on tidymodels instructions

#now we will fit our two models
model_dose <- linear_reg() #linear model with dose as predictor using training data
model_dose_fit <- 
  model_dose %>% 
  fit(Y ~ DOSE, data = train_data)
model_dose_fit 

model_all <- linear_reg() #linear model with all productors using training data
model_all_fit <- 
  model_all %>% 
  fit(Y ~ DOSE + AGE + SEX + WT + HT, data = train_data)
model_all_fit
```

Next, we'll calculate the RMSE for these two new models and a null model.

The null model has a RMSE of 948.35.

The model with only dose as a predictor has a RMSE of 702.81.

The full model has a RMSE of 627.44. This is the best model given this is the lowest RMSE out of the 3 models.

```{r}
set.seed(rngseed) #set seed

#predictions for model1
model_dose_pred <- predict(
  model_dose_fit,
  new_data = train_data)

#put predictions with data
model_dose_result <- train_data %>%
  select(Y, DOSE) %>%
  bind_cols(model_dose_pred)

#get RMSE
model_dose_result %>%
  yardstick::rmse(
    truth = Y, 
    estimate = .pred)

#do the same for the model with all predictors
#predictions for model2
model_all_pred <- predict(
  model_all_fit,
  new_data = train_data)

#put predictions with data, add other vairables
model_all_result <- train_data %>%
  select(Y, DOSE, AGE, SEX, WT, HT) %>%
  bind_cols(model_all_pred)

#get RMSE
model_all_result %>%
  yardstick::rmse(
    truth = Y, 
    estimate = .pred)

#I used chatGPT to help with this
# Define null model
null_mod <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("regression")

# Create a workflow without specifying intercept
null_wf <- workflow() %>%
  add_model(null_mod) %>%
  add_formula(Y ~ .)  # Only Y, no predictors will be used

# Fit the null model
null_fit <- fit(null_wf, data = train_data)

# Get predictions from the null model
null_preds <- predict(null_fit, train_data)

# Combine predictions with actual data
null_preds <- bind_cols(train_data, null_preds)

# Calculate RMSE
rmse_result <- rmse(null_preds, truth = Y, estimate = .pred)

# Print the result
print(rmse_result)
```

Now, we're going to do a 10-fold cross-validation with the testing data.

The simple CV model has an RMSE of 690.54.

The full CV model has an RMSE of 645.69.

The cross validation process has the same conclusion as the earlier process-- that the full model is best, with lowest RMSE. However, it's not quite as low, and the single model has a much closer RMSE this time. I'd be interested to see if this holds with even more iterations.

Next, I'm going to change the seed and rerun, then we'll see how the results look.

With a different seed value, the single model has a RMSE of 691.72.

The full model has a RMSE of 633.46.

This agrees with earlier results, that the full model has the lowest RMSE. However, the single and full models aren't quite so close. This makes me feel more confident in these results.

```{r}

set.seed(1938) #set seed

folds <- vfold_cv(train_data, v = 10) #set number of folds
folds

#for dose model
dose_wf <- 
  workflow() %>%
  add_model(model_dose) %>%
  add_formula(Y ~ DOSE)

# 10-fold cv for model_dose
dose_fit_rs <- 
  dose_wf %>% 
  fit_resamples(folds)
dose_fit_rs
# Collect metrics for model_dose
dose_metrics <- collect_metrics(dose_fit_rs)




#for model with all predictors
all_wf <- 
  workflow() %>%
  add_model(model_all) %>%
  add_formula(Y ~ DOSE + WT + AGE + SEX + HT)

# 10-fold cv for model_all
all_fit_rs <- 
  all_wf %>% 
  fit_resamples(folds)
all_fit_rs

# Collect metrics for model_all
all_metrics <- collect_metrics(all_fit_rs)




# Filter RMSE for both models
dose_rmse <- dose_metrics %>% filter(.metric == "rmse")
all_rmse <- all_metrics %>% filter(.metric == "rmse")

# Print RMSE for both models
print(dose_rmse)
print(all_rmse)

```

# This section added by Mohammed Zuber

Now, here I am generating a scatter plot of observed vs. predicted values for all three models.

```{r}
# Load necessary packages
library(ggplot2)
library(dplyr)

# Combine predictions into a single dataframe
model_dose_result <- model_dose_result %>%
  mutate(model = "Model 1: DOSE only")

model_all_result <- model_all_result %>%
  mutate(model = "Model 2: All Predictors")

null_preds <- null_preds %>%
  mutate(model = "Null Model")

# Combine all results
all_predictions <- bind_rows(model_dose_result, model_all_result, null_preds)

# Plot observed vs predicted values
ggplot(all_predictions, aes(x = Y, y = .pred, color = model)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  labs(title = "Observed vs. Predicted Values",
       x = "Observed Y",
       y = "Predicted Y",
       color = "Model") +
  scale_x_continuous(limits = c(0, 5000)) +
  scale_y_continuous(limits = c(0, 5000)) +
  theme_minimal()

```

The scatter plot compares observed Y values to predicted Y values for three models: the null model, Model 1 (DOSE only), and Model 2 (all predictors). The null model predictions form a horizontal line, indicating it only predicts the mean Y value for all observations without considering any predictors. Model 1, which includes only DOSE as a predictor, shows three distinct horizontal lines, reflecting the limited number of dose levels and its inability to capture finer variations. Model 2, which includes all predictors, produces a more continuous spread of predictions that align more closely with the 45-degree reference line, indicating improved predictive accuracy. However, there is still some scatter around the diagonal, suggesting that additional factors not included in the model may influence Y. Overall, Model 2 performs the best, but further refinements or additional variables may be needed to improve prediction accuracy.

Now Plotting predicted vs. residuals to check for patterns.

```{r}
# Compute residuals for Model 2
model_all_result <- model_all_result %>%
  mutate(residuals = .pred - Y)

# Residuals plot
ggplot(model_all_result, aes(x = .pred, y = residuals)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs. Predicted Values (Model 2)",
       x = "Predicted Values",
       y = "Residuals") +
  theme_minimal()

```

The residuals vs. predicted values plot for Model 2 shows that residuals are generally scattered around zero, indicating a reasonably good fit.

In this step, I am Using bootstrapping to assess uncertainty in Model 2 predictions.

```{r}

# Set seed for reproducibility
set.seed(rngseed)

# Generate 100 bootstrap samples from training data
boot_samples <- bootstraps(train_data, times = 100)

# Function to fit Model 2 on a bootstrap sample and predict on training data
bootstrap_preds <- map(boot_samples$splits, function(split) {
  
  # Extract bootstrap sample
  data_sample <- analysis(split)
  
  # Fit Model 2 (full model) on the bootstrap sample
  fit <- model_all %>% fit(Y ~ DOSE + AGE + SEX + WT + HT, data = data_sample)
  
  # Generate predictions on the original training data
  predict(fit, new_data = train_data)$.pred
})

# Convert list of predictions into a matrix (rows: data points, columns: bootstrap samples)
bootstrap_preds_matrix <- do.call(cbind, bootstrap_preds)

# Compute the median and 89% confidence intervals (5.5% and 94.5% percentiles)
bootstrap_summary <- apply(bootstrap_preds_matrix, 1, quantile, probs = c(0.055, 0.5, 0.945)) %>%
  t() %>%  # Transpose for better structure
  as.data.frame()

# Rename columns for clarity
colnames(bootstrap_summary) <- c("lower_bound", "median", "upper_bound")

# Merge the bootstrap results with the observed values
bootstrap_results <- train_data %>%
  mutate(original_pred = model_all_result$.pred,  # Original model predictions
         lower_bound = bootstrap_summary$lower_bound,  # Lower confidence bound
         median = bootstrap_summary$median,  # Median bootstrap prediction
         upper_bound = bootstrap_summary$upper_bound)  # Upper confidence bound

# Visualization: Observed vs Predicted values with Bootstrap Confidence Intervals
ggplot(bootstrap_results, aes(x = Y, y = original_pred)) +
  geom_point(color = "blue", alpha = 0.6) +  # Original model predictions as black dots
  geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound), width = 0.2, color = "purple") +  # CI bars
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +  # Ideal 45-degree line
  scale_x_continuous(limits = c(0, 5000)) +  # Set x-axis range from 0 to 5000
  scale_y_continuous(limits = c(0, 5000)) +  # Set y-axis range from 0 to 5000
  labs(title = "Bootstrap Confidence Intervals for Model 2 Predictions",
       x = "Observed",
       y = "Predicted") +
  theme_minimal()
```

The bootstrap confidence interval plot for Model 2 shows predicted values against observed values, with error bars representing the uncertainty in predictions. Most predictions align closely with the 45-degree reference line, indicating that the model performs reasonably well in capturing the true values. However, the presence of wider confidence intervals for some points suggests higher uncertainty in predictions, particularly at higher observed values. This implies that the model may have greater variability in its estimates for certain data points, possibly due to missing predictors or high noise in the data. While the majority of points follow the expected trend, a few deviations indicate that some observations are not well predicted, reinforcing the need for potential improvements. Overall, the model shows a strong predictive trend, but refinement through additional features or more complex modeling techniques may further enhance accuracy.
