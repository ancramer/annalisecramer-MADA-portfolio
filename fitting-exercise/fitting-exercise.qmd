---
title: "Fitting Exercise"
---

To start, I'll load packages.
```{r}
library(tidyverse)
library(tidymodels)
library(nlmixr2data)
library(corrplot)
library(here)
```
Load the data and explore variable names, then plot. Then write code to make a plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. Stratify by dose (e.g., use a different color for each dose, or facets).
```{r}
data <- read_csv(here("fitting-exercise", "data", "Mavoglurant_A2121_nmpk.csv"))
names(data) #WT=weight, CMP=compartment number, EVID=eventID, MDV=missingDV, AMT=DoseAmountKeyword, DV=dependentvariableMavoglurant, OCC=occasion, HT=height
ggplot(data = data, aes(x = TIME, y = DV, color = factor(DOSE), group = ID)) +
  geom_line() +
  labs(
    title = "Mavoglurant concentration over time by dose group",
    x = "Time in hours",
    y = "Mavoglurant Concentration (DV)",
    color = "Dose")
```

Let us keep patients who only recived one dose.
```{r}
data2 <- data %>%
  filter(OCC == 1)
```

Next, we will exclude observations with TIME=0, and compute the sum of each DV variable. We will select observations where time=0, too. Data3 combines sums of time $\ne$ 0 observations and time = 0 observations.
```{r}
data_no0 <- data2 %>%
  filter(TIME != 0) #get observations where time is not 0

data_sum <- data_no0 %>% #from those non time=0, sum by dose
  group_by(ID) %>%
  summarize(Y = sum(DV, na.rm = TRUE)) %>%
  ungroup()

data_with0 <- data2 %>%
  filter(TIME == 0)

data3 <- left_join(data_sum, data_with0, by="ID")
```

Below is code that converts RACE and SEX to factor variables and keeps only certain variables that we want to work with: Y,DOSE,AGE,SEX,RACE,WT,HT
```{r}
data4 <- data3 %>%
  mutate(RACE = as.factor(RACE)) %>% #convert to factors
  mutate(SEX = as.factor(SEX)) %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT)

here()
saveRDS(data4, file = here("fitting-exercise", "data", "data4.rds"))
```

Let's explore the data with tables and plots.

First, we'll make a correlation plot, I used chatGPT to help with this. We can see most of these variables have weak correlations with each other, with the exception of Y/Dose and Weight/Height, both of which have moderately steong negative correlations.
```{r}
corr_matrix <- cor(data4[sapply(data4, is.numeric)], use = "complete.obs")

# Plot the correlation matrix
corrplot(corr_matrix, method = "circle", type = "upper", 
         tl.col = "black", tl.srt = 45, 
         main = "Correlation Plot for data4")
```

We will next plot the distributions for our variables to check if they're reasonable. I used chatGPT to help with this becuase I was having trouble getting the data into the right format (long).

I think most of these distributions look good. Dose shows a smallers group for the moderate dose, we will keep that in mind. Age shows a that people in their 30s are underrepresented, and perhaps those in their late 20s are overrepresented.
```{r}
# Select numeric columns only
numeric_data4 <- data4 %>% select(where(is.numeric))

# Pivot data to long format for ggplot
long_data4 <- pivot_longer(numeric_data4, cols = everything(), names_to = "Variable", values_to = "Value")

# Plot the distributions
ggplot(long_data4, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.7) +
  facet_wrap(~ Variable, scales = "free") +
  labs(title = "Distributions of Numeric Variables in data4", x = "Value", y = "Count") +
  theme_minimal()
```

Now, let's make some scatterplots and boxplots.

Notes:

- Not much trend with Age or Weight 

- Height shows fewer individuals with low height/high dose

- Dose doesn't tell us much that we didn't already knkow from previous figures

- Interested to know what races 7 and 88 are, I don't want to mess with them without access to a codebook-- could be unknown, don't know, NA, or multiple, some of those are valid answers

- Sex has a few outliers but I am not going to remove them
```{r}
#scatterplots
ggplot(data4, aes(x = AGE, y = Y)) +
  geom_point(color = "dodgerblue2") +
  labs(title = "Y vs Age", x = "Age", y = "Total Drug (Y)")

ggplot(data4, aes(x = WT, y = Y)) +
  geom_point(color = "indianred2") +
  labs(title = "Y vs Weight", x = "Weight", y = "Total Drug (Y)")

ggplot(data4, aes(x = HT, y = Y)) +
  geom_point(color = "olivedrab3") +
  labs(title = "Y vs Height", x = "Height", y = "Total Drug (Y)")

ggplot(data4, aes(x = DOSE, y = Y)) +
  geom_point(color = "darkorchid2") +
  labs(title = "Y vs Dose", x = "Dose", y = "Total Drug (Y)")

#boxplots
ggplot(data4, aes(x = factor(RACE), y = Y)) +
  geom_boxplot(fill = "hotpink2") +
  labs(title = "Y vs Race", x = "Race", y = "Total Drug (Y)")

ggplot(data4, aes(x = factor(SEX), y = Y)) +
  geom_boxplot(fill = "orange2") +
  labs(title = "Y vs Sex", x = "Sex", y = "Total Drug (Y)")
```

Next, a summary table with usefule summary statistics. ChatGPT helped with this.

Means and medians are pretty close for all categories-- this tells me there's no crazy outliers.
```{r}
summary_table <- data4 %>%
  summarise(across(where(is.numeric), 
                   list(mean = ~ mean(.x, na.rm = TRUE),
                        median = ~ median(.x, na.rm = TRUE),
                        min = ~ min(.x, na.rm = TRUE),
                        max = ~ max(.x, na.rm = TRUE)))) %>%
  pivot_longer(everything(), names_to = c("variable", ".value"), names_sep = "_") %>%
  mutate(range = paste0(min, " - ", max)) %>%
  select(variable, mean, median, range)

print(summary_table)
```

With our exploratory analysis complete, we will begin building models.

Fit a linear model to the continuous outcome (Y) using the main predictor of interest, which we’ll assume here to be DOSE.
```{r}
model1 <- linear_reg()

model1_fit <- 
  model1 %>% 
  fit(Y ~ DOSE, data = data4)

model1_fit
```
Fit a linear model to the continuous outcome (Y) using all predictors.
```{r}
model2 <- linear_reg()

model2_fit <- 
  model2 %>% 
  fit(Y ~ DOSE + AGE + SEX + RACE + WT + HT, data = data4)

model2_fit
```
For both models, compute RMSE and R-squared and print them.
```{r}
#predictions for model1
model1_pred <- predict(
  model1_fit,
  new_data = data4
)

#put predictions with data
model1_result <- data4 %>%
  select(Y, DOSE) %>%
  bind_cols(model1_pred)

#get RMSE
model1_result %>%
  yardstick::rmse(
    truth = Y, 
    estimate = .pred
  )

#get R^2
model1_result %>%
  yardstick::rsq(
    truth = Y, 
    estimate = .pred
  )

#do the same for model2 (full model)
#predictions for model2
model2_pred <- predict(
  model2_fit,
  new_data = data4
)

#put predictions with data, add other vairables
model2_result <- data4 %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT) %>%
  bind_cols(model2_pred)

#get RMSE
model2_result %>%
  yardstick::rmse(
    truth = Y, 
    estimate = .pred
  )

#get R^2
model2_result %>%
  yardstick::rsq(
    truth = Y, 
    estimate = .pred
  )

```

We will make the same reduced and full model, but this time SEX will be the outcome instead of Y.

Fit a logistic model to the categorical/binary outcome (SEX) using the main predictor of interest, which we’ll again assume here to be DOSE.
```{r}
model3 <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

model3_fit <- model3 %>%
  fit(SEX ~ DOSE, data = data4)

```

Fit a logistic model to SEX using all predictors.
```{r}
model4 <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

model4_fit <- model4 %>%
  fit(SEX ~ DOSE + AGE + RACE + HT + WT + Y, data = data4)

```

For both models, compute accuracy and ROC-AUC and print them.
```{r}
#make predicitons for class
model3_pred <- predict(model3_fit, new_data = data4)
model4_pred <- predict(model4_fit, new_data = data4)

#put predictions with data
model3_result <- data4 %>%
  bind_cols(model3_pred)
model4_result <- data4 %>%
  bind_cols(model4_pred)

#calculate accuracy
model3_accuracy <- metrics(data = tibble(truth = data4$SEX, predicted = model3_pred$.pred_class), 
                    truth = truth, estimate = predicted)
model4_accuracy <- metrics(data = tibble(truth = data4$SEX, predicted = model4_pred$.pred_class), 
                    truth = truth, estimate = predicted)

#print accuracy
print(model3_accuracy)
print(model4_accuracy)

#make predictions for non-class
model3_pred <- predict(model3_fit, new_data = data4, type = "prob")
model4_pred <- predict(model4_fit, new_data = data4, type = "prob")

#calculate ROC_AUC
model3_roc_auc <- roc_auc(data = tibble(truth = data4$SEX,
                                 .pred_1 = model3_pred$.pred_1), 
                   truth = truth, .pred_1)

model4_roc_auc <- roc_auc(data = tibble(truth = data4$SEX, 
                                 .pred_1 = model4_pred$.pred_1), 
                   truth = truth, .pred_1)

print(model3_roc_auc)
print(model4_roc_auc)

```

This section marks the beginning of the modeule 10 exercise.
```{r}
rngseed = 1234 #use this later to set seed

data5 <- data4 %>% #remove race vairbale because we won't be using it
  select(Y, DOSE, AGE, SEX, WT, HT)

set.seed(rngseed) #set seed

data_split <- initial_split(data5, prop = 3/4) #3/4 of the data is for training

train_data <- training(data_split) #create dataframes for the sets
test_data  <- testing(data_split) #based on tidymodels instructions

#now we will fit our two models
model_dose <- linear_reg() #linear model with dose as predictor using training data
model_dose_fit <- 
  model_dose %>% 
  fit(Y ~ DOSE, data = train_data)
model_dose_fit 

model_all <- linear_reg() #linear model with all productors using training data
model_all_fit <- 
  model_all %>% 
  fit(Y ~ DOSE + AGE + SEX + WT + HT, data = train_data)
model_all_fit
```

Next, we'll calculate the RMSE for these two new models and a null model.

The null model has a RMSE of 948.35.

The model with only dose as a predictor has a RMSE of 702.81.

The full model has a RMSE of 627.44. This is the best model given this is the lowest RMSE out of the 3 models.
```{r}
set.seed(rngseed) #set seed

#predictions for model1
model_dose_pred <- predict(
  model_dose_fit,
  new_data = train_data)

#put predictions with data
model_dose_result <- train_data %>%
  select(Y, DOSE) %>%
  bind_cols(model_dose_pred)

#get RMSE
model_dose_result %>%
  yardstick::rmse(
    truth = Y, 
    estimate = .pred)

#do the same for the model with all predictors
#predictions for model2
model_all_pred <- predict(
  model_all_fit,
  new_data = train_data)

#put predictions with data, add other vairables
model_all_result <- train_data %>%
  select(Y, DOSE, AGE, SEX, WT, HT) %>%
  bind_cols(model_all_pred)

#get RMSE
model_all_result %>%
  yardstick::rmse(
    truth = Y, 
    estimate = .pred)

#I used chatGPT to help with this
# Define null model
null_mod <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("regression")

# Create a workflow without specifying intercept
null_wf <- workflow() %>%
  add_model(null_mod) %>%
  add_formula(Y ~ .)  # Only Y, no predictors will be used

# Fit the null model
null_fit <- fit(null_wf, data = train_data)

# Get predictions from the null model
null_preds <- predict(null_fit, train_data)

# Combine predictions with actual data
null_preds <- bind_cols(train_data, null_preds)

# Calculate RMSE
rmse_result <- rmse(null_preds, truth = Y, estimate = .pred)

# Print the result
print(rmse_result)
```

Now, we're going to do a 10-fold cross-validation with the testing data.

The simple CV model has an RMSE of 690.54.

The full CV model has an RMSE of 645.69.

The cross validation process has the same conclusion as the earlier process-- that the full model is best, with lowest RMSE. However, it's not quite as low, and the single model has a much closer RMSE this time. I'd be interested to see if this holds with even more iterations.

Next, I'm going to change the seed and rerun, then we'll see how the results look.

With a different seed value, the single model has a RMSE of 691.72.

The full model has a RMSE of 633.46.

This agrees with earlier results, that the full model has the lowest RMSE. However, the single and full models aren't quite so close. This makes me feel more confident in these results.

```{r}

set.seed(1938) #set seed

folds <- vfold_cv(train_data, v = 10) #set number of folds
folds

#for dose model
dose_wf <- 
  workflow() %>%
  add_model(model_dose) %>%
  add_formula(Y ~ DOSE)

# 10-fold cv for model_dose
dose_fit_rs <- 
  dose_wf %>% 
  fit_resamples(folds)
dose_fit_rs
# Collect metrics for model_dose
dose_metrics <- collect_metrics(dose_fit_rs)




#for model with all predictors
all_wf <- 
  workflow() %>%
  add_model(model_all) %>%
  add_formula(Y ~ DOSE + WT + AGE + SEX + HT)

# 10-fold cv for model_all
all_fit_rs <- 
  all_wf %>% 
  fit_resamples(folds)
all_fit_rs

# Collect metrics for model_all
all_metrics <- collect_metrics(all_fit_rs)




# Filter RMSE for both models
dose_rmse <- dose_metrics %>% filter(.metric == "rmse")
all_rmse <- all_metrics %>% filter(.metric == "rmse")

# Print RMSE for both models
print(dose_rmse)
print(all_rmse)

```







